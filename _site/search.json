[
  {
    "objectID": "research/00_high_yield_analysis/index.html",
    "href": "research/00_high_yield_analysis/index.html",
    "title": "An Analysis of High Yield Debt vs. Equities",
    "section": "",
    "text": "Performance Comparison\nLet’s visualize how Equities have performed relative to High Yield Debt:\n\nAggregateYearly\n\n\n\n\nCode\ng <- high_yield_data %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    group_by(symbol) %>% \n    arrange(date) %>% \n    mutate(value = value / first(value)) %>% \n    ungroup() %>% \n    ggplot(aes(date, value, color = name)) +\n    geom_line() +\n    theme_bw() +\n    labs(\n        color = \"\",\n        y = \"Wealth Index ($1)\",\n        x = \"\"\n    )\n\nggplotly(g)\n\n\n\n\n\n\nWe can see that, since 1984, the performance of Equities relative to High Yield Debt has been roughly comparable, with Equities slightly outperforming. Equities returned approximately 24x and High Yield Debt returned 20x. However, let’s see if we can identify the time periods where High Yield Debt outperformed Equities, and why.\n\n\n\n\nCode\nyearly_performance_tbl <- high_yield_data %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(year = year(date)) %>% \n    group_by(name) %>% \n    arrange(date) %>% \n    ungroup() %>% \n    group_by(name, year) %>% \n    summarize(pct_ret = (last(value) / first(value)) - 1) %>% \n    ungroup()\n\ndates_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(difference > 0) %>% \n    pull(year)\n\nyearly_performance_tbl %>% \n    ggplot() +\n    geom_col(data = yearly_performance_tbl,\n             mapping = aes(year, pct_ret, fill = name),\n             position = \"dodge\", color = \"black\") +\n    geom_rect(\n        data = tibble(start_date = c(dates_vec - .5),\n                      end_date = c(dates_vec + .5)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .3, fill = \"grey\") +\n    theme_bw() +\n    scale_fill_brewer(direction = 1) +\n    labs(x = \"\",\n         y = \"Return (%)\",\n         fill = \"\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_continuous(guide = guide_axis(angle = 45), n.breaks = 12) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nShaded regions indicate years where High Yield Debt ourperformed Equities\n\n\n\n\nFrom the above, we can see there seems to be cyclicality in the performance of High Yield Debt relative to Equities, with alternating 2-4 year periods of outperformance followed by 1-3 year periods of underperformance.\n\n\n\n\n\nKey Takeaways\nIn order to better understand the logical cause-and-effect relationships at play between our variables, let’s re-visualize our yearly performance comparison, but this time we will add our other relevant variables into the mix and attempt to find helpful trends:\n\nSmoothActual\n\n\n\n\nCode\ndates_positive_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(`High Yield Index` > 0 & difference > 0) %>% \n    pull(year)\n\nhigh_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>%\n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_smooth(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name), se = F\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\n\nCode\ng3 <- high_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_line(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name),\n        size = .75,\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\ng3\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\nHere are some primary takeaways from the visuals:\n\nOutperformance generally occurs when:\n\n‘Interest Rates’ are falling\nYield Spreads and the YTW are elevated\n\nEspecially in relation to the 2-5 years prior\n\nPreceeding periods are associated with economic bubbles\n\n1990 Oil Shock / Recession\n2001 Dot Com Bubble\n2008 Great Financial Crisis\n2022 Covid/Tech Bubble\n\n\nPositive outperformance occurs later in the period\n\n\n\nBuilding a Simple Narrative\nObviously, these broad points are interrelated; let’s try to build an extremely simple narrative that captures these relationships:\n\n\nThis narrative is by no means detailed, nor complete. In, fact it can be superbly incorrect, but that is for the reader to judge.\nDuring prolonged periods of strong stock market upswings, especially those wherein current growth exceeds productive growth, market participants bet on good times continuing - typically levering up in the process. This makes complete sense; why finance growth with equity if it is performing so well? Better to issue debt instead… investors and banks are happy to accept the debt, these companies are performing well (too well). But, just like it’s not a good idea to push yourself past physical capacity for extended periods of time, it is not good for a company to grow faster than its economic potential for extended periods of time; eventually something breaks. During this initial period of hurt, both equity holders and debt holders get whacked, valuations drop and the first bankruptcies occur. However, since debt holders get paid first, they get hurt slightly less than equity holders. Now, we are at a critical point in time… investors start to panic and make some back of the napkin calculations… and conclude that at current rates, debt is no longer a profitable investment, so they demand higher yields. Notice that YTW and High Yield Spreads briefly shoot up during these periods (1990, 2001, 2008…). However, the market seems to systematically underestimate two important things: 1) the Federal Reserve’s willingness to supply the proverbial medicine by lowering interest rates and 2) the associated aid from lower interest rates. We can notice from our graph that, after the tough first year where both asset classes perform badly, the Fed steps in and stimulates the economy by lowering the cost of borrowing (… we also notice that yields will quickly follow suit). Naturally, companies are now able to negotiate better deals on borrowing money, so they take on more debt at a lower rate and use it to service their older, more expensive debt. Obviously, these companies are kicking the can down the road, and we can see that problems will occur when the Fed can no longer lower interest rates, but for now, this narrative sort of makes sense. Notice the prolonged periods of High Yield’s outperformance occur approximately a year after the initial crash (1990, 2001, 2008) for 2-4 years because investors were able to invest when yields were at their peak, and the Fed stimulated the economy shortly thereafter. Now, we flip back to the origin of the story… High Yield Debt is starting to see a prolonged period of strong performance… things seem less risky and therefore yields decline and the pendulum starts to swing in favor of equities again…\n\n\nApplying the Key Takeaways and Narrative\nLet’s attempt to codify our narrative and key takeaways, so that we can isolate periods of strong High Yield Debt returns (and hopefully identify future ones as well). I will tell the computer to go through the data and highlight a region if:\n\nthe current YTW is 30% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Yield Spread is 75% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Federal Funds Rate is 33% lower than its 3 Year Moving Average w/ a 2-year lag\n\nHere is the resulting plot:\n\n\nCode\nytw_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ytw\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 8),\n        .f = mean,\n        na.rm = T,\n        .period = 12,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.3 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nhys_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"hys\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 365 * 2),\n        .f = mean,\n        na.rm = T,\n        .period = 365 * 3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.75 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nffr_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ffr\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 12*2),\n        .f = mean,\n        na.rm = T,\n        .period = 12*3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value < .67 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\ng1 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    bind_rows(\n        tibble(\n            start_date = ffr_signal_dates_vec,\n            end_date = ffr_signal_dates_vec + months(1),\n            type = \"Fed Funds Rate\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    scale_fill_brewer() +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng2 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng3 <- g3 +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ncowplot::plot_grid(g1, g2, g3, ncol = 1, align = \"hv\")\n\n\n\n\n\nPlot 2 is the same as Plot 1, but Fed Funds Rate is removed. Plot 3 is being re-provided for ease of comparison.\n\n\n\n\nAs we can see, these 3 simple metrics, especially the YTW metric, provide a decent signal for periods of strong High Yield Debt returns. And while these metrics have been created with a knowledge of the past, they certainly corroborate the simple narrative we created above. Possible next steps include applying these signals to other developed markets while being mindful of their current position in their interest rate cycle, improving these signals, and assessing hypothetical performance, etc… I leave these as exercises to the reader…\n\n\nThe Future: Where are we heading?\nIn our above research, one of our key takeaways involved the Federal Funds Rate (i.e. the ‘interest’ rate). This should be self-evident as interest rates measure the cost of borrowing money and are therefore the primary drivers of the economy and financial markets. We noted that during periods of economic hurt (typically after some sort of ‘bubble’) the Fed would lower rates and kick the can down the road. However, we have now reached that special point in time wherein interest rates can no longer be lowered. Instead, the Fed has been forced to aggressively raise rates, subsequently popping the Tech bubble. This makes for an interesting investment environment going forward. We have noted that High Yield Debt performs better during these reactionary declining interest rate environments, and equities during the post-recovery ‘rising’ rate environment. However, in spite of all this, my outlook for equities is still slightly more pessimistic. Instead, I think that High Yield Debt is likely to outperform over the next 2 years, followed by Equities picking back up…\n\n\nFinal Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice. Feel free to reach out at maxsands700@gmail.com with questions and comments… Cheers :)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "My Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nHigh Yield Debt\n\n\n\nRead Time: 15 mins\n\n\n\nMax Sands\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "2022 - Present\nCertified:\n\nData Science for Business w/ R\nML & High Performance Time Series Forecasting w/ R\nPredictive Shiny Web Applications w/ R\nShiny Development w/ R & AWS (in progress)\n\n\n\n\n2019 - 2022\nDesautels Faculty of Management, Montréal, Canada\nBachelor of Commerce, Major in Finance, Double Concentration in Accounting & Statistics"
  },
  {
    "objectID": "resume.html#choate-hall-stewart-choate-investment-advisors",
    "href": "resume.html#choate-hall-stewart-choate-investment-advisors",
    "title": "",
    "section": "Choate Hall & Stewart / Choate Investment Advisors",
    "text": "Choate Hall & Stewart / Choate Investment Advisors\nBusiness & Investment Analyst | 2022 - Present | Boston, MA\n\nProviding general investment research and assessing manager performance for both the Equities and Fixed Income teams\nDeveloping unique solutions and models for the Investment and Operations divisions:\n\nBuilt in-house Monte Carlo Simulation tool for Retirement Analysis\n\nIncludes Sensitivity Analysis across several key inputs\nIncludes report generation - both static PDFs and interactive HTML\n\nAutomating report and presentation deck generation"
  },
  {
    "objectID": "resume.html#mcgill-university-1",
    "href": "resume.html#mcgill-university-1",
    "title": "",
    "section": "McGill University",
    "text": "McGill University\nTeaching Assistant | 2021 - 2022 | Montréal, Canada\n\nDelivering lectures, mentoring/tutoring students, and grading all assessments - typically reserved for Graduate students"
  },
  {
    "objectID": "resume.html#ironhold-capital",
    "href": "resume.html#ironhold-capital",
    "title": "",
    "section": "IronHold Capital",
    "text": "IronHold Capital\nInvestment Analyst & Executive Assistant | 2020 - 2021 | New York, United States\n\nAided in raising capital, marketing efforts, reviewing legal contracts, and meeting with Family Offices & HNIs on behalf of the CEO\nWorked directly under the CIO and conducted research on Indian and U.S. Equities - the fund followed a Value-based strategy\nAuthor of the fund’s newsletter that covered key macroeconomic events, political news, and Family Office industry insights"
  },
  {
    "objectID": "resume.html#private-tutor",
    "href": "resume.html#private-tutor",
    "title": "",
    "section": "Private Tutor",
    "text": "Private Tutor\n2018 - 2021\n\nTutored students in Mathematics, Statistics, Physics, English, and Finance, and revised college admission essays"
  },
  {
    "objectID": "resume.html#replicating-ray-dalios-work-forecasting-the-big-debt-cycles",
    "href": "resume.html#replicating-ray-dalios-work-forecasting-the-big-debt-cycles",
    "title": "",
    "section": "Replicating Ray Dalio’s Work: Forecasting the ‘Big’ & ‘Debt’ Cycles",
    "text": "Replicating Ray Dalio’s Work: Forecasting the ‘Big’ & ‘Debt’ Cycles\n2022 - Present\n\nBuilding a system that continuously retrieves data from various sources and uses Machine Learning to predict several countries’ current and future stages of their ‘Big’ and ‘Debt’ Cycles, with the eventual goal of predicting asset returns across classes, sectors, and geographies"
  },
  {
    "objectID": "resume.html#predicting-the-spread-between-the-russell-1000-2000-growth-value",
    "href": "resume.html#predicting-the-spread-between-the-russell-1000-2000-growth-value",
    "title": "",
    "section": "Predicting the Spread between the Russell 1000, 2000 Growth & Value",
    "text": "Predicting the Spread between the Russell 1000, 2000 Growth & Value\n2022 - Present\n\nUsing data from Bloomberg, the FRED, Google, and other sources in conjunctions with Machine Learning methods to predict the weekly return spreads across the Russell 1000, 2000, Growth & Value ETFs"
  },
  {
    "objectID": "resume.html#coding-w-bloomberg-automating-equity-screens-rough-draft-dcfs",
    "href": "resume.html#coding-w-bloomberg-automating-equity-screens-rough-draft-dcfs",
    "title": "",
    "section": "Coding w/ Bloomberg: Automating Equity Screens & Rough Draft DCFs",
    "text": "Coding w/ Bloomberg: Automating Equity Screens & Rough Draft DCFs\n2022\n\nWrote code in R that connects to and scrapes information from a Bloomberg Terminal by utilizing its Desktop API\n\nAutomated Bloomberg’s EQS function so that I could run continuous equity screens (i.e. every ‘Saturday’ since MM/DD/YY)\nUsed scraped information to automate the generation of a ‘rough draft’ DCF for any company; companies with a significant discrepancy between ‘rough draft’ implied share price and 7-day VWAP would garner additional, refined analysis"
  },
  {
    "objectID": "resume.html#coding-w-amazon-web-services-aws-gemini-kraken-automating-cryptocurrency-trading-strategy",
    "href": "resume.html#coding-w-amazon-web-services-aws-gemini-kraken-automating-cryptocurrency-trading-strategy",
    "title": "",
    "section": "Coding w/ Amazon Web Services (AWS), Gemini, & Kraken: Automating Cryptocurrency Trading Strategy",
    "text": "Coding w/ Amazon Web Services (AWS), Gemini, & Kraken: Automating Cryptocurrency Trading Strategy\n2022\n\nWrote code in Python that connects to Gemini (cryptocurrency exchange) web API and allows me to place limit orders if certain conditions are met; automated trading by running the code externally every 5 minutes through AWS Cloudwatch & AWS Lambda\n\nRepeated the above with Kraken (cryptocurrency exchange) web API to place market orders"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Sands",
    "section": "",
    "text": "Hi All - I’m Max and Welcome to my Site. I’m a Business Analyst with a passion for Data Science and Investment Research. On here, you can find Research Articles covering these topics; feel free to reach out via LinkedIn if they pique your interest, you would like to collaborate, or if you have questions. Cheers :)\n\n  \n    \n\n    \n  \n    \n     Instagram\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\n\nMcGill University | Finance | 2019 - 2022\n\n\n\nChoate Investment Advisors | Business Analyst | 2022 - Present\nMcGill University | T.A. - Adv. Bus. Stats | 2021 - 2022\nIronHold Capital | Investment Analyst | 2020 - 2021"
  },
  {
    "objectID": "index.html#current-projects",
    "href": "index.html#current-projects",
    "title": "Max Sands",
    "section": "Current Projects",
    "text": "Current Projects\nReplicating Ray Dalio’s Work: Ray Dalio is personal idol of mine because he is simultaneously an economist, data scientist, and narrator, but, above all else, he exemplifies the power of ‘first-principles’ thinking. He is able to synthesize centuries of history and data into logical, measurable, cause-and-effect relationships. I am currently attempting to build a system that forecasts major countries’ “Big” and “Debt” Cycles using Machine Learning (see Ray Dalio’s Principles for Dealing with With the Changing World Order and Principles for Navigating Big Debt Crises).\nPredicting the Spread between the Russell 1000, 2000 Growth & Value: Using data from the Bloomberg, the FRED, Google, and other sources in conjunction with Machine Learning to predict weekly returns spreads for these 4 ETFs.\nIf you are interested about these projects, would like to collaborate, or would like to request data I’ve gathered, please reach out via LinkedIn - thank you."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html",
    "href": "instructional_articles/00_r_basics/index.html",
    "title": "R in 5 Minutes",
    "section": "",
    "text": "The purpose of this article is 3-fold:\n\nto demonstrate the basics of R as concisely as possible so that you can get up and running on your own projects, even if you’ve had no exposure to coding.\nto act as a basic guide for the non-technical readers interested in following my Research Articles at a more granular level.\nto familiarize myself with the process of writing and explaining topics before I publish my research (and to make sure that my website is working…)\n\n\n\nI would quickly like to explain my background and why I think it is important to have a basic knowledge of ‘coding’:\nI am a Business & Investment Analyst, and 9 months ago I had absolutely no knowledge of ‘coding’; my technical ability was comparable to that of your average dog. I can now tell you 9 months in that understanding the basics of ‘coding’ goes a very long way.\nFirstly, as long as you do a task correctly the first time in code, you can then automate away that task (and its different variations). Whether its performing the same calculations in an Excel file that your boss sends you every morning, or publishing your company’s quarterly financial statements, the same principle applies.\nSecondly, we are living in a world where data is everywhere, and the ability to code allows one to dig into the data and draw valuable insights from it. For anyone in an analytical position (whether Financial Analyst, Medical Researcher, or CEO), this is extremely important and allows you to stand on the shoulders of giants.\nThirdly, you can leverage tools that others have built. There is so much free code on the web and someone else may have already built a tool or completed a task that you are trying to do. This is extremely helpful.\nLastly, a word of caution: coding is not everything. You can be the world’s greatest coder, but if you lack the ability to build a logical, easily-explainable narrative from data, then your value is limited to the tools that you can build for others. In other words, true value comes from the ability to not only work with data, but also derive meaning from it and think originally.\nOk, that’s all; let’s get into it!"
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "href": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "title": "R in 5 Minutes",
    "section": "The Basics of Data",
    "text": "The Basics of Data\nData is simply a spreadsheet of values, and we would like our data to be in a ‘tidy’ format.\n\nTidy Data\nData is considered tidy when each column represents a variable and each row consists of an observation. Consider the following dataset (and feel free to inspect the code and guess what each line means):\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNotice how this data is tidy; each column represents a variable (price, color, etc.) and each row is an observed diamond. Your goal should be to have your data in this format because it is easy to manipulate."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#gathering-data",
    "href": "instructional_articles/00_r_basics/index.html#gathering-data",
    "title": "R in 5 Minutes",
    "section": "Gathering Data",
    "text": "Gathering Data\nData is typically gathered from an API, a database, or simply an Excel/csv spreadsheet that you may have. For now, we will use a built-in R dataset called diamonds."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "href": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "title": "R in 5 Minutes",
    "section": "Manipulating Data",
    "text": "Manipulating Data\nAs long as data is in a tidy format, there are only a few actions that we need to do when manipulating data:\n\n\n\nfilter\nfilter data according to certain conditions\n\n\nsummarize\nsummarize the data (e.g. finding the average)\n\n\ngroup\ngroup similar observations\n\n\npivot\n‘pivoting’ the data in different ways\n\n\nselect\nselect relevant information\n\n\nmutate\nchanging the data in some fashion\n\n\n\n\nFiltering\nLet’s pretend that we only want to consider diamonds with a carat greater than .7 and a depth greater than 63: (click on the “Code” section)\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56\n2759\n5.81\n5.85\n3.72\n\n\n0.96\nFair\nF\nSI2\n66.3\n62\n2759\n6.27\n5.95\n4.07\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56\n2760\n5.80\n5.75\n3.65\n\n\n0.91\nFair\nH\nSI2\n64.4\n57\n2763\n6.11\n6.09\n3.93\n\n\n0.91\nFair\nH\nSI2\n65.7\n60\n2763\n6.03\n5.99\n3.95\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58\n2764\n5.64\n5.68\n3.60\n\n\n\n\n\n\nLet’s continue to filter down and consider only the subset with a cut of “Very Good”:\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    filter(cut == \"Very Good\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56.0\n2759\n5.81\n5.85\n3.72\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56.0\n2760\n5.80\n5.75\n3.65\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58.0\n2764\n5.64\n5.68\n3.60\n\n\n0.71\nVery Good\nG\nVS1\n63.3\n59.0\n2768\n5.52\n5.61\n3.52\n\n\n0.72\nVery Good\nG\nVS2\n63.7\n56.4\n2776\n5.62\n5.69\n3.61\n\n\n0.75\nVery Good\nD\nSI2\n63.1\n58.0\n2782\n5.78\n5.73\n3.63\n\n\n\n\n\n\nYou will now see that we have from our original 53,940 diamonds, we have filtered down to 1,550 that adhere to our conditions.\nAt this point you may have two questions:\n\nWhat is the %>%?\n\nThis is called a pipe and you can translate it to “and then”. It allows us to perform several operations consecutively. So if we look at the code, we first start with the diamonds dataset by typing diamonds, and then we filter according to carat and depth, and then we filter according to cut. The pipe is extremely useful and it is native to R.\n\nWhat does the head() function do?\n\nIt prints only the first 6 observations, that way you don’t have a table with 50,000 rows on your screen.\n\nWhat if I want to filter down to several different cuts, not just “Very Good”\n\nGreat question, here’s what you would do:\n\n\nCode\ndiamonds %>% \n    filter(cut %in% c(\"Ideal\", \"Premium\")) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.23\nIdeal\nJ\nVS1\n62.8\n56\n340\n3.93\n3.90\n2.46\n\n\n0.22\nPremium\nF\nSI1\n60.4\n61\n342\n3.88\n3.84\n2.33\n\n\n0.31\nIdeal\nJ\nSI2\n62.2\n54\n344\n4.35\n4.37\n2.71\n\n\n\n\n\n\nWe tell R to filter down to the observations where cut matches one of the strings in the vector c(\"Ideal\", \"Premium\"). The c() function creates a vector.\n\n\nSummarizing\nLet’s say we want to summarize the data and find the average diamond price, along with its standard deviation:\n\n\nCode\ndiamonds %>% \n    summarize(avg_price = mean(price),\n              st_dev    = sd(price))\n\n\n\n\n\n\navg_price\nst_dev\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\nNotice that we can take our 50,000+ diamonds and summarize the data down to an average price…\nYou will notice that in the summarize function I start by naming the column I want (avg_price) and then I tell R what to do (find the mean of the price variable/column. The mean() & sd() functions calculate mean and standard deviation respectively). I could just as easily call the columns “thing1” & “thing2”:\n\n\nCode\ndiamonds %>% \n    summarize(thing1 = mean(price),\n              thing2    = sd(price))\n\n\n\n\n\n\nthing1\nthing2\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\n\n\nGrouping\nSummarizing the entire data is important, but let’s say we want to find the average diamond price within each color group…\n\n\nCode\ndiamonds %>% \n    group_by(color) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup()\n\n\n\n\n\n\ncolor\navg_price\n\n\n\n\nD\n3169.954\n\n\nE\n3076.752\n\n\nF\n3724.886\n\n\nG\n3999.136\n\n\nH\n4486.669\n\n\nI\n5091.875\n\n\nJ\n5323.818\n\n\n\n\n\n\nWe can take things a step further and group by color and cut…\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nYou will notice that we now have average price for each color and cut. I also only showed the first 10 rows of output by using the slice() function.\n\n\nPivoting\nPivoting is probably the most complicated of the broad actions I am showing you, but the previous segment allows for a great transition. I decided to show only the first 10 rows of output rather than inundate you with 35 rows, but there must be a better way of showing the output, right? I mean we have letters repeating in the color column. This would make more sense:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    )\n\n\n\n\n\n\ncolor\nFair\nGood\nVery Good\nPremium\nIdeal\n\n\n\n\nD\n4291.061\n3405.382\n3470.467\n3631.293\n2629.095\n\n\nE\n3682.312\n3423.644\n3214.652\n3538.914\n2597.550\n\n\nF\n3827.003\n3495.750\n3778.820\n4324.890\n3374.939\n\n\nG\n4239.255\n4123.482\n3872.754\n4500.742\n3720.706\n\n\nH\n5135.683\n4276.255\n4535.390\n5216.707\n3889.335\n\n\nI\n4685.446\n5078.533\n5255.880\n5946.181\n4451.970\n\n\nJ\n4975.655\n4574.173\n5103.513\n6294.592\n4918.186\n\n\n\n\n\n\nWe tell R to take our 35 row table, and pivot it so that we have a color column followed by columns with the different cuts, wherein each value is the average price.\nThe names_from argument asks us what variable to we want to pivot on (we said ‘cut’ and therefore R took all of the cut values and made them columns). The values_from argument asks us which variable we would like to R to occupy the new columns with (we said ‘avg_price’ and therefore R occupied all of the ‘cells’ in our pivot table with the corresponding values from the avg_price column).\nQuick Tip: hitting the tab key when your cursor is inside of a function’s parentheses will show all of the function’s available arguments (2 of which are names_from and values_from for the pivot_longer() function.)\nImportant Note: You will notice that now we have violated the premise of tidy data. The columns Fair:Ideal are not variables. They are types of “cut” (cut is the variable). For the purposes of coding, and data manipulation, we want our data to be in a tidy format. However, for the purposes of presentation, we typically want our data to be in a ‘wide’ format (hence pivot_wider).\nWe can do the opposite and revert our table back into a ‘long’ format with pivot_longer() :\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols = Fair:Ideal\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\nname\nvalue\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nWe can also rename the columns back to their original names within the pivot_longer() function:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols      = Fair:Ideal,\n        names_to  = \"cut\",\n        values_to = \"avg_price\"\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nThat’s on pivoting…\n\n\nSelecting\nSelecting is straightforward. Here are the first 6 rows of our original dataset:\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nLet’s say we are about to investigate something but we only need price, carat, and cut… then it is best practice to select those variables/columns first (imagine we have thousands of variables/columns…):\n\n\nCode\ndiamonds %>% \n    select(price, carat, cut) %>% \n    head()\n\n\n\n\n\n\nprice\ncarat\ncut\n\n\n\n\n326\n0.23\nIdeal\n\n\n326\n0.21\nPremium\n\n\n327\n0.23\nGood\n\n\n334\n0.29\nPremium\n\n\n335\n0.31\nGood\n\n\n336\n0.24\nVery Good\n\n\n\n\n\n\nWe can also select by omission:\n\n\nCode\ndiamonds %>% \n    select(-x, -y, -z) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n\n\n\n\n\n\nWe can select variables carat through clarity:\n\n\nCode\ndiamonds %>% \n    select(carat:clarity) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\n\n\n\n\n0.23\nIdeal\nE\nSI2\n\n\n0.21\nPremium\nE\nSI1\n\n\n0.23\nGood\nE\nVS1\n\n\n0.29\nPremium\nI\nVS2\n\n\n0.31\nGood\nJ\nSI2\n\n\n0.24\nVery Good\nJ\nVVS2\n\n\n\n\n\n\nAnd again by omission:\n\n\nCode\ndiamonds %>% \n    select(-carat:-clarity) %>% \n    head()\n\n\n\n\n\n\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nVery simple.\n\n\nMutating\nWhat if we want to perform some sort of calculation or change the data in some way? This is the purpose of mutating…\nIn our dataset, we have the variables x, y, z which represent the length, width, and height of the diamond. If we pretend all the diamonds are cubes, we can calculate the cubic volume of each diamond by multiplying the dimensions of each diamond. Let’s do this:\n\n\nCode\ndiamonds %>% \n    select(x:z) %>% \n    mutate(volume = x * y * z) %>% \n    head()\n\n\n\n\n\n\nx\ny\nz\nvolume\n\n\n\n\n3.95\n3.98\n2.43\n38.20203\n\n\n3.89\n3.84\n2.31\n34.50586\n\n\n4.05\n4.07\n2.31\n38.07688\n\n\n4.20\n4.23\n2.63\n46.72458\n\n\n4.34\n4.35\n2.75\n51.91725\n\n\n3.94\n3.96\n2.48\n38.69395\n\n\n\n\n\n\nNotice how mutate() is similar in structure to summarize(); first we tell R what we would like name our new variable/column (“volume”), and then we tell R how to calculate it.\nMutate can also change a current column:\n\n\nCode\ndiamonds %>% \n    mutate(carat = \"Hello World\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\nHello World\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\nHello World\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\nHello World\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\nHello World\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\nHello World\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\nHello World\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNow, all observations of carat are “Hello World”."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "href": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "title": "R in 5 Minutes",
    "section": "Basic Modeling",
    "text": "Basic Modeling\nWe will build a linear model to explain diamond prices. In R, the function to create a linear model is lm():\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ carat) %>% \n    summary()\n\n\n\nCall:\nlm(formula = price ~ carat, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2256.36      13.06  -172.8   <2e-16 ***\ncarat        7756.43      14.07   551.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: < 2.2e-16\n\n\nWe just built a linear model that regressed carat on diamond price. As you can see, we can use a diamond’s caratage to explain 85% of price variation. Our model also tells us that for every 1 unit increase in caratage, diamond prices increases by $7,756 on average.\nHowever, I’m sure you will agree that the output is not visually pleasing. Moreover, it is not easy to manipulate since it is not in a tabular format.\nLet’s, once again, stand on the shoulders of giants and utilize a tool that someone else has built to clean up the output. Just like you installed tidyverse, install the broom package by running install.packages(\"broom\"). Then, load the package by running library(broom).\n\n\nCode\nlibrary(broom)\n\n\nThis time let’s regress price on all other variables and use the tidy() function from the broom package to tidy the output:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    tidy()\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n5753.761857\n396.629824\n14.5066294\n0.0000000\n\n\ncarat\n11256.978307\n48.627509\n231.4940348\n0.0000000\n\n\ncut.L\n584.457278\n22.478150\n26.0011290\n0.0000000\n\n\ncut.Q\n-301.908158\n17.993919\n-16.7783441\n0.0000000\n\n\ncut.C\n148.034703\n15.483328\n9.5609097\n0.0000000\n\n\ncut^4\n-20.793893\n12.376508\n-1.6801098\n0.0929418\n\n\ncolor.L\n-1952.160010\n17.341767\n-112.5698421\n0.0000000\n\n\ncolor.Q\n-672.053621\n15.776995\n-42.5970601\n0.0000000\n\n\ncolor.C\n-165.282926\n14.724927\n-11.2247022\n0.0000000\n\n\ncolor^4\n38.195186\n13.526539\n2.8237221\n0.0047487\n\n\ncolor^5\n-95.792932\n12.776114\n-7.4978145\n0.0000000\n\n\ncolor^6\n-48.466440\n11.613917\n-4.1731348\n0.0000301\n\n\nclarity.L\n4097.431318\n30.258596\n135.4137965\n0.0000000\n\n\nclarity.Q\n-1925.004097\n28.227228\n-68.1967102\n0.0000000\n\n\nclarity.C\n982.204550\n24.151516\n40.6684433\n0.0000000\n\n\nclarity^4\n-364.918493\n19.285011\n-18.9223900\n0.0000000\n\n\nclarity^5\n233.563110\n15.751700\n14.8278029\n0.0000000\n\n\nclarity^6\n6.883492\n13.715100\n0.5018915\n0.6157459\n\n\nclarity^7\n90.639737\n12.103482\n7.4887321\n0.0000000\n\n\ndepth\n-63.806100\n4.534554\n-14.0710870\n0.0000000\n\n\ntable\n-26.474085\n2.911655\n-9.0924516\n0.0000000\n\n\nx\n-1008.261098\n32.897748\n-30.6483316\n0.0000000\n\n\ny\n9.608887\n19.332896\n0.4970226\n0.6191751\n\n\nz\n-50.118891\n33.486301\n-1.4966983\n0.1344776\n\n\n\n\n\n\nYou will notice that I used ‘.’ to tell R ‘all other variables’ rather than type each of them out. More importantly, the output is much cleaner and easier to manipulate.\nHowever, we cannot see the model’s accuracy. For this, we need to use the glance() function from broom:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    glance()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\ndf.residual\nnobs\n\n\n\n\n0.9197915\n0.9197573\n1130.094\n26881.83\n0\n23\n53916\n53940\n\n\n\n\n\n\nNow we have accuracy metrics in a nice format.\nLastly, if we would like to see the model’s fit for each observation, we can use the augment() function from broom (scroll to the right):\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    augment() %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprice\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nx\ny\nz\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n326\n0.23\nIdeal\nE\nSI2\n61.5\n55\n3.95\n3.98\n2.43\n-1346.3643\n1672.3643\n0.0003742\n1130.082\n0.0000342\n1.4801217\n\n\n326\n0.21\nPremium\nE\nSI1\n59.8\n61\n3.89\n3.84\n2.31\n-664.5954\n990.5954\n0.0004133\n1130.097\n0.0000132\n0.8767411\n\n\n327\n0.23\nGood\nE\nVS1\n56.9\n65\n4.05\n4.07\n2.31\n211.1071\n115.8929\n0.0009098\n1130.105\n0.0000004\n0.1025982\n\n\n334\n0.29\nPremium\nI\nVS2\n62.4\n58\n4.20\n4.23\n2.63\n-830.7372\n1164.7372\n0.0004062\n1130.094\n0.0000180\n1.0308641\n\n\n335\n0.31\nGood\nJ\nSI2\n63.3\n58\n4.34\n4.35\n2.75\n-3459.2242\n3794.2242\n0.0007715\n1129.987\n0.0003629\n3.3587358\n\n\n336\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n3.94\n3.96\n2.48\n-1380.4876\n1716.4876\n0.0007230\n1130.081\n0.0000696\n1.5194380\n\n\n\n\n\n\nThe broom package is so useful because it cleans up model output, but more importantly, it can be used with many other (more complex) models."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "href": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "title": "R in 5 Minutes",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nBeing able to visualize data is essential for understanding it; the famous saying “a picture is worth a thousands words” is doubly true in today’s age.\nLet’s start out by plotting diamond price against caratage.\n\nCreating a Canvas\nFirst we need to create a canvas with the ggplot() function:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price))\n\n\n\n\n\nNotice that we start with the diamonds dataset and then we create a canvas with the ggplot() function. The aes() function stands for aesthetic and allows us to pick which variables/columns we want to use in our plot. In this case we tell R that we want to plot carat on the x-axis and price on the y-axis.\n\n\nAdding Geoms\nIn our plot we would like to add dots that represent each data point. In R adding these elements are called geometries (i.e. geoms):\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point()\n\n\n\n\n\nNotice how when creating plots with ggplot, we can no longer use the pipe (%>%). Instead, we use a + sign to add layers to the plot.\nFrom our plot we can tell that there is a clear positive relationship between price and caratage.\n\n\nModifying Geoms\nOur plot contains so many points and it is overwhelming; let’s modify the plot so that the points are more transparent with the alpha argument of geom_point().\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point(alpha = .15, color = \"midnightblue\") +\n    geom_smooth()\n\n\n\n\n\nYou will notice that the points are more transparent and that we also modified their color. We also included a smoother line with geom_smooth().\n\n\nAdding Aesthetics\nUp to now our plot has had only 2 aesthetics (x and y). But, all of the arguments that can be passed to geoms (alpha, color, etc.) are actually aesthetics that can be passed in the main aes() function. This probably sounds confusing but the following code will make much more sense:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth()\n\n\n\n\n\nYou will notice that instead of locally changing the color argument in the geom_point() function, we have put in the main aes() function wherein we set it equal to cut. By doing this, we are telling R that the color of each geometry should be defined by the cut variable/column.\n\n\nFaceting\nOur plot is overwhelming with all the different colors on one canvas so lets create a faceted canvas… Rather than explain in words, the following code should be self evident:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut)\n\n\n\n\n\nThis is called a faceted plot because we have created facets according to the cut variable/column. You will note that we need to put a ~ before the specified variable; this is just how the facet_wrap function works.\nWe can also decide to facet according to some other variables, like so:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~clarity, scales = \"free\")\n\n\n\n\n\nYou will notice that I also supplied the scales argument within the facet_wrap() function which allows each faceted plot to have different x and y scales that fit accordingly. Compare the x and y axes of the ‘VS1’ plot with those of the ‘VVS2’. They have different scales.\n\n\nAdding Labels\nLet’s add labels to our plot…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    )\n\n\n\n\n\n\n\nChanging Theme\nR has some preset plotting themes…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_linedraw()\n\n\n\n\n\n…there are several others.\n\n\nModifying Scales\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw() +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nNotice we converted the axis/scale on the plot to a dollar format…\n\n\nExample of More Plots\nWith these basic tools, you now have the ability to create so many different types of plots to gain insights from your data.\nHere are a few more plots with code to give you a flavor…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram(position = \"dodge\") +\n    theme_bw() +\n    scale_fill_brewer()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_density() +\n    theme_bw() +\n    scale_fill_brewer() +\n    facet_wrap(~cut)\n\n\n\n\n\nThere are other packages that help with creating nice plots… install and load ggridges.\n\n\nCode\nlibrary(ggridges)\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = stat(x))) +\n    geom_density_ridges_gradient(scale = 2) +\n    scale_fill_viridis_c(name = \"Price (in $)\", option = \"C\") +\n    theme_minimal() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = factor(stat(quantile)))) +\n    stat_density_ridges(\n        geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n        quantiles = 4, quantile_lines = TRUE\n    ) +\n    scale_fill_brewer() +\n    theme_linedraw() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCreating Interactive Plots\nWe also have the ability to create interactive plots with the help of a package called plotly. This is another example of the power of open-source coding, which gives us the ability to leverage code that others have built (that we may not have the expertise to create ourselves…). Like we did with the tidyverse, run install.packages(\"plotly\") and then load it into your environment with library(plotly). All we have to do to make a plot interactive, is to save it into our environment using the assignment operator - <-. I am going to save my plot as g and then we have to run ggplotly(g).\nLook at the code below:\n\n\nCode\nlibrary(plotly)\ng <- diamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\nggplotly(g)\n\n\n\n\n\n\nThis is just a taste of the plots that can be generated…"
  },
  {
    "objectID": "instructional_articles.html",
    "href": "instructional_articles.html",
    "title": "Instructional Articles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nApplication-based Learning\n\n\n\nMax Sands\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]