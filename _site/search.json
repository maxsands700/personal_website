[
  {
    "objectID": "research/01_management_trading_analysis/index.html",
    "href": "research/01_management_trading_analysis/index.html",
    "title": "Management Trading",
    "section": "",
    "text": "Intro\nIn this article we will investigate if following management trading can provide superior investment returns. To do this, I’ve gathered a list of all the U.S. companies wherein management accounted for .01% (or more) of all volume traded for that company’s stock in the week prior. Other constraints, like a minimum company market capitalization, were also applied. I then retrieved each company’s stock prices for the following 3 months. Here is the resulting data since 2000:\n\nDataVariable Definitions\n\n\n\n\nCode\nmgmt_data %>% \n    head() %>% \n    gt() %>% \n    gt::fmt_currency(\n        columns = c(market_cap, price),\n        suffixing = T\n    ) %>% \n    gt::fmt_number(\n        columns = c(p_e), decimals = 0\n    ) %>% \n    gt::fmt_percent(\n        columns = c(mgmt_buy_volume, total_return_ytd)\n    )\n\n\n\n\n\n\n  \n  \n    \n      screen_date\n      date\n      symbol\n      short_name\n      mgmt_buy_volume\n      market_cap\n      p_e\n      total_return_ytd\n      price\n    \n  \n  \n    2000-07-07\n2000-07-07\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$24.06\n    2000-07-07\n2000-07-10\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$24.06\n    2000-07-07\n2000-07-11\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$23.31\n    2000-07-07\n2000-07-12\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$23.62\n    2000-07-07\n2000-07-13\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$25.69\n    2000-07-07\n2000-07-14\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$25.25\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\n\nscreen_date\nThe date where the computer went back in time and filtered all the companies that had management buy volume account for .01% (or more) of all trading volume in the week prior. You will notice that screen_date is always a Saturday.\n\n\ndate\nThe date associated with the company’s stock price (all other variables are constant).\n\n\nsymbol\nThe company’s stock ticker\n\n\nshort_name\nThe company’s name\n\n\nmgmt_buy_volume\nManagement’s proportion of trade volume\n\n\nmarket_cap\nThe company’s current market value (number of shares * share price)\n\n\np_e\nThe company’s Price-to-Earnings ratio (the amount of money paid for $1 of the company’s earnings)\n\n\ntotal_return_ytd\nThe Year-to-Date return on the company’s stock\n\n\nprice\nThe company’s closing stock price\n\n\n\n\n\n\n\n\nExamining the Data\nLet’s dig into the data and get a feel for what we are looking at. As usual, we will use visuals to help us.\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date) %>% \n    distinct(symbol) %>% \n    ungroup() %>% \n    count(screen_date) %>% \n    ggplot(aes(screen_date, n)) +\n    geom_col() +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        title = \"Count of Companies that Adhere to our Conditions\",\n        subtitle = \"Each Bar represents a Week\"\n    ) +\n    theme(text = element_text(size = 15))\n\n\n\n\n\nAs we can see from our plot, the weekly number of companies that adhere to our conditions gradually increase over time and rapidly increase after 2020. There are a few logical reasons for this:\n\nI filtered for companies with market caps greater than $200M, but $200M in 2001 is worth much more than $200M in 2022. It would have been better to filter while adjusting for inflation, but I forgot to do this…\nThe overall number of companies in the U.S. has greatly increased since 2001\nOver the past several years, we have had very low interest rates. With the cost of borrowing money so low, people have been spending money, creating companies, and driving valuations up.\n\nSince we don’t have many companies during 2000-2004, let’s make the executive decision to only use data from 2005 and on. Here is the same plot as before but lets group by year this time:\n\n\nCode\nmgmt_data <- mgmt_data %>% \n    filter(screen_date >= ymd(\"2005-01-01\"))\n\nmgmt_data %>% \n    group_by(screen_date) %>% \n    distinct(symbol) %>% \n    ungroup() %>% \n    mutate(year = year(screen_date)) %>% \n    count(year) %>% \n    ggplot(aes(year, n)) +\n    geom_col() +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        title = \"Count of Companies that Adhere to our Conditions\",\n        subtitle = \"Each Bar represents a Year\"\n    ) +\n    theme(text = element_text(size = 15))\n\n\n\n\n\nWhile we would still like to see more companies is the earlier years, this will have to do. Let’s continue forward and assess the performance of our filtered stocks relative to traditional index funds.\n\n\nPerformance Comparison\nLet’s use the Russell 2000 Index and the SP500 Index as our benchmarks. However, rather than use the indices themselves, let’s use ETFs instead as these are a better representation of actual investment performance since individuals cannot actually invest in the indices. Here is the code to obtain that data and the following graph representing their performance since 2005:\n\n\nCode\nprice_data <- tq_get(c(\"IWM\", \"SPY\"), from = \"2005-01-01\") %>% \n    select(symbol, date, adjusted) %>% \n    mutate(name = ifelse(symbol == \"IWM\", \"Russell 2000 ETF\", \"SP500 ETF\"))\n\nprice_data %>% \n    group_by(symbol) %>% \n    mutate(adjusted = adjusted / first(adjusted)) %>% \n    ungroup() %>% \n    ggplot(aes(date, adjusted, color = name)) +\n    geom_line() +\n    theme_bw() +\n    scale_color_grey() +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    labs(y = \"Portfolio Wealth ($1)\",\n         x = \"\",\n         color = \"\") +\n    theme(legend.position = \"top\", text = element_text(size = 15))\n\n\n\n\n\nNow that we have this data, let’s compare the performance of the ETFs to that of our companies. In order to do this, we need to establish an investment horizon that makes sense for our companies. In other words, should we pretend that we invest in our companies for a day? 3 days? A week? A Year? etc. Let’s investigate this:\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date, symbol) %>% \n    mutate(days_after_screen = row_number() - 1,\n           price_index = price / first(price)) %>% \n    ungroup() %>% \n    filter(days_after_screen <= 60) %>% \n    group_by(days_after_screen) %>% \n    summarize(average_return = mean(price_index, na.rm = T) - 1) %>% \n    ggplot(aes(days_after_screen, average_return)) +\n    geom_col() +\n    theme_bw() +\n    labs(\n        title = \"Average Return vs. Investment Horizon\",\n        y = \"Average Return\",\n        x = \"Investment Horizon (in Days)\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format(), n.breaks = 6) +\n    scale_x_continuous(n.breaks = 7) +\n    theme(text = element_text(size = 15))\n\n\n\n\n\nWe can already tell from the data that management clearly has some inside information that the markets are oblivious to; the average return for each of these companies after just 5 days is approximately 0.5%. This may not sound like much at first, but if you stop to do some simple calculations, you will realize that if you invest $1 at this rate, and compound your investment at a weekly (5 day) frequency over 52 weeks, then you will have approximately $1.3 at the end of the year. This equates to a 30% yearly return while the SP500 averages 10%.\nMoving forward, lets continue with a hypothetical investment horizon of a week (5 days) as this will deliver the most ‘bang for our buck’. While the 60 day average return is approximately 2.5%, we will lose out on the benefits from a shorter compounding frequency. Moreover, a week is a clean frequency to work with, and it matches nicely with the fact that our screens are run at a weekly frequency.\nNow that we have established our ‘investment horizon,’ let’s pretend that we invest equally in these companies each week and compare our performance to that of the SP500 and the Russell 2000:\n\nAggregateYearly\n\n\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date, symbol) %>% \n    mutate(days_after_screen = row_number() - 1,\n           price_index = price / first(price)) %>% \n    ungroup() %>% \n    filter(days_after_screen == 5) %>% \n    group_by(screen_date) %>% \n    summarize(portfolio_index = mean(price_index, na.rm = T)) %>% \n    left_join(\n        price_data %>% \n    mutate(date = floor_date(date, unit = \"weeks\") - days(2)) %>% \n    group_by(name, date) %>% \n    summarize(return = adjusted/first(adjusted)) %>% \n    slice_tail() %>% \n    ungroup() %>% \n    pivot_wider(names_from = name, values_from = return) %>% \n    janitor::clean_names(),\n        by = c(\"screen_date\" = \"date\")\n    ) %>% \n    mutate(across(-screen_date, .fns = cumprod)) %>% \n    pivot_longer(-screen_date) %>% \n    \n    ggplot(aes(screen_date, value, color = name)) +\n    geom_line() +\n    theme_bw() +\n    scale_color_grey() +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    labs(\n        y = \"Portfolio Wealth Index ($1)\",\n        color = \"\",\n        x = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 15))\n\n\n\n\n\n\n\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date, symbol) %>% \n    mutate(days_after_screen = row_number() - 1,\n           price_index = price / first(price)) %>% \n    ungroup() %>% \n    filter(days_after_screen == 5) %>% \n    group_by(screen_date) %>% \n    summarize(portfolio_index = mean(price_index, na.rm = T)) %>% \n    left_join(\n        price_data %>% \n    mutate(date = floor_date(date, unit = \"weeks\") - days(2)) %>% \n    group_by(name, date) %>% \n    summarize(return = adjusted/first(adjusted)) %>% \n    slice_tail() %>% \n    ungroup() %>% \n    pivot_wider(names_from = name, values_from = return) %>% \n    janitor::clean_names(),\n        by = c(\"screen_date\" = \"date\")\n    ) %>% \n    mutate(year = year(screen_date)) %>% \n    group_by(year) %>% \n    mutate(across(-screen_date, .fns = cumprod)) %>% \n    slice_tail() %>% \n    ungroup() %>% \n    select(-screen_date, -russell_2000_etf) %>%\n    pivot_longer(-year) %>% \n    mutate(value = value - 1) %>% \n    ggplot(aes(year, value, fill = name)) +\n    geom_col(position = \"dodge\") +\n    theme_bw() +\n    scale_fill_grey() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(\n        y = \"Return (%)\",\n        x = \"\",\n        fill = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 15))\n\n\n\n\n\n\n\n\nIt is obviously apparent, from the above, that following management trading can provide superior investment returns…\n\n\nFinal Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "research/00_high_yield_analysis/index.html",
    "href": "research/00_high_yield_analysis/index.html",
    "title": "High Yield Debt vs. Equities",
    "section": "",
    "text": "Intro\nIn this article we will investigate:\n\nThe time periods where High Yield Debt outperformed Equities\nWhy these time periods occurred\nWhere both of these asset classes may be heading over the next few years\n\nLet’s load the data:\n\nDataVariable DefinitionsPlot\n\n\n\n\nCode\nhigh_yield_data_filled %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(across(.cols = c(2,3,4,6,8), .fns = ~scales::percent(.x, accuracy = .01, scale = 1))) %>% \n    mutate(across(.cols = c(5,7), .fns = ~scales::number(.x, accuracy = 1))) %>% \n    head() %>% \n    rename(Date = date) %>% \n    gt()\n\n\n\n\n\n\n  \n  \n    \n      Date\n      Inflation\n      Coupon Rate\n      Federal Funds Rate\n      High Yield Index\n      High Yield Spread\n      SP500 Index\n      Yield to Worst\n    \n  \n  \n    1997-12-31\n1.70%\n8.29%\n5.56%\n527\n2.96%\n970\n8.88%\n    1998-01-01\n1.70%\n8.28%\n5.56%\n527\n2.99%\n973\n8.87%\n    1998-01-02\n1.70%\n8.28%\n5.56%\n527\n3.02%\n975\n8.87%\n    1998-01-03\n1.70%\n8.28%\n5.56%\n528\n3.04%\n976\n8.87%\n    1998-01-04\n1.70%\n8.27%\n5.56%\n528\n3.07%\n976\n8.86%\n    1998-01-05\n1.70%\n8.27%\n5.55%\n528\n3.09%\n977\n8.86%\n  \n  \n  \n\n\n\n\n\n\n\n\n\nSP500 Index\nA stock market index composed of 500 large companies traded on U.S. stock exchanges. Each constituent’s weight in the index is proportional to its market capitalization.\n\n\nHigh Yield Index\nA bond index composed of high yield debt - corporate bonds with an investment grade of BB and below.\n\n\nHigh Yield Spread\nThe yield difference between High Yield Debt and U.S. Treasuries (U.S. Government Debt). For specific information on this series and how it is calculated please visit the FRED.\n\n\nFederal Funds Rate\nThe rate at which major U.S. banks lend their ‘federal funds’/reserve balances to each other overnight. This is generally the rate referred to when people mention ‘the interest rate’.\n\n\nCoupon Rate\nThe annual rate of income received by an investor for holding a bond. A bond with a 5% coupon rate and face value of $100 will result in annual income receipts of $5.\n\n\nYield to Worst\nThe lowest possible yield that can be earned by a bond that fully adheres to its contracted terms (perhaps a bond has a callable provision etc.). This measure does not include default risk.\n\n\nInflation\nThe yearly rate of increase in prices.\n\n\n\n\n\n\n\nCode\nhigh_yield_data %>% \n    filter(date >= min_date) %>% \n    plot_time_series(10) +\n    theme(text = element_text(size = 16))\n\n\n\n\n\nShaded regions indicate a time period when High Yield spreads exceeded 10%\n\n\n\n\n\n\n\n\n\nPerformance Comparison\nLet’s visualize how Equities have performed relative to High Yield Debt:\n\nAggregateYearly\n\n\n\n\nCode\nhigh_yield_data %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    group_by(symbol) %>% \n    arrange(date) %>% \n    mutate(value = value / first(value)) %>% \n    ungroup() %>% \n    ggplot(aes(date, value, color = name)) +\n    geom_line() +\n    theme_bw() +\n    labs(\n        color = \"\",\n        y = \"Wealth Index ($1)\",\n        x = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 16))\n\n\n\n\n\nWe can see that, since 1984, the performance of Equities relative to High Yield Debt has been roughly comparable, with Equities slightly outperforming. Equities returned approximately 24x and High Yield Debt returned 20x. However, let’s see if we can identify the time periods where High Yield Debt outperformed Equities, and why.\n\n\n\n\nCode\nyearly_performance_tbl <- high_yield_data %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(year = year(date)) %>% \n    group_by(name) %>% \n    arrange(date) %>% \n    ungroup() %>% \n    group_by(name, year) %>% \n    summarize(pct_ret = (last(value) / first(value)) - 1) %>% \n    ungroup()\n\ndates_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(difference > 0) %>% \n    pull(year)\n\nyearly_performance_tbl %>% \n    ggplot() +\n    geom_col(data = yearly_performance_tbl,\n             mapping = aes(year, pct_ret, fill = name),\n             position = \"dodge\", color = \"black\") +\n    geom_rect(\n        data = tibble(start_date = c(dates_vec - .5),\n                      end_date = c(dates_vec + .5)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .3, fill = \"grey\") +\n    theme_bw() +\n    scale_fill_brewer(direction = 1) +\n    labs(x = \"\",\n         y = \"Return (%)\",\n         fill = \"\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_continuous(guide = guide_axis(angle = 45), n.breaks = 12) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nShaded regions indicate years where High Yield Debt ourperformed Equities\n\n\n\n\nFrom the above, we can see there seems to be cyclicality in the performance of High Yield Debt relative to Equities, with alternating 2-4 year periods of outperformance followed by 1-3 year periods of underperformance.\n\n\n\n\n\nKey Takeaways\nIn order to better understand the logical cause-and-effect relationships at play between our variables, let’s re-visualize our yearly performance comparison, but this time we will add our other relevant variables into the mix and attempt to find helpful trends:\n\nActualSmooth\n\n\n\n\nCode\ndates_positive_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(`High Yield Index` > 0 & difference > 0) %>% \n    pull(year)\n\ng3 <- high_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_line(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name),\n        size = .75,\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\ng3\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\n\nCode\nhigh_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>%\n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_smooth(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name), se = F\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\nHere are some primary takeaways from the visuals:\n\nOutperformance generally occurs when:\n\n‘Interest Rates’ are falling\nYield Spreads and the YTW are elevated\n\nEspecially in relation to the 2-5 years prior\n\nPreceeding periods are associated with economic bubbles\n\n1990 Oil Shock / Recession\n2001 Dot Com Bubble\n2008 Great Financial Crisis\n2022 Covid/Tech Bubble\n\n\nPositive outperformance occurs later in the period\n\n\n\nBuilding a Simple Narrative\nObviously, these broad points are interrelated; let’s try to build an extremely simple narrative that captures these relationships:\n\n\nThis narrative is by no means detailed, nor complete. In, fact it can be superbly incorrect, but that is for the reader to judge.\nDuring prolonged periods of strong stock market upswings, especially those wherein current growth exceeds productive growth, market participants bet on good times continuing - typically levering up in the process. This makes complete sense; why finance growth with equity if it is performing so well? Better to issue debt instead… Investors and banks are happy to accept the debt; these companies are performing well (too well). But, just like it’s not a good idea to push yourself past physical capacity for extended periods of time, it is not good for a company to grow faster than its economic potential for extended periods of time; eventually something breaks.\nDuring this initial period of hurt, both equity holders and debt holders get whacked, valuations drop and the first bankruptcies occur. However, since debt holders get paid first, they get hurt slightly less than equity holders. Now, we are at a critical point in time… investors start to panic and make some back of the napkin calculations… and conclude that at current rates, debt is no longer a profitable investment, so they demand higher yields. Notice that YTW and High Yield Spreads briefly shoot up during these periods (1990, 2001, 2008…). However, the market seems to systematically underestimate two important things: 1) the Federal Reserve’s willingness to supply the proverbial medicine by lowering interest rates and 2) the associated aid from lower interest rates.\nWe can notice from our graph that, after the tough first year where both asset classes perform badly, the Fed steps in and stimulates the economy by lowering the cost of borrowing (… we also notice that yields will quickly follow suit). Naturally, companies are now able to negotiate better deals on borrowing money, so they take on more debt at a lower rate and use it to service their older, more expensive debt. Obviously, these companies are kicking the can down the road, and we can see that problems will occur when the Fed can no longer lower interest rates, but for now, this narrative sort of makes sense. Notice the prolonged periods of High Yield’s outperformance occur approximately a year after the initial crash (1990, 2001, 2008) for 2-4 years because investors were able to invest when yields were at their peak, and the Fed stimulated the economy shortly thereafter.\nNow, we flip back to the origin of the story… High Yield Debt is starting to see a prolonged period of strong performance… things seem less risky and therefore yields decline and the pendulum starts to swing in favor of equities again…\n\nA Note from OpenAI’s ChatGPT\nWhen tasked with the following - “Identify time periods where High Yield Debt outperformed Equities. Give a hypothesis for why it outperforms during periods of high inflation and economic downturns.” - OpenAI’s model gave the following answer:\n\n“High Yield Debt has historically outperformed Equities during periods of economic downturns, such as the 2008 Financial Crisis and the 2020 Coronavirus pandemic. Additionally, High Yield Debt has outperformed Equities during periods of rising inflation, such as the 1970s and 1980s.\nThe reason why High Yield Debt outperforms Equities during periods of high inflation and economic downturns is because it offers a higher yield than Equities, which can help to offset the effects of inflation and provide a more stable return in a volatile market. Additionally, High Yield Debt is generally less volatile than Equities, making it a more attractive investment during periods of economic uncertainty. Finally, High Yield Debt is typically less sensitive to changes in interest rates, which can be beneficial during periods of rising inflation.”\n- ChatGPT\n\n\n\n\nGoing Back Even Further\nWhile we don’t have data on a High Yield Bond Index, its Yield Spread, YTW, and Coupon Rate, before 1984, Aswath Damadoran, a professor at NYU known for gathering and sharing financial data, published the following helpful dataset:\n\n\nCode\nread_excel(here(\"raw_data\", \"High Yield Analysis\", \"damadoran_return_data.xlsx\")) %>% \n    select(-Inflation) %>% \n    head() %>% \n    gt() %>% \n    gt::tab_header(title = \"Yearly Real Returns\") %>% \n    gt::fmt_percent(-year) %>% \n    gt::tab_footnote(footnote = \"All returns are stated in real terms (i.e. adjusted for inflation)\")\n\n\n\n\n\n\n  \n    \n      Yearly Real Returns\n    \n    \n  \n  \n    \n      year\n      SP500\n      3 Month Treasury Bill\n      10 Year Treasury Bond\n      Baa Corporate Bonds\n      Real Estate\n    \n  \n  \n    1928\n45.49%\n4.29%\n2.01%\n4.43%\n2.68%\n    1929\n−8.83%\n2.56%\n3.60%\n2.42%\n−2.63%\n    1930\n−20.01%\n11.69%\n11.68%\n7.41%\n2.24%\n    1931\n−38.07%\n12.82%\n7.45%\n−7.02%\n1.29%\n    1932\n1.82%\n12.64%\n21.25%\n37.74%\n−0.21%\n    1933\n48.85%\n0.20%\n1.08%\n12.11%\n−4.54%\n  \n  \n  \n    \n       All returns are stated in real terms (i.e. adjusted for inflation)\n    \n  \n\n\n\n\nLet’s treat Baa Corporate Bonds as a proxy for ‘High Yield’ Debt and visualize its performance relative to the SP500 since 1928.\n\n\nCode\ndates_vec <- damadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(difference = `Baa Corporate Bonds` - SP500) %>% \n    filter(difference >= 0) %>% \n    pull(date)\n\ndates_positive_vec <- damadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(difference = `Baa Corporate Bonds` - SP500) %>% \n    filter(difference >= 0) %>% \n    filter(`Baa Corporate Bonds` > 0) %>% \n    pull(date)\n\ndamadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>% \n            mutate(end_date = end_date + years(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"grey\", alpha = .6\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(end_date = end_date + years(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"purple\", alpha = .15\n    ) +\n    geom_col(aes(date, value, fill = name), position = \"dodge\") +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(size = 16)) +\n    labs(y=\"\",x=\"\",fill=\"\") +\n    scale_y_continuous(labels = scales::percent_format(), n.breaks = 12) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year)\n\n\n\n\n\nThe above chart, while limited in its usefulness, provides us with some possible amendments to our key takeaways:\n\nOur observation that positive outperformance occurs later in the outperformance period does not seem to hold well during the 1929-1934 & 1940-1943 time-frames.\nThere appears to be marginally less cyclicality in outperformance.\nMost importantly, the above solidifies our claim that High Yield Outperforms during periods associated with financial bubbles (1929, 1940, etc.).\n\n\n\nApplying the Key Takeaways and Narrative\nLet’s attempt to codify our narrative and key takeaways, so that we can isolate periods of strong High Yield Debt returns (and hopefully identify future ones as well). I will tell the computer to go through the data and highlight a region if:\n\nthe current YTW is 30% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Yield Spread is 75% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Federal Funds Rate is 33% lower than its 3 Year Moving Average w/ a 2-year lag\n\nHere is the resulting plot:\n\n\nCode\nytw_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ytw\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 8),\n        .f = mean,\n        na.rm = T,\n        .period = 12,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.3 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nhys_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"hys\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 365 * 2),\n        .f = mean,\n        na.rm = T,\n        .period = 365 * 3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.75 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nffr_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ffr\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 12*2),\n        .f = mean,\n        na.rm = T,\n        .period = 12*3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value < .67 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\ng1 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    bind_rows(\n        tibble(\n            start_date = ffr_signal_dates_vec,\n            end_date = ffr_signal_dates_vec + months(1),\n            type = \"Fed Funds Rate\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    scale_fill_brewer() +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng2 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng3 <- g3 +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ncowplot::plot_grid(g1, g2, g3, ncol = 1, align = \"hv\")\n\n\n\n\n\nPlot 2 is the same as Plot 1, but Fed Funds Rate is removed. Plot 3 is being re-provided for ease of comparison.\n\n\n\n\nAs we can see, these 3 simple metrics, especially the YTW metric, provide a decent signal for periods of strong High Yield Debt returns. And while these metrics have been created with a knowledge of the past, they certainly corroborate the simple narrative we created above. Possible next steps include applying these signals to other developed markets while being mindful of their current position in their interest rate cycle, improving these signals, and assessing hypothetical performance, etc… I leave these as exercises to the reader…\n\n\nThe Future: Where are we heading?\nIn our above research, one of our key takeaways involved the Federal Funds Rate (i.e. the ‘interest’ rate). This should be self-evident as the interest rate measures the cost of borrowing money and is therefore the primary drivers of the economy and financial markets. We noted that during periods of economic hurt (typically after some sort of ‘bubble’) the Fed would lower rates and kick the can down the road. However, we have now reached that special point in time wherein interest rates can no longer be lowered. Instead, the Fed has been forced to aggressively raise rates, subsequently popping the Tech bubble. This makes for an interesting investment environment going forward. We have noted that High Yield Debt performs better during these reactionary, declining interest rate environments, and equities during the post-recovery ‘rising’ rate environment. However, in spite of all this, my outlook for equities is still slightly more pessimistic. Instead, I think that High Yield Debt is likely to outperform over the next 2 years, followed by Equities picking back up…\n\n\nFinal Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "research/02_weather_and_markets/index.html",
    "href": "research/02_weather_and_markets/index.html",
    "title": "Weather & the Stock Market",
    "section": "",
    "text": "In recent years, behavioral finance - the field of study that combines psychology and economics to better understand financial decision making - has grown in popularity. There have been many studies that prove the irrationality of human decision-making processes due to psychological and emotional factors. With this in mind, we will investigate if weather conditions in New York have any noticeable impact on daily stock market returns.\nLet’s load the data…\n\nWeather DataStock Market DataVariable Definitions\n\n\n\n\nCode\nweather_data <- read_rds(here(\"raw_data\", \"Weather and Markets\", \"new_york_weather_data_clean.rds\"))\n\nweather_data %>%\n    # head() %>%\n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>%\n    mutate(across(.cols = where(~is.numeric(.x)), .fns = round)) %>%\n    datatable()\n\n\n\n\n\n\n\n\n\nWe will consider the SP500 Index as a proxy for the stock market:\n\n\nCode\nstock_data <- tq_get(\"^GSPC\", from = \"1978-12-29\")\n\nstock_data <- stock_data %>% \n    mutate(pct_ret = (adjusted / lag(adjusted)) - 1) %>% \n    slice(-1) %>% \n    select(date, pct_ret)\n\nstock_data %>% \n    head() %>% \n    set_names(c(\"Date\", \"Return (%)\")) %>% \n    gt() %>% \n    gt::fmt_percent(columns = 2)\n\n\n\n\n\n\n  \n  \n    \n      Date\n      Return (%)\n    \n  \n  \n    1979-01-02\n0.65%\n    1979-01-03\n1.11%\n    1979-01-04\n0.80%\n    1979-01-05\n0.56%\n    1979-01-08\n−0.33%\n    1979-01-09\n0.54%\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\n\nTod\nThe time of day (Morning, Midday, Afternoon).\n\n\nTemp\nThe temperature in degrees Fahrenheit.\n\n\nVisibility\nThe maximum distance at which an object can clearly be discerned.\n\n\nDew Point\nThe minimum threshold temperature that results in a relative humidity level of 100%.\n\n\nFeels Like\nA measure of how hot/cold it feels like outside when accounting for other variables like wind chill, humidity, etc.\n\n\nTemp Min\nThe minimum temperature during the associated time stamp.\n\n\nTemp Max\nThe maximum temperature during the associated time stamp.\n\n\nPressure\nThe weight of the air. High air pressure (heavy air) is associated with calm weather conditions whereas low air pressure (light air) is associated with active weather conditions.\n\n\nHumidity\nThe amount of water vapor in the air.\n\n\nWind Speed\nThe speed of the wind in miles per hour.\n\n\nWind Deg\nThe direction of the wind in circular degrees.\n\n\nClouds All\nCloudiness of the sky in percent.\n\n\nWeather Id\nThe ID code associated with the weather.\n\n\nWeather Main\nThe Primary Weather Category.\n\n\nWeather Description\nThe Secondary Weather Category.\n\n\nWeather Icon\nThe ID code of the icon being displayed on weather apps."
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#sp500-returns",
    "href": "research/02_weather_and_markets/index.html#sp500-returns",
    "title": "Weather & the Stock Market",
    "section": "SP500 Returns",
    "text": "SP500 Returns\n\n\nCode\navg_ret <- stock_data %>% \n    summarize(avg_ret = mean(pct_ret)) %>% \n    pull(avg_ret)\n\nstock_data %>% \n    ggplot(aes(date, pct_ret)) +\n    geom_point(alpha = .5) +\n    geom_hline(yintercept = avg_ret, color = \"red\") +\n    theme_bw() +\n    labs(\n        y = \"\", x = \"\",\n        title = \"SP500 Daily Return (%)\",\n        subtitle = str_glue(\"Average: {scales::percent(avg_ret, accuracy = .0001)}\")\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size=15))\n\n\n\n\n\nAs we can see from the data, there are several days with extreme returns; on October 19, 1987 (‘Black Monday’) the market declined by approximately 22%, and in March of 2020, the stock market dipped when news of the Covid-19 pandemic arose. While these events are extremely important from a historical perspective, it is unlikely that the weather contributed significantly to these extreme returns. Therefore, we will consider days like these to be outliers, and we will remove them from our data. Here is a cleaned version of the data:\n\n\nCode\nstock_summary <- stock_data %>% \n    summarize(\n        mean = mean(pct_ret, na.rm = T),\n        st_dev = sd(pct_ret, na.rm = T)\n    )\n\nstock_data %>% \n    mutate(is_outlier = case_when(\n        pct_ret > stock_summary$mean + 2*stock_summary$st_dev ~ \"Outlier\",\n        pct_ret < stock_summary$mean - 2*stock_summary$st_dev ~ \"Outlier\",\n        T ~ \"Not Outlier\"\n    )) %>% \n    ggplot(aes(date, pct_ret, color = is_outlier)) +\n    geom_point() +\n    theme_bw() +\n    labs(\n        y = \"\", x = \"\",\n        title = \"SP500 Daily Return (%)\",\n        color = \"\"\n    ) +\n    scale_color_hue(direction = -1) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size=15), legend.position = \"top\")\n\n\n\n\n\nCode\nstock_data <- stock_data %>% \n    mutate(is_outlier = case_when(\n        pct_ret > stock_summary$mean + 2*stock_summary$st_dev ~ \"Outlier\",\n        pct_ret < stock_summary$mean - 2*stock_summary$st_dev ~ \"Outlier\",\n        T ~ \"Not Outlier\"\n    )) %>% \n    filter(is_outlier == \"Not Outlier\") %>% \n    select(-is_outlier)\n\n\nGoing forward, we will solely use the blue data points…"
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#the-impact-of-the-weather",
    "href": "research/02_weather_and_markets/index.html#the-impact-of-the-weather",
    "title": "Weather & the Stock Market",
    "section": "The Impact of the Weather",
    "text": "The Impact of the Weather\nLet’s examine the returns on days with different morning weather conditions for each month:\n\nJanFebMarAprMayJunJulAugSepOctNovDec\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\nAs we can see from the above plots, each of the distributions are relatively similar for different morning weather conditions. Therefore, morning weather seems to have little effect on the distribution of daily stock market returns.\nLet’s investigate if temperature differences have any impact on market returns…"
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#the-impact-of-temperature-differences",
    "href": "research/02_weather_and_markets/index.html#the-impact-of-temperature-differences",
    "title": "Weather & the Stock Market",
    "section": "The Impact of Temperature Differences",
    "text": "The Impact of Temperature Differences\nLet’s hypothesize that on days where it is colder than usual, returns are worse than days where it is warmer than usual. To quantify this hypothesis, let’s see if the difference of Feels Like from that month’s average Feels Like yields any interesting results on stock market returns:\n\nJanFebMarAprMayJunJulAugSepOctNovDec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLikewise, there is no evidence that variations in temperature can help explain daily stock returns."
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#simple-modelling---linear-regression",
    "href": "research/02_weather_and_markets/index.html#simple-modelling---linear-regression",
    "title": "Weather & the Stock Market",
    "section": "Simple Modelling - Linear Regression",
    "text": "Simple Modelling - Linear Regression\nFrom our brief analysis above, it seems unlikely that we will be able to use weather data to model stock market returns accurately, but let’s run through a quick linear regression and examine the results.\n\n\nCode\ndata_prep <- data %>% \n    left_join(feels_like_summary) %>% \n    mutate(feels_like_difference = feels_like - avg_feels_like) %>% \n    select(-avg_feels_like) %>% \n    filter(tod == \"Morning\") %>% \n    filter(!is.na(pct_ret)) %>% \n    mutate(wday = wday(date, label = T)) %>% \n    select(date, month, wday, everything(), -tod, -weather_id, -weather_icon)\n\nlm_output <- data_prep %>% \n    select(pct_ret, month, weather_main, feels_like, feels_like_difference) %>% \n    lm(formula = pct_ret ~ . - 1) %>% \n    summary()\n\nlm_output %>% \n    broom::glance() %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>%\n    mutate(across(.cols = where(~is.numeric(.x)), .fns = ~round(.x, digits = 4))) %>% \n    gt()\n\n\n\n\n\n\n  \n  \n    \n      R.squared\n      Adj.r.squared\n      Sigma\n      Statistic\n      P.value\n      Df\n      Df.residual\n      Nobs\n    \n  \n  \n    0.0068\n0.0048\n0.0081\n3.298\n0\n22\n10544\n10567\n  \n  \n  \n\n\n\n\n\n\nCode\nlm_output %>% \n    broom::tidy() %>% \n    arrange(p.value) %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>%\n    mutate(across(.cols = where(~is.numeric(.x)), .fns = ~round(.x, digits = 3))) %>%\n    DT::datatable()\n\n\n\n\n\n\n\nOnce again, we confirm that the weather cannot help explain variation in daily stock returns (with our model only explaining .7%). In fact, the month of the year seems to be more significant than the weather when explaining daily stock return variation."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nWeather & the Stock Market\n\n\n\nEquities\n\n\n\nRead Time: 5-10 mins\n\n\n\nMax Sands\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManagement Trading\n\n\n\nEquities\n\n\n\nRead Time: 5-10 mins\n\n\n\nMax Sands\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHigh Yield Debt vs. Equities\n\n\n\nMacro\n\n\nHigh Yield Debt\n\n\n\nRead Time: 15-25 mins\n\n\n\nMax Sands\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "2022 - Present\nCertified:\n\nData Science for Business w/ R\nML & High Performance Time Series Forecasting w/ R\nPredictive Shiny Web Applications w/ R\nShiny Development w/ R & AWS (in progress)\n\n\n\n\n2019 - 2022\nDesautels Faculty of Management, Montréal, Canada\nBachelor of Commerce, Major in Finance, Double Concentration in Accounting & Statistics"
  },
  {
    "objectID": "resume.html#choate-hall-stewart-choate-investment-advisors",
    "href": "resume.html#choate-hall-stewart-choate-investment-advisors",
    "title": "",
    "section": "Choate Hall & Stewart / Choate Investment Advisors",
    "text": "Choate Hall & Stewart / Choate Investment Advisors\nBusiness & Investment Analyst | 2022 - Present | Boston, MA\n\nXXXXX\nXXXXX\n\nXXXXX\n\nXXXXX\nXXXXX\n\nXXXXX"
  },
  {
    "objectID": "resume.html#mcgill-university-1",
    "href": "resume.html#mcgill-university-1",
    "title": "",
    "section": "McGill University",
    "text": "McGill University\nTeaching Assistant | 2021 - 2022 | Montréal, Canada\n\nDelivering lectures, mentoring/tutoring students, and grading all assessments - typically reserved for Graduate students"
  },
  {
    "objectID": "resume.html#ironhold-capital",
    "href": "resume.html#ironhold-capital",
    "title": "",
    "section": "IronHold Capital",
    "text": "IronHold Capital\nInvestment Analyst & Executive Assistant | 2020 - 2021 | New York, United States\n\nAided in raising capital, marketing efforts, reviewing legal contracts, and meeting with Family Offices & HNIs on behalf of the CEO\nWorked directly under the CIO and conducted research on Indian and U.S. Equities - the fund followed a Value-based strategy\nAuthor of the fund’s newsletter that covered key macroeconomic events, political news, and Family Office industry insights"
  },
  {
    "objectID": "resume.html#private-tutor",
    "href": "resume.html#private-tutor",
    "title": "",
    "section": "Private Tutor",
    "text": "Private Tutor\n2018 - 2021\n\nTutored students in Mathematics, Statistics, Physics, English, and Finance, and revised college admission essays"
  },
  {
    "objectID": "resume.html#replicating-ray-dalios-work-forecasting-the-big-debt-cycles",
    "href": "resume.html#replicating-ray-dalios-work-forecasting-the-big-debt-cycles",
    "title": "",
    "section": "Replicating Ray Dalio’s Work: Forecasting the ‘Big’ & ‘Debt’ Cycles",
    "text": "Replicating Ray Dalio’s Work: Forecasting the ‘Big’ & ‘Debt’ Cycles\n2022 - Present\n\nBuilding a system that continuously retrieves data from various sources and uses Machine Learning to predict several countries’ current and future stages of their ‘Big’ and ‘Debt’ Cycles, with the eventual goal of predicting asset returns across classes, sectors, and geographies"
  },
  {
    "objectID": "resume.html#insider-trading---scraping-sec-form-4-filings",
    "href": "resume.html#insider-trading---scraping-sec-form-4-filings",
    "title": "",
    "section": "Insider Trading - Scraping SEC Form 4 Filings",
    "text": "Insider Trading - Scraping SEC Form 4 Filings\n2022 - Present\n\nWrote code in R that scrapes the SEC Edgar site and all of its Form 4 filings. I am currently developing a trading strategy that incorporates this data."
  },
  {
    "objectID": "resume.html#coding-w-bloomberg-automating-equity-screens-rough-draft-dcfs",
    "href": "resume.html#coding-w-bloomberg-automating-equity-screens-rough-draft-dcfs",
    "title": "",
    "section": "Coding w/ Bloomberg: Automating Equity Screens & Rough Draft DCFs",
    "text": "Coding w/ Bloomberg: Automating Equity Screens & Rough Draft DCFs\n2022\n\nWrote code in R that connects to and scrapes information from a Bloomberg Terminal by utilizing its Desktop API\n\nAutomated Bloomberg’s EQS function so that I could run continuous equity screens (i.e. every ‘Saturday’ since MM/DD/YY)\nUsed scraped information to automate the generation of a ‘rough draft’ DCF for any company; companies with a significant discrepancy between ‘rough draft’ implied share price and 7-day VWAP would garner additional, refined analysis"
  },
  {
    "objectID": "resume.html#coding-w-amazon-web-services-aws-gemini-kraken-automating-cryptocurrency-trading-strategy",
    "href": "resume.html#coding-w-amazon-web-services-aws-gemini-kraken-automating-cryptocurrency-trading-strategy",
    "title": "",
    "section": "Coding w/ Amazon Web Services (AWS), Gemini, & Kraken: Automating Cryptocurrency Trading Strategy",
    "text": "Coding w/ Amazon Web Services (AWS), Gemini, & Kraken: Automating Cryptocurrency Trading Strategy\n2022\n\nWrote code in Python that connects to Gemini (cryptocurrency exchange) web API and allows me to place limit orders if certain conditions are met; automated trading by running the code externally every 5 minutes through AWS Cloudwatch & AWS Lambda\n\nRepeated the above with Kraken (cryptocurrency exchange) web API to place market orders"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Sands",
    "section": "",
    "text": "Hi All - I’m Max and Welcome to my Site. I’m a Business Analyst with a passion for Data Science and Investment Research. On here, you can find mini Research Articles covering these topics; feel free to reach out via LinkedIn if they pique your interest, you would like to collaborate, or if you have questions. Cheers :)\n\n  \n    \n\n    \n  \n    \n     Instagram\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\n\nMcGill University | Finance | 2019 - 2022\n\n\n\nChoate Investment Advisors | Business Analyst | 2022 - Present\nMcGill University | T.A. - Adv. Bus. Stats | 2021 - 2022\nIronHold Capital | Investment Analyst | 2020 - 2021"
  },
  {
    "objectID": "index.html#current-projects",
    "href": "index.html#current-projects",
    "title": "Max Sands",
    "section": "Current Projects",
    "text": "Current Projects\nReplicating Ray Dalio’s Work: Ray Dalio is personal idol of mine because he is simultaneously an economist, data scientist, and narrator, but, above all else, he exemplifies the power of ‘first-principles’ thinking. He is able to synthesize centuries of history and data into logical, measurable, cause-and-effect relationships. I am currently attempting to build a system that forecasts major countries’ “Big” and “Debt” Cycles using Machine Learning (see Ray Dalio’s Principles for Dealing with With the Changing World Order and Principles for Navigating Big Debt Crises).\nInsider Trading - Scraping SEC Form 4 Filings: Wrote code in R that scrapes the SEC Edgar site and all of its Form 4 filings. I am currently developing a trading strategy that incorporates this data.\nIf you are interested about these projects, would like to collaborate, or would like to request data I’ve gathered, please reach out via LinkedIn - thank you."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html",
    "href": "instructional_articles/00_r_basics/index.html",
    "title": "R in 5 Minutes",
    "section": "",
    "text": "The purpose of this article is 3-fold:\n\nto demonstrate the basics of R as concisely as possible so that you can get up and running on your own projects, even if you’ve had no exposure to coding.\nto act as a basic guide for the non-technical readers interested in following my Research Articles at a more granular level.\nto familiarize myself with the process of writing and explaining topics before I publish my research (and to make sure that my website is working…)\n\n\n\nI would quickly like to explain my background and why I think it is important to have a basic knowledge of ‘coding’:\nI am a Business & Investment Analyst, and 9 months ago I had absolutely no knowledge of ‘coding’; my technical ability was comparable to that of your average dog. I can now tell you 9 months in that understanding the basics of ‘coding’ goes a very long way.\nFirstly, as long as you do a task correctly the first time in code, you can then automate away that task (and its different variations). Whether its performing the same calculations in an Excel file that your boss sends you every morning, or publishing your company’s quarterly financial statements, the same principle applies.\nSecondly, we are living in a world where data is everywhere, and the ability to code allows one to dig into the data and draw valuable insights from it. For anyone in an analytical position (whether Financial Analyst, Medical Researcher, or CEO), this is extremely important and allows you to stand on the shoulders of giants.\nThirdly, you can leverage tools that others have built. There is so much free code on the web and someone else may have already built a tool or completed a task that you are trying to do. This is extremely helpful.\nLastly, a word of caution: coding is not everything. You can be the world’s greatest coder, but if you lack the ability to build a logical, easily-explainable narrative from data, then your value is limited to the tools that you can build for others. In other words, true value comes from the ability to not only work with data, but also derive meaning from it and think originally.\nOk, that’s all; let’s get into it!"
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "href": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "title": "R in 5 Minutes",
    "section": "The Basics of Data",
    "text": "The Basics of Data\nData is simply a spreadsheet of values, and we would like our data to be in a ‘tidy’ format.\n\nTidy Data\nData is considered tidy when each column represents a variable and each row consists of an observation. Consider the following dataset (and feel free to inspect the code and guess what each line means):\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNotice how this data is tidy; each column represents a variable (price, color, etc.) and each row is an observed diamond. Your goal should be to have your data in this format because it is easy to manipulate."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#gathering-data",
    "href": "instructional_articles/00_r_basics/index.html#gathering-data",
    "title": "R in 5 Minutes",
    "section": "Gathering Data",
    "text": "Gathering Data\nData is typically gathered from an API, a database, or simply an Excel/csv spreadsheet that you may have. For now, we will use a built-in R dataset called diamonds."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "href": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "title": "R in 5 Minutes",
    "section": "Manipulating Data",
    "text": "Manipulating Data\nAs long as data is in a tidy format, there are only a few actions that we need to do when manipulating data:\n\n\n\nfilter\nfilter data according to certain conditions\n\n\nsummarize\nsummarize the data (e.g. finding the average)\n\n\ngroup\ngroup similar observations\n\n\npivot\n‘pivoting’ the data in different ways\n\n\nselect\nselect relevant information\n\n\nmutate\nchanging the data in some fashion\n\n\n\n\nFiltering\nLet’s pretend that we only want to consider diamonds with a carat greater than .7 and a depth greater than 63: (click on the “Code” section)\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56\n2759\n5.81\n5.85\n3.72\n\n\n0.96\nFair\nF\nSI2\n66.3\n62\n2759\n6.27\n5.95\n4.07\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56\n2760\n5.80\n5.75\n3.65\n\n\n0.91\nFair\nH\nSI2\n64.4\n57\n2763\n6.11\n6.09\n3.93\n\n\n0.91\nFair\nH\nSI2\n65.7\n60\n2763\n6.03\n5.99\n3.95\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58\n2764\n5.64\n5.68\n3.60\n\n\n\n\n\n\nLet’s continue to filter down and consider only the subset with a cut of “Very Good”:\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    filter(cut == \"Very Good\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56.0\n2759\n5.81\n5.85\n3.72\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56.0\n2760\n5.80\n5.75\n3.65\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58.0\n2764\n5.64\n5.68\n3.60\n\n\n0.71\nVery Good\nG\nVS1\n63.3\n59.0\n2768\n5.52\n5.61\n3.52\n\n\n0.72\nVery Good\nG\nVS2\n63.7\n56.4\n2776\n5.62\n5.69\n3.61\n\n\n0.75\nVery Good\nD\nSI2\n63.1\n58.0\n2782\n5.78\n5.73\n3.63\n\n\n\n\n\n\nYou will now see that we have from our original 53,940 diamonds, we have filtered down to 1,550 that adhere to our conditions.\nAt this point you may have three questions:\n\nWhat is the %>%?\n\nThis is called a pipe and you can translate it to “and then”. It allows us to perform several operations consecutively. So if we look at the code, we first start with the diamonds dataset by typing diamonds, and then we filter according to carat and depth, and then we filter according to cut. The pipe is extremely useful and it is native to R.\n\nWhat does the head() function do?\n\nIt prints only the first 6 observations, that way you don’t have a table with 50,000 rows on your screen.\n\nWhat if I want to filter down to several different cuts, not just “Very Good”\n\nGreat question, here’s what you would do:\n\n\nCode\ndiamonds %>% \n    filter(cut %in% c(\"Ideal\", \"Premium\")) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.23\nIdeal\nJ\nVS1\n62.8\n56\n340\n3.93\n3.90\n2.46\n\n\n0.22\nPremium\nF\nSI1\n60.4\n61\n342\n3.88\n3.84\n2.33\n\n\n0.31\nIdeal\nJ\nSI2\n62.2\n54\n344\n4.35\n4.37\n2.71\n\n\n\n\n\n\nWe tell R to filter down to the observations where cut matches one of the strings in the vector c(\"Ideal\", \"Premium\"). The c() function creates a vector.\n\n\nSummarizing\nLet’s say we want to summarize the data and find the average diamond price, along with its standard deviation:\n\n\nCode\ndiamonds %>% \n    summarize(avg_price = mean(price),\n              st_dev    = sd(price))\n\n\n\n\n\n\navg_price\nst_dev\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\nNotice that we can take our 50,000+ diamonds and summarize the data down to an average price…\nYou will notice that in the summarize function I start by naming the column I want (avg_price) and then I tell R what to do (find the mean of the price variable/column. The mean() & sd() functions calculate mean and standard deviation respectively). I could just as easily call the columns “thing1” & “thing2”:\n\n\nCode\ndiamonds %>% \n    summarize(thing1 = mean(price),\n              thing2    = sd(price))\n\n\n\n\n\n\nthing1\nthing2\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\n\n\nGrouping\nSummarizing the entire data is important, but let’s say we want to find the average diamond price within each color group…\n\n\nCode\ndiamonds %>% \n    group_by(color) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup()\n\n\n\n\n\n\ncolor\navg_price\n\n\n\n\nD\n3169.954\n\n\nE\n3076.752\n\n\nF\n3724.886\n\n\nG\n3999.136\n\n\nH\n4486.669\n\n\nI\n5091.875\n\n\nJ\n5323.818\n\n\n\n\n\n\nWe can take things a step further and group by color and cut…\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nYou will notice that we now have average price for each color and cut. I also only showed the first 10 rows of output by using the slice() function.\n\n\nPivoting\nPivoting is probably the most complicated of the broad actions I am showing you, but the previous segment allows for a great transition. I decided to show only the first 10 rows of output rather than inundate you with 35 rows, but there must be a better way of showing the output, right? I mean we have letters repeating in the color column. This would make more sense:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    )\n\n\n\n\n\n\ncolor\nFair\nGood\nVery Good\nPremium\nIdeal\n\n\n\n\nD\n4291.061\n3405.382\n3470.467\n3631.293\n2629.095\n\n\nE\n3682.312\n3423.644\n3214.652\n3538.914\n2597.550\n\n\nF\n3827.003\n3495.750\n3778.820\n4324.890\n3374.939\n\n\nG\n4239.255\n4123.482\n3872.754\n4500.742\n3720.706\n\n\nH\n5135.683\n4276.255\n4535.390\n5216.707\n3889.335\n\n\nI\n4685.446\n5078.533\n5255.880\n5946.181\n4451.970\n\n\nJ\n4975.655\n4574.173\n5103.513\n6294.592\n4918.186\n\n\n\n\n\n\nWe tell R to take our 35 row table, and pivot it so that we have a color column followed by columns with the different cuts, wherein each value is the average price.\nThe names_from argument asks us what variable to we want to pivot on (we said ‘cut’ and therefore R took all of the cut values and made them columns). The values_from argument asks us which variable we would like to R to occupy the new columns with (we said ‘avg_price’ and therefore R occupied all of the ‘cells’ in our pivot table with the corresponding values from the avg_price column).\nQuick Tip: hitting the tab key when your cursor is inside of a function’s parentheses will show all of the function’s available arguments (2 of which are names_from and values_from for the pivot_longer() function.)\nImportant Note: You will notice that now we have violated the premise of tidy data. The columns Fair:Ideal are not variables. They are types of “cut” (cut is the variable). For the purposes of coding, and data manipulation, we want our data to be in a tidy format. However, for the purposes of presentation, we typically want our data to be in a ‘wide’ format (hence pivot_wider).\nWe can do the opposite and revert our table back into a ‘long’ format with pivot_longer() :\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols = Fair:Ideal\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\nname\nvalue\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nWe can also rename the columns back to their original names within the pivot_longer() function:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols      = Fair:Ideal,\n        names_to  = \"cut\",\n        values_to = \"avg_price\"\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nThat’s on pivoting…\n\n\nSelecting\nSelecting is straightforward. Here are the first 6 rows of our original dataset:\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nLet’s say we are about to investigate something but we only need price, carat, and cut… then it is best practice to select those variables/columns first (imagine we have thousands of variables/columns…):\n\n\nCode\ndiamonds %>% \n    select(price, carat, cut) %>% \n    head()\n\n\n\n\n\n\nprice\ncarat\ncut\n\n\n\n\n326\n0.23\nIdeal\n\n\n326\n0.21\nPremium\n\n\n327\n0.23\nGood\n\n\n334\n0.29\nPremium\n\n\n335\n0.31\nGood\n\n\n336\n0.24\nVery Good\n\n\n\n\n\n\nWe can also select by omission:\n\n\nCode\ndiamonds %>% \n    select(-x, -y, -z) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n\n\n\n\n\n\nWe can select variables carat through clarity:\n\n\nCode\ndiamonds %>% \n    select(carat:clarity) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\n\n\n\n\n0.23\nIdeal\nE\nSI2\n\n\n0.21\nPremium\nE\nSI1\n\n\n0.23\nGood\nE\nVS1\n\n\n0.29\nPremium\nI\nVS2\n\n\n0.31\nGood\nJ\nSI2\n\n\n0.24\nVery Good\nJ\nVVS2\n\n\n\n\n\n\nAnd again by omission:\n\n\nCode\ndiamonds %>% \n    select(-carat:-clarity) %>% \n    head()\n\n\n\n\n\n\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nVery simple.\n\n\nMutating\nWhat if we want to perform some sort of calculation or change the data in some way? This is the purpose of mutating…\nIn our dataset, we have the variables x, y, z which represent the length, width, and height of the diamond. If we pretend all the diamonds are cubes, we can calculate the cubic volume of each diamond by multiplying the dimensions of each diamond. Let’s do this:\n\n\nCode\ndiamonds %>% \n    select(x:z) %>% \n    mutate(volume = x * y * z) %>% \n    head()\n\n\n\n\n\n\nx\ny\nz\nvolume\n\n\n\n\n3.95\n3.98\n2.43\n38.20203\n\n\n3.89\n3.84\n2.31\n34.50586\n\n\n4.05\n4.07\n2.31\n38.07688\n\n\n4.20\n4.23\n2.63\n46.72458\n\n\n4.34\n4.35\n2.75\n51.91725\n\n\n3.94\n3.96\n2.48\n38.69395\n\n\n\n\n\n\nNotice how mutate() is similar in structure to summarize(); first we tell R what we would like name our new variable/column (“volume”), and then we tell R how to calculate it.\nMutate can also change a current column:\n\n\nCode\ndiamonds %>% \n    mutate(carat = \"Hello World\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\nHello World\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\nHello World\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\nHello World\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\nHello World\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\nHello World\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\nHello World\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNow, all observations of carat are “Hello World”."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "href": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "title": "R in 5 Minutes",
    "section": "Basic Modeling",
    "text": "Basic Modeling\nWe will build a linear model to explain diamond prices. In R, the function to create a linear model is lm():\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ carat) %>% \n    summary()\n\n\n\nCall:\nlm(formula = price ~ carat, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2256.36      13.06  -172.8   <2e-16 ***\ncarat        7756.43      14.07   551.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: < 2.2e-16\n\n\nWe just built a linear model that regressed carat on diamond price. As you can see, we can use a diamond’s caratage to explain 85% of price variation. Our model also tells us that for every 1 unit increase in caratage, diamond prices increases by $7,756 on average.\nHowever, I’m sure you will agree that the output is not visually pleasing. Moreover, it is not easy to manipulate since it is not in a tabular format.\nLet’s, once again, stand on the shoulders of giants and utilize a tool that someone else has built to clean up the output. Just like you installed tidyverse, install the broom package by running install.packages(\"broom\"). Then, load the package by running library(broom).\n\n\nCode\nlibrary(broom)\n\n\nThis time let’s regress price on all other variables and use the tidy() function from the broom package to tidy the output:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    tidy()\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n5753.761857\n396.629824\n14.5066294\n0.0000000\n\n\ncarat\n11256.978307\n48.627509\n231.4940348\n0.0000000\n\n\ncut.L\n584.457278\n22.478150\n26.0011290\n0.0000000\n\n\ncut.Q\n-301.908158\n17.993919\n-16.7783441\n0.0000000\n\n\ncut.C\n148.034703\n15.483328\n9.5609097\n0.0000000\n\n\ncut^4\n-20.793893\n12.376508\n-1.6801098\n0.0929418\n\n\ncolor.L\n-1952.160010\n17.341767\n-112.5698421\n0.0000000\n\n\ncolor.Q\n-672.053621\n15.776995\n-42.5970601\n0.0000000\n\n\ncolor.C\n-165.282926\n14.724927\n-11.2247022\n0.0000000\n\n\ncolor^4\n38.195186\n13.526539\n2.8237221\n0.0047487\n\n\ncolor^5\n-95.792932\n12.776114\n-7.4978145\n0.0000000\n\n\ncolor^6\n-48.466440\n11.613917\n-4.1731348\n0.0000301\n\n\nclarity.L\n4097.431318\n30.258596\n135.4137965\n0.0000000\n\n\nclarity.Q\n-1925.004097\n28.227228\n-68.1967102\n0.0000000\n\n\nclarity.C\n982.204550\n24.151516\n40.6684433\n0.0000000\n\n\nclarity^4\n-364.918493\n19.285011\n-18.9223900\n0.0000000\n\n\nclarity^5\n233.563110\n15.751700\n14.8278029\n0.0000000\n\n\nclarity^6\n6.883492\n13.715100\n0.5018915\n0.6157459\n\n\nclarity^7\n90.639737\n12.103482\n7.4887321\n0.0000000\n\n\ndepth\n-63.806100\n4.534554\n-14.0710870\n0.0000000\n\n\ntable\n-26.474085\n2.911655\n-9.0924516\n0.0000000\n\n\nx\n-1008.261098\n32.897748\n-30.6483316\n0.0000000\n\n\ny\n9.608887\n19.332896\n0.4970226\n0.6191751\n\n\nz\n-50.118891\n33.486301\n-1.4966983\n0.1344776\n\n\n\n\n\n\nYou will notice that I used ‘.’ to tell R ‘all other variables’ rather than type each of them out. More importantly, the output is much cleaner and easier to manipulate.\nHowever, we cannot see the model’s accuracy. For this, we need to use the glance() function from broom:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    glance()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\ndf.residual\nnobs\n\n\n\n\n0.9197915\n0.9197573\n1130.094\n26881.83\n0\n23\n53916\n53940\n\n\n\n\n\n\nNow we have accuracy metrics in a nice format.\nLastly, if we would like to see the model’s fit for each observation, we can use the augment() function from broom (scroll to the right):\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    augment() %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprice\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nx\ny\nz\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n326\n0.23\nIdeal\nE\nSI2\n61.5\n55\n3.95\n3.98\n2.43\n-1346.3643\n1672.3643\n0.0003742\n1130.082\n0.0000342\n1.4801217\n\n\n326\n0.21\nPremium\nE\nSI1\n59.8\n61\n3.89\n3.84\n2.31\n-664.5954\n990.5954\n0.0004133\n1130.097\n0.0000132\n0.8767411\n\n\n327\n0.23\nGood\nE\nVS1\n56.9\n65\n4.05\n4.07\n2.31\n211.1071\n115.8929\n0.0009098\n1130.105\n0.0000004\n0.1025982\n\n\n334\n0.29\nPremium\nI\nVS2\n62.4\n58\n4.20\n4.23\n2.63\n-830.7372\n1164.7372\n0.0004062\n1130.094\n0.0000180\n1.0308641\n\n\n335\n0.31\nGood\nJ\nSI2\n63.3\n58\n4.34\n4.35\n2.75\n-3459.2242\n3794.2242\n0.0007715\n1129.987\n0.0003629\n3.3587358\n\n\n336\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n3.94\n3.96\n2.48\n-1380.4876\n1716.4876\n0.0007230\n1130.081\n0.0000696\n1.5194380\n\n\n\n\n\n\nThe broom package is so useful because it cleans up model output, but more importantly, it can be used with many other (more complex) models."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "href": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "title": "R in 5 Minutes",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nBeing able to visualize data is essential for understanding it; the famous saying “a picture is worth a thousands words” is doubly true in today’s age.\nLet’s start out by plotting diamond price against caratage.\n\nCreating a Canvas\nFirst we need to create a canvas with the ggplot() function:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price))\n\n\n\n\n\nNotice that we start with the diamonds dataset and then we create a canvas with the ggplot() function. The aes() function stands for aesthetic and allows us to pick which variables/columns we want to use in our plot. In this case we tell R that we want to plot carat on the x-axis and price on the y-axis.\n\n\nAdding Geoms\nIn our plot we would like to add dots that represent each data point. In R adding these elements are called geometries (i.e. geoms):\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point()\n\n\n\n\n\nNotice how when creating plots with ggplot, we can no longer use the pipe (%>%). Instead, we use a + sign to add layers to the plot.\nFrom our plot we can tell that there is a clear positive relationship between price and caratage.\n\n\nModifying Geoms\nOur plot contains so many points and it is overwhelming; let’s modify the plot so that the points are more transparent with the alpha argument of geom_point().\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point(alpha = .15, color = \"midnightblue\") +\n    geom_smooth()\n\n\n\n\n\nYou will notice that the points are more transparent and that we also modified their color. We also included a smoother line with geom_smooth().\n\n\nAdding Aesthetics\nUp to now our plot has had only 2 aesthetics (x and y). But, all of the arguments that can be passed to geoms (alpha, color, etc.) are actually aesthetics that can be passed in the main aes() function. This probably sounds confusing but the following code will make much more sense:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth()\n\n\n\n\n\nYou will notice that instead of locally changing the color argument in the geom_point() function, we have put in the main aes() function wherein we set it equal to cut. By doing this, we are telling R that the color of each geometry should be defined by the cut variable/column.\n\n\nFaceting\nOur plot is overwhelming with all the different colors on one canvas so lets create a faceted canvas… Rather than explain in words, the following code should be self evident:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut)\n\n\n\n\n\nThis is called a faceted plot because we have created facets according to the cut variable/column. You will note that we need to put a ~ before the specified variable; this is just how the facet_wrap function works.\nWe can also decide to facet according to some other variables, like so:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~clarity, scales = \"free\")\n\n\n\n\n\nYou will notice that I also supplied the scales argument within the facet_wrap() function which allows each faceted plot to have different x and y scales that fit accordingly. Compare the x and y axes of the ‘VS1’ plot with those of the ‘VVS2’. They have different scales.\n\n\nAdding Labels\nLet’s add labels to our plot…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    )\n\n\n\n\n\n\n\nChanging Theme\nR has some preset plotting themes…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_linedraw()\n\n\n\n\n\n…there are several others.\n\n\nModifying Scales\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw() +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nNotice we converted the axis/scale on the plot to a dollar format…\n\n\nExample of More Plots\nWith these basic tools, you now have the ability to create so many different types of plots to gain insights from your data.\nHere are a few more plots with code to give you a flavor…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram(position = \"dodge\") +\n    theme_bw() +\n    scale_fill_brewer()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_density() +\n    theme_bw() +\n    scale_fill_brewer() +\n    facet_wrap(~cut)\n\n\n\n\n\nThere are other packages that help with creating nice plots… install and load ggridges.\n\n\nCode\nlibrary(ggridges)\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = stat(x))) +\n    geom_density_ridges_gradient(scale = 2) +\n    scale_fill_viridis_c(name = \"Price (in $)\", option = \"C\") +\n    theme_minimal() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = factor(stat(quantile)))) +\n    stat_density_ridges(\n        geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n        quantiles = 4, quantile_lines = TRUE\n    ) +\n    scale_fill_brewer() +\n    theme_linedraw() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCreating Interactive Plots\nWe also have the ability to create interactive plots with the help of a package called plotly. This is another example of the power of open-source coding, which gives us the ability to leverage code that others have built (that we may not have the expertise to create ourselves…). Like we did with the tidyverse, run install.packages(\"plotly\") and then load it into your environment with library(plotly). All we have to do to make a plot interactive, is to save it into our environment using the assignment operator - <-. I am going to save my plot as g and then we have to run ggplotly(g).\nLook at the code below:\n\n\nCode\nlibrary(plotly)\ng <- diamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\nggplotly(g)\n\n\n\n\n\n\nThis is just a taste of the plots that can be generated…"
  },
  {
    "objectID": "instructional_articles.html",
    "href": "instructional_articles.html",
    "title": "Instructional Articles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR in 5 Minutes\n\n\n\nR\n\n\n\nApplication-based Learning\n\n\n\nMax Sands\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]