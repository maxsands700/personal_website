[
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html",
    "title": "★EPS Revision Breadth Analysis",
    "section": "",
    "text": "EPS Revision Breadth is a financial metric that measures the number of upward (positive) and downward (negative) revisions to earnings per share (EPS) estimates by analysts. It is calculated as the difference between the number of positive and negative revisions divided by the total number of revisions made.\nSo, if we consider 30 analysts, 20 of whom revised their earnings estimates upwards, and 10 of whom revised their earnings downwards, this would yield an EPS Revision Breadth of 33%.\nThis metric is important because it provides insight into the market’s confidence in a company’s earnings. A high breadth of positive revisions to EPS estimates indicates that analysts are becoming more optimistic about the company’s future earnings, which can be a sign of increased investor confidence and a potential increase in the company’s stock price. Conversely, a high breadth of negative revisions to EPS estimates can indicate declining investor confidence and a potential decrease in the company’s stock price.\nAs a result, EPS Revision Breadth has the potential to be a useful indicator for investors to assess the market’s sentiment towards a company’s earnings and make informed investment decisions. In the following, we will investigate EPS Revision Breadth for the SP500 as a whole, rather than for specific companies."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#thesis",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#thesis",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Thesis",
    "text": "Thesis\nIdentifying tops and bottoms in EPS Revision Breadth can aid in distinguishing periods of strong vs. weak equity returns."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#data-overview",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#data-overview",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Data Overview",
    "text": "Data Overview\n\n\nCode\ndata_raw %>% \n    head() %>% \n    set_names(c(\"date\", \"EPS Revision Breadth\", \"SP500 Price\")) %>% \n    gt(rowname_col = \"date\") %>% \n    fmt_percent(columns = 2, decimals = 1) %>% \n    fmt_currency(columns = 3, decimals = 0)\n\n\n\n\n\n\n  \n  \n    \n      \n      EPS Revision Breadth\n      SP500 Price\n    \n  \n  \n    2023-01-30\n−15.2%\n$4,018\n    2023-01-27\n−15.7%\n$4,071\n    2023-01-26\n−15.4%\n$4,060\n    2023-01-25\n−15.1%\n$4,016\n    2023-01-24\n−13.8%\n$4,017\n    2023-01-23\n−13.7%\n$4,020"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#eps-revisions-identifying-tops-bottoms",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#eps-revisions-identifying-tops-bottoms",
    "title": "★EPS Revision Breadth Analysis",
    "section": "EPS Revisions: Identifying Tops & Bottoms",
    "text": "EPS Revisions: Identifying Tops & Bottoms\nSince this data is cyclical and stationary, we can attempt to identify tops and bottoms, most simply, by partitioning the data into buckets based on its EPS Revision Breadth Value. The following chart demonstrates this by highlighting the top 10% of EPS RB in orange, and the bottom 10% in blue:\n\n\nCode\ndata_raw_bucketed <- data_raw %>% \n    mutate(quantile = ntile(eps_revision_breadth, n = 10)) %>% \n    arrange(date)\n\nmajor_troughs_tbl <- tibble(\n    start_date = c(ymd(\"2008-09-01\"), ymd(\"2020-03-01\")),\n    end_date   = c(ymd(\"2009-06-01\"), ymd(\"2020-07-10\"))\n)\n\n\nggplot() +\n    geom_line(\n        data = data_raw_bucketed,\n        mapping = aes(date, eps_revision_breadth)\n    ) +\n    geom_point(\n        data = data_raw_bucketed %>% \n            filter(quantile == 10),\n        mapping = aes(date, eps_revision_breadth),\n        color = \"darkorange\"\n    ) +\n    geom_point(\n        data = data_raw_bucketed %>% \n            filter(quantile == 1),\n        mapping = aes(date, eps_revision_breadth),\n        color = \"midnightblue\"\n    ) +\n    geom_rect(\n        data = major_troughs_tbl,\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"darkred\",\n        alpha = .3\n    ) +\n    labs(\n        x = \"\",\n        y = \"\",\n        title = \"EPS Revision Breadth (%)\"\n    ) +\n    theme_tq() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size = 16))\n\n\n\n\n\nMajor Troughs are shaded in red\n\n\n\n\nAs we can see from the plot, highlighting the top and bottom 10% of values adequately identifies the peaks and troughs in the time series. However we clearly need to address the fact that the troughs similar to those identified in 2015 are systematically different from those that occur in 2008 and 2020 (shaded in red). We will therefore categorize troughs into two groups, which should be treated and analyzed differently:\n\nMinor Troughs that arise as a natural artifact of the business cycle\nMajor Troughs that arise due to periods of extreme economic turbulence, typically after the popping of a financial bubble\n\nThe ability to distinguish between these two types of troughs is essential because it is unlikely that returns before, during, and after Major Troughs are similar to those for Minor troughs.\nWhile we won’t dive into predicting when these Major Troughs occur here, I believe that tracking sector-level debt, the velocity & acceleration of that debt, and the speed, ability, and ease in which major players in those sectors can service their debt, is a key ingredient in modelling this out (see Bridgewater Research).\nAs such a basic decision tree could look as follows:\n\n\nCode\nmermaid(\"\ngraph TD\n\nA(EPS RB < 10th Percentile?)-- No --> B[Not a Trough]\nB --> Z(Act accordingly...)\nA -- Yes --> C(Does separate model that includes Debt levels etc. indicate a Major Trough?)\n\nC -- No --> D[Minor Trough]\nC -- Yes --> E[Major Trough]\n\nD --> F(Act accordingly...)\nE --> G(Act accordingly...)\n\")"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#return-analysis",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#return-analysis",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Return Analysis",
    "text": "Return Analysis\nLets identify average 1M, 3M, 6M, and 1YR returns, had you invested at points during a peak or trough in EPS RB:\n\n\nCode\nreturn_summary_tbl_incl_major <- data_raw_bucketed %>% \n    mutate(\n        month_1_ret = (sp500_price / lag(sp500_price, n=25) - 1),\n        month_3_ret = (sp500_price / lag(sp500_price, n=25*3) - 1),\n        month_6_ret = (sp500_price / lag(sp500_price, n=25*6) - 1),\n        month_12_ret = (sp500_price / lag(sp500_price, n=25*12) - 1)\n    ) %>% \n    group_by(quantile) %>% \n    summarize(across(.cols = contains(\"month\"), .fns = ~mean(.x, na.rm = T)))\n\nreturn_summary_tbl_excl_major <- data_raw_bucketed %>% \n    mutate(\n        month_1_ret = (sp500_price / lag(sp500_price, n=25) - 1),\n        month_3_ret = (sp500_price / lag(sp500_price, n=25*3) - 1),\n        month_6_ret = (sp500_price / lag(sp500_price, n=25*6) - 1),\n        month_12_ret = (sp500_price / lag(sp500_price, n=25*12) - 1)\n    ) %>% \n    group_by(quantile) %>% \n    # filter out major trough time periods\n    filter(!(date %>% between(ymd(\"2008-09-01\"), ymd(\"2009-06-01\")))) %>%\n    filter(!(date %>% between(ymd(\"2020-03-01\"), ymd(\"2020-07-10\")))) %>% \n    summarize(across(.cols = contains(\"month\"), .fns = ~mean(.x, na.rm = T)))\n\n\n\nVisualTable\n\n\n\n\nCode\nreturn_summary_tbl_excl_major %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n    pivot_longer(cols = -Quantile, names_to = \"Time Horizon\", values_to = \"Return_excl\") %>% \n    bind_cols(return_summary_tbl_incl_major %>% \n                   set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n                   pivot_longer(cols = -Quantile, names_to = \"Time Horizon\", values_to = \"Return_incl\") %>% \n                   select(Return_incl)) %>% \n    pivot_longer(cols = contains(\"Return\")) %>%\n    mutate(Quantile = as.factor(Quantile),\n           `Time Horizon` = as_factor(`Time Horizon`)) %>% \n    ggplot(aes(Quantile, value)) +\n    geom_col(\n        data = . %>% \n            filter(name == \"Return_excl\"),\n        fill = \"midnightblue\",\n        color = \"black\",\n        alpha = .7\n    ) +\n    geom_point(\n        data = . %>% \n            filter(name == \"Return_incl\"),\n        size = 3,\n        color = \"darkorange\"\n    ) +\n    facet_wrap(~`Time Horizon`, ncol = 1, scales = \"free_y\") +\n    labs(y =\"\", title = \"Returns by Quantile\", subtitle = str_glue(\"Bar = Excluding Major Troughs\n                                                                   Point = Including Major Troughs\")) +\n    theme_tq() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\nExcluding Major TroughsIncluding Major Troughs\n\n\n\n\nCode\nreturn_summary_tbl_excl_major %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n    gt() %>% \n    fmt_percent(columns = -Quantile, decimals = 1) %>% \n    tab_header(title = \"SP500 Returns by Quantile of EPS Revision Breadth\",\n               subtitle = \"Excluding Major Troughs\")\n\n\n\n\n\n\n  \n    \n      SP500 Returns by Quantile of EPS Revision Breadth\n    \n    \n      Excluding Major Troughs\n    \n  \n  \n    \n      Quantile\n      Month 1 Ret\n      Month 3 Ret\n      Month 6 Ret\n      Month 12 Ret\n    \n  \n  \n    1\n1.4%\n−2.3%\n−4.0%\n−0.7%\n    2\n1.1%\n−0.3%\n0.8%\n5.8%\n    3\n1.1%\n2.4%\n2.4%\n9.5%\n    4\n0.4%\n2.7%\n5.0%\n10.7%\n    5\n0.8%\n4.9%\n6.8%\n13.5%\n    6\n0.6%\n4.9%\n5.7%\n12.7%\n    7\n1.6%\n3.2%\n7.7%\n13.6%\n    8\n2.0%\n4.6%\n10.0%\n15.7%\n    9\n1.0%\n5.2%\n12.3%\n15.1%\n    10\n1.2%\n6.9%\n15.6%\n27.2%\n  \n  \n  \n\n\n\n\n\n\n\n\nCode\nreturn_summary_tbl_incl_major %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n    gt() %>% \n    fmt_percent(columns = -Quantile, decimals = 1) %>% \n    tab_header(title = \"SP500 Returns by Quantile of EPS Revision Breadth\",\n               subtitle = \"Including Major Troughs\")\n\n\n\n\n\n\n  \n    \n      SP500 Returns by Quantile of EPS Revision Breadth\n    \n    \n      Including Major Troughs\n    \n  \n  \n    \n      Quantile\n      Month 1 Ret\n      Month 3 Ret\n      Month 6 Ret\n      Month 12 Ret\n    \n  \n  \n    1\n−0.8%\n−9.0%\n−13.3%\n−12.3%\n    2\n1.0%\n−0.4%\n0.2%\n4.7%\n    3\n1.2%\n2.8%\n2.1%\n8.7%\n    4\n0.5%\n2.8%\n4.9%\n10.1%\n    5\n0.8%\n4.9%\n6.8%\n13.5%\n    6\n0.6%\n4.9%\n5.7%\n12.7%\n    7\n1.6%\n3.2%\n7.7%\n13.6%\n    8\n2.0%\n4.6%\n10.0%\n15.7%\n    9\n1.0%\n5.2%\n12.3%\n15.1%\n    10\n1.2%\n6.9%\n15.6%\n27.2%\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nMain Takeaway\n\n\nCode\nqtile_1_epsrb_value <- data_raw_bucketed %>% \n    filter(quantile == 1) %>% \n    filter(eps_revision_breadth == max(eps_revision_breadth)) %>% \n    pull(eps_revision_breadth)\n\nqtile_10_epsrb_value <- data_raw_bucketed %>% \n    filter(quantile == 10) %>% \n    filter(eps_revision_breadth == min(eps_revision_breadth)) %>% \n    pull(eps_revision_breadth)\n\n\nEvidently, during times of lower EPS RB, returns are much lower than those during times when EPS RB is higher (for Time Horizons greater than 1 month). Therefore identifying where we are (and where we will be heading) in the EPS RB cycle is crucial.\nFrom what we have seen thus far, we can establish these basic rules of thumb:\n\nWhen EPS Revision Breadth is below its 10th Percentile (which translates to an EPS RB of approximately -16%), we should consider underweighting Equities\nWhen EPS Revision Breadth is above its 90th Percentile (which translates to an EPS RB of approximately 24%), we should consider overweighting Equities\nWhen EPS Revision Breadth is between these two values, weight Equities according to personal judgement"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#building-an-active-portfolio",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#building-an-active-portfolio",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Building an Active Portfolio",
    "text": "Building an Active Portfolio\nLet’s utilize our basic heuristics to build an active, long-only portfolio, consisting solely of Equities and Fixed Income, with asset class weights that vary according to EPS Revision Breadth. In other words, the portfolio’s allocation to Equities and Fixed Income is a function of EPS Revision Breadth at certain point in time. Therefore, we need to define a function that under-weights Equities during periods of very low EPS Revision Breadth, and does the converse for periods with moderate to high EPS Revision Breadth.\nA sample function could look as follows:\n\n\nCode\ndata_raw_bucketed %>% \n    group_by(quantile) %>% \n    summarize(epsrb_upper_bound = max(eps_revision_breadth)) %>% \n    add_row(quantile = c(0, 11), epsrb_upper_bound = c(-1, 1)) %>% \n    arrange(quantile) %>% \n    add_column(`Equity Allocation` = c(0, 0, 0.05, 0.45, 0.55, 0.70, 0.825, 0.9, 0.925, 0.95, 1, 1)) %>% \n    mutate(`Fixed Income Allocation` = 1-`Equity Allocation`) %>% \n    pivot_longer(contains(\"Allocation\")) %>% \n    ggplot(aes(epsrb_upper_bound, value, color = name)) +\n    geom_point() +\n    geom_line() +\n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(\n        x = \"EPS Revision Breadth\",\n        y = \"Portfolio Weight\",\n        color = \"\"\n    ) +\n    theme_tq() +\n    scale_color_tq() +\n    theme(legend.position = \"top\", text = element_text(size = 16))\n\n\n\n\n\nAs we can see, our function should be bounded between 0 and 1 on the y-axis, and have a similar shape to our ‘prototype’ above. These are both characteristics of the ‘sigmoid’ function:\n\\[\nf(x) = \\frac{1}{1 + e^{-c_1(x - c_2)}}\n\\]\nIf we test out a few constants, we see that the values of C1 = 20, and C2 = -7.5%, generates a function similar to our sample:\n\n\nCode\nsigmoid <- function(x, c1, c2){\n    y <- 1 / (1 + exp(-c1 * (x - c2)))\n    \n    return(y)\n}\n\ntibble(\n    eps_rb = seq(-1, 1 ,by=.01),\n    `Equity Allocation` = sigmoid(eps_rb, c1 = 20, c2 = -.075),\n    `Fixed Income Allocation` = 1-`Equity Allocation`\n) %>% \n    pivot_longer(contains(\"Allocation\")) %>% \n    ggplot(aes(eps_rb, value, color = name)) +\n    geom_line() +\n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_tq() +\n    scale_color_tq() +\n    labs(\n        x = \"EPS Revision Breadth\", y = \"Portfolio Weight\", title = \"Sigmoid Function\", subtitle = \"C1 = 20, C2 = -7.5%\", color = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 16))\n\n\n\n\n\n\nPortfolio Comparison\nLet’s apply our weighting scheme above and compare its performance to a traditional, static 60/40 Portfolio, an SP500-only Portfolio, and a Fixed-Income-only portfolio. We will consider the iShares Core U.S. Aggregate Bond ETF (“The AGG”) as a proxy for Fixed Income.\n\n\nCode\nprice_data <- tq_get(c(\"SPY\", \"AGG\"), from = \"2003-09-30\") %>% \n    select(symbol, date, adjusted)\n\nstart_tbl <- price_data %>% \n    group_by(symbol) %>% \n    mutate(pct_ret = (adjusted / lag(adjusted)) - 1) %>% \n    ungroup() %>% \n    select(-adjusted) %>% \n    pivot_wider(names_from = symbol, values_from = pct_ret) %>% \n    left_join(data_raw %>% \n                  select(-sp500_price))\n\nreturns_and_index <- start_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075),\n           port_fi_weight = 1 - port_equity_weight) %>%\n    \n    drop_na() %>% \n    mutate(portfolio_return = (port_equity_weight * SPY) + (port_fi_weight * AGG),\n           portfolio_60_40_return = (.6 * SPY) + (.4 * AGG)) %>% \n    pivot_longer(cols = c(SPY, AGG, contains(\"portfolio\"))) %>% \n    group_by(name) %>% \n    mutate(price_index = cumprod(1+value)) %>%\n    ungroup() %>% \n    mutate(name = case_when(\n        name == \"SPY\" ~ \"SP500\",\n        name == \"portfolio_60_40_return\" ~ \"60/40 Portfolio\",\n        name == \"portfolio_return\" ~ \"Active Portfolio\",\n        name == \"AGG\" ~ \"AGG\"\n    ))\n\nreturns_and_index %>% \n    ggplot(aes(date, price_index, color = name)) +\n    geom_line() +\n    theme_tq() +\n    scale_color_tq() +\n    theme(legend.position = \"top\", text = element_text(size = 16)) +\n    labs(x = \"\", y = \"Wealth Index ($1)\", color = \"\", title = \"Portfolio Comparison\") +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nAs we can see, our portfolio outperforms the traditional 60/40 Portfolio, but under-performs the SP500, on a nominal basis.\nHowever, when comparing the returns of these 4 portfolios on a risk-adjusted basis, our Active Portfolio fares very well, which means that our portfolio yields the most return per unit of risk.\nFor clarification, we consider investment returns as reward, and the standard deviation of returns (volatility) as risk. The Sharpe Ratio divides the average return by the standard deviation of these returns, while the Sortino Ratio divides the average return by the standard deviation of the negative returns only.\n\n\nCode\nmonthly_returns <- returns_and_index %>% \n    select(date, name, value) %>% \n    mutate(year = year(date),\n           month = month(date)) %>% \n    group_by(month, year, name) %>% \n    mutate(month_return = cumprod(1+value) - 1) %>% \n    slice_tail(n=1) %>% \n    ungroup() %>% \n    group_by(name)\n\nmonthly_returns %>% \n    summarize(\n        mean_monthly_return = mean(month_return),\n        st_dev_monthly_return = sd(month_return)\n    ) %>% \n    left_join(\n       monthly_returns %>% \n           filter(month_return < 0) %>% \n           group_by(name) %>% \n           summarize(downside_st_dev_monthly_return = sd(month_return))\n    ) %>% \n    mutate(sharpe_ratio = mean_monthly_return / st_dev_monthly_return,\n           sortino_ratio = mean_monthly_return / downside_st_dev_monthly_return) %>% \n    arrange(desc(sharpe_ratio)) %>% \n    set_names(names(.) %>% str_remove_all(\"monthly_return\") %>% str_replace_all(\"st_dev\", \"standard deviation\") %>%  str_replace_all(\"_\", \" \") %>% str_to_title()) %>% \n    gt(rowname_col = \"Name\") %>% \n    fmt_percent(columns = 2, decimals = 2) %>% \n    fmt_percent(columns = 3:last_col(), decimals = 1) %>% \n    tab_style(\n        style = cell_text(weight = \"bold\"),\n        locations = cells_body(columns = contains(\"Ratio\"))\n    ) %>% \n    gt::tab_header(title = \"Return Summary\", subtitle = \"Monthly Periodicity\") %>% \n    gt::tab_footnote(locations = cells_column_labels(columns = contains(\"Ratio\")), footnote = \"Calculation does not include the risk-free rate\")\n\n\n\n\n\n\n  \n    \n      Return Summary\n    \n    \n      Monthly Periodicity\n    \n  \n  \n    \n      \n      Mean \n      Standard Deviation \n      Downside Standard Deviation \n      Sharpe Ratio1\n      Sortino Ratio1\n    \n  \n  \n    Active Portfolio\n0.74%\n3.0%\n2.0%\n25.1%\n36.9%\n    60/40 Portfolio\n0.63%\n2.7%\n2.1%\n23.6%\n30.0%\n    AGG\n0.25%\n1.2%\n0.8%\n21.2%\n30.4%\n    SP500\n0.85%\n4.3%\n3.3%\n19.9%\n26.2%\n  \n  \n  \n    \n      1 Calculation does not include the risk-free rate\n    \n  \n\n\n\n\nThis occurs because our Active Portfolio is able to identify time periods when it is preferable to hold fixed income rather than equities, and vice versa. As a result, our portfolio has an average allocation of 75/25!\n\n\nCode\nstart_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075),\n           port_fi_weight = 1 - port_equity_weight) %>%\n    summarize(across(.cols = contains(\"port\"), .fns = ~mean(.x, na.rm = T))) %>% \n    set_names(c(\"Average Equity Allocation\", \"Average Fixed Income Allocation\")) %>% \n    gt() %>% \n    fmt_percent(columns = everything())\n\n\n\n\n\n\n  \n  \n    \n      Average Equity Allocation\n      Average Fixed Income Allocation\n    \n  \n  \n    74.17%\n25.83%\n  \n  \n  \n\n\n\n\nWe can plot our portfolio’s allocation to equities over time:\n\n\nCode\nggplot() +\n    geom_line(\n        data = start_tbl %>% \n                    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = 0),\n                    port_fi_weight = 1 - port_equity_weight),\n        mapping = aes(date, port_equity_weight)\n    ) +\n    geom_rect(\n        data = major_troughs_tbl %>% \n            slice(2),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"red\",\n        alpha = .3\n    ) +\n    theme_tq() +\n    scale_color_tq() +\n    scale_y_continuous(labels = scales::label_percent()) +\n    labs(x = \"\", y = \"Allocation to Equities\") +\n    theme(text = element_text(size = 16))\n\n\n\n\n\nThe red shaded region indicates the start of the Covid-19 Pandemic\n\n\n\n\nAs we can see, our portfolio underweights equities during times when EPS Revision Breadth is low, and overweights equities when EPS Revision breadth is high. However, we notice that it will therefore underweight equities in 2020, when the Covid-19 pandemic started.\nWhile the start of the Covid-19 pandemic was momentous, markets severely overreacted, and many savvy investors capitalized on these overreactions. Unlike in 2001 and 2008, when market expectations plummeted due to the burst of a financial bubble, in 2022 market expectations plummeted due to frenzied speculation. Because of this, it is unlikely that a rational investor would have sold all of his equities; so, lets course correct our portfolio with human judgement.\n\n\nCourse Correcting w/ Human Judgement\nLet’s assume that we are savvy investors, and that in March of 2022 we increased our allocation to equities to 75% because we understood that the markets were acting irrationally. If we do this while keeping everything else the same, here is how our active portfolio would have performed:\n\n\nCode\nreturns_and_index_mod <- start_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075)) %>% \n    mutate(port_equity_weight = ifelse(\n        date %>% between(major_troughs_tbl$start_date[2], major_troughs_tbl$end_date[2]),\n        .75, port_equity_weight)) %>% \n    mutate(port_fi_weight = 1 - port_equity_weight) %>% \n    drop_na() %>% \n    mutate(portfolio_return = (port_equity_weight * SPY) + (port_fi_weight * AGG),\n           portfolio_60_40_return = (.6 * SPY) + (.4 * AGG)) %>% \n    pivot_longer(cols = c(SPY, AGG, contains(\"portfolio\"))) %>% \n    group_by(name) %>% \n    mutate(price_index = cumprod(1+value)) %>%\n    ungroup() %>% \n    mutate(name = case_when(\n        name == \"SPY\" ~ \"SP500\",\n        name == \"portfolio_60_40_return\" ~ \"60/40 Portfolio\",\n        name == \"portfolio_return\" ~ \"Active Portfolio\",\n        name == \"AGG\" ~ \"AGG\"\n    ))\n\nreturns_and_index_mod %>% \n    ggplot(aes(date, price_index, color = name)) +\n    geom_line() +\n    theme_tq() +\n    scale_color_tq() +\n    theme(legend.position = \"top\", text = element_text(size = 16)) +\n    labs(x = \"\", y = \"Wealth Index ($1)\", color = \"\", title = \"Portfolio Comparison\") +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nNow, we notice that our active portfolio barely under-performs the market while still mitigating risk:\n\nReturn SummaryAverage AllocationEquity Allocation over Time\n\n\n\n\nCode\nmonthly_returns_mod <- returns_and_index_mod %>% \n    select(date, name, value) %>% \n    mutate(year = year(date),\n           month = month(date)) %>% \n    group_by(month, year, name) %>% \n    mutate(month_return = cumprod(1+value) - 1) %>% \n    slice_tail(n=1) %>% \n    ungroup() %>% \n    group_by(name)\n\nmonthly_returns_mod %>% \n    summarize(\n        mean_monthly_return = mean(month_return),\n        st_dev_monthly_return = sd(month_return)\n    ) %>% \n    left_join(\n       monthly_returns %>% \n           filter(month_return < 0) %>% \n           group_by(name) %>% \n           summarize(downside_st_dev_monthly_return = sd(month_return))\n    ) %>% \n    mutate(sharpe_ratio = mean_monthly_return / st_dev_monthly_return,\n           sortino_ratio = mean_monthly_return / downside_st_dev_monthly_return) %>% \n    arrange(desc(sharpe_ratio)) %>% \n    set_names(names(.) %>% str_remove_all(\"monthly_return\") %>% str_replace_all(\"st_dev\", \"standard deviation\") %>%  str_replace_all(\"_\", \" \") %>% str_to_title()) %>% \n    gt(rowname_col = \"Name\") %>% \n    fmt_percent(columns = 2, decimals = 2) %>% \n    fmt_percent(columns = 3:last_col(), decimals = 1) %>% \n    tab_style(\n        style = cell_text(weight = \"bold\"),\n        locations = cells_body(columns = contains(\"Ratio\"))\n    ) %>% \n    gt::tab_header(title = \"Return Summary\", subtitle = \"Monthly Periodicity\") %>% \n    gt::tab_footnote(locations = cells_column_labels(columns = contains(\"Ratio\")), footnote = \"Calculation does not include the risk-free rate\")\n\n\n\n\n\n\n  \n    \n      Return Summary\n    \n    \n      Monthly Periodicity\n    \n  \n  \n    \n      \n      Mean \n      Standard Deviation \n      Downside Standard Deviation \n      Sharpe Ratio1\n      Sortino Ratio1\n    \n  \n  \n    Active Portfolio\n0.77%\n3.1%\n2.0%\n25.0%\n38.3%\n    60/40 Portfolio\n0.63%\n2.7%\n2.1%\n23.6%\n30.0%\n    AGG\n0.25%\n1.2%\n0.8%\n21.2%\n30.4%\n    SP500\n0.85%\n4.3%\n3.3%\n19.9%\n26.2%\n  \n  \n  \n    \n      1 Calculation does not include the risk-free rate\n    \n  \n\n\n\n\n\n\n\n\nCode\nstart_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075)) %>% \n    mutate(port_equity_weight = ifelse(\n        date %>% between(major_troughs_tbl$start_date[2], major_troughs_tbl$end_date[2]),\n        .75, port_equity_weight)) %>% \n    mutate(port_fi_weight = 1 - port_equity_weight) %>% \n    summarize(across(.cols = contains(\"port\"), .fns = ~mean(.x, na.rm = T))) %>% \n    set_names(c(\"Average Equity Allocation\", \"Average Fixed Income Allocation\")) %>% \n    gt() %>% \n    fmt_percent(columns = everything())\n\n\n\n\n\n\n  \n  \n    \n      Average Equity Allocation\n      Average Fixed Income Allocation\n    \n  \n  \n    75.44%\n24.56%\n  \n  \n  \n\n\n\n\n\n\n\n\nCode\nggplot() +\n    geom_line(\n        data = start_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075)) %>% \n    mutate(port_equity_weight = ifelse(\n        date %>% between(major_troughs_tbl$start_date[2], major_troughs_tbl$end_date[2]),\n        .75, port_equity_weight)),\n        mapping = aes(date, port_equity_weight)\n    ) +\n    theme_tq() +\n    scale_color_tq() +\n    scale_y_continuous(labels = scales::label_percent()) +\n    labs(x = \"\", y = \"Allocation to Equities\") +\n    theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\nModifying C1 & C2\nDuring the construction of our portfolio, we arbitrarily used C1 = 20 and C2 = -.075 as inputs for our sigmoid function because they replicated our rough, ‘prototype’ weighting scheme. In addition, these inputs seemed sensible for the average investor as they yielded, on average, a 75/25 weighting scheme. However, we can adjust these parameters depending on the risk appetite of the investor.\nThe following demonstrates graphically what occurs as you change C1:\n\nAs we can see, C1 represents our sensitivity to changes in EPS Revision Breadth, and C2 represents the EPS Revision Breadth value that corresponds to an equity allocation of 50%. Therefore, we can elect to modify our weighting scheme for different investment styles. The following could represent a sample weighting scheme for a more risk averse investor:\n\n\nCode\ntibble(\n    eps_rb = seq(-1, 1 ,by=.01),\n    `Equity Allocation` = sigmoid(eps_rb, c1 = 10, c2 = 0.1),\n    `Fixed Income Allocation` = 1-`Equity Allocation`\n) %>% \n    pivot_longer(contains(\"Allocation\")) %>% \n    ggplot(aes(eps_rb, value, color = name)) +\n    geom_line() +\n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_tq() +\n    scale_color_tq() +\n    labs(\n        x = \"EPS Revision Breadth\", y = \"Portfolio Weight\", title = \"Sigmoid Function\", subtitle = \"C1 = 10, C2 = 10%\", color = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 16))"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n★EPS Revision Breadth Analysis\n\n\n\nEquities\n\n\n★\n\n\n\nRead Time: 10-15 mins\n\n\n\nMax Sands\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeather & the Stock Market\n\n\n\nEquities\n\n\n\nRead Time: 5-10 mins\n\n\n\nMax Sands\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManagement Trading\n\n\n\nEquities\n\n\n\nRead Time: 5-10 mins\n\n\n\nMax Sands\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n★High Yield Debt vs. Equities\n\n\n\nMacro\n\n\nHigh Yield Debt\n\n\n★\n\n\n\nRead Time: 15-25 mins\n\n\n\nMax Sands\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "2022 - Present\nCertified:\n\nData Science for Business w/ R\nML & High Performance Time Series Forecasting w/ R\nPredictive Shiny Web Applications w/ R\nShiny Development w/ R & AWS (in progress)\n\n\n\n\n2019 - 2022\nDesautels Faculty of Management, Montréal, Canada\nBachelor of Commerce, Major in Finance, Double Concentration in Accounting & Statistics"
  },
  {
    "objectID": "resume.html#choate-hall-stewart-choate-investment-advisors",
    "href": "resume.html#choate-hall-stewart-choate-investment-advisors",
    "title": "",
    "section": "Choate Hall & Stewart / Choate Investment Advisors",
    "text": "Choate Hall & Stewart / Choate Investment Advisors\nBusiness & Investment Analyst | 2022 - Present | Boston, MA\n\nXXXXX\nXXXXX\n\nXXXXX\n\nXXXXX\nXXXXX\n\nXXXXX"
  },
  {
    "objectID": "resume.html#mcgill-university-1",
    "href": "resume.html#mcgill-university-1",
    "title": "",
    "section": "McGill University",
    "text": "McGill University\nTeaching Assistant | 2021 - 2022 | Montréal, Canada\n\nDelivering lectures, mentoring/tutoring students, and grading all assessments - typically reserved for Graduate students"
  },
  {
    "objectID": "resume.html#ironhold-capital",
    "href": "resume.html#ironhold-capital",
    "title": "",
    "section": "IronHold Capital",
    "text": "IronHold Capital\nInvestment Analyst & Executive Assistant | 2020 - 2021 | New York, United States\n\nAided in raising capital, marketing efforts, reviewing legal contracts, and meeting with Family Offices & HNIs on behalf of the CEO\nWorked directly under the CIO and conducted research on Indian and U.S. Equities - the fund followed a Value-based strategy\nAuthor of the fund’s newsletter that covered key macroeconomic events, political news, and Family Office industry insights"
  },
  {
    "objectID": "resume.html#private-tutor",
    "href": "resume.html#private-tutor",
    "title": "",
    "section": "Private Tutor",
    "text": "Private Tutor\n2018 - 2021\n\nTutored students in Mathematics, Statistics, Physics, English, and Finance, and revised college admission essays"
  },
  {
    "objectID": "resume.html#replicating-ray-dalios-work-forecasting-the-big-debt-cycles",
    "href": "resume.html#replicating-ray-dalios-work-forecasting-the-big-debt-cycles",
    "title": "",
    "section": "Replicating Ray Dalio’s Work: Forecasting the ‘Big’ & ‘Debt’ Cycles",
    "text": "Replicating Ray Dalio’s Work: Forecasting the ‘Big’ & ‘Debt’ Cycles\n2022 - Present\n\nBuilding a system that continuously retrieves data from various sources and uses Machine Learning to predict several countries’ current and future stages of their ‘Big’ and ‘Debt’ Cycles, with the eventual goal of predicting asset returns across classes, sectors, and geographies"
  },
  {
    "objectID": "resume.html#insider-trading---scraping-sec-form-4-filings",
    "href": "resume.html#insider-trading---scraping-sec-form-4-filings",
    "title": "",
    "section": "Insider Trading - Scraping SEC Form 4 Filings",
    "text": "Insider Trading - Scraping SEC Form 4 Filings\n2022 - Present\n\nWrote code in R that scrapes the SEC Edgar site and all of its Form 4 filings. I am currently developing a trading strategy that incorporates this data."
  },
  {
    "objectID": "resume.html#coding-w-bloomberg-automating-equity-screens-rough-draft-dcfs",
    "href": "resume.html#coding-w-bloomberg-automating-equity-screens-rough-draft-dcfs",
    "title": "",
    "section": "Coding w/ Bloomberg: Automating Equity Screens & Rough Draft DCFs",
    "text": "Coding w/ Bloomberg: Automating Equity Screens & Rough Draft DCFs\n2022\n\nWrote code in R that connects to and scrapes information from a Bloomberg Terminal by utilizing its Desktop API\n\nAutomated Bloomberg’s EQS function so that I could run continuous equity screens (i.e. every ‘Saturday’ since MM/DD/YY)\nUsed scraped information to automate the generation of a ‘rough draft’ DCF for any company; companies with a significant discrepancy between ‘rough draft’ implied share price and 7-day VWAP would garner additional, refined analysis"
  },
  {
    "objectID": "resume.html#coding-w-amazon-web-services-aws-gemini-kraken-automating-cryptocurrency-trading-strategy",
    "href": "resume.html#coding-w-amazon-web-services-aws-gemini-kraken-automating-cryptocurrency-trading-strategy",
    "title": "",
    "section": "Coding w/ Amazon Web Services (AWS), Gemini, & Kraken: Automating Cryptocurrency Trading Strategy",
    "text": "Coding w/ Amazon Web Services (AWS), Gemini, & Kraken: Automating Cryptocurrency Trading Strategy\n2022\n\nWrote code in Python that connects to Gemini (cryptocurrency exchange) web API and allows me to place limit orders if certain conditions are met; automated trading by running the code externally every 5 minutes through AWS Cloudwatch & AWS Lambda\n\nRepeated the above with Kraken (cryptocurrency exchange) web API to place market orders"
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html",
    "href": "instructional_articles/00_r_basics/index.html",
    "title": "R in 5 Minutes",
    "section": "",
    "text": "The purpose of this article is 3-fold:\n\nto demonstrate the basics of R as concisely as possible so that you can get up and running on your own projects, even if you’ve had no exposure to coding.\nto act as a basic guide for the non-technical readers interested in following my Research Articles at a more granular level.\nto familiarize myself with the process of writing and explaining topics before I publish my research (and to make sure that my website is working…)\n\n\n\nI would quickly like to explain my background and why I think it is important to have a basic knowledge of ‘coding’:\nI am a Business & Investment Analyst, and 9 months ago I had absolutely no knowledge of ‘coding’; my technical ability was comparable to that of your average dog. I can now tell you 9 months in that understanding the basics of ‘coding’ goes a very long way.\nFirstly, as long as you do a task correctly the first time in code, you can then automate away that task (and its different variations). Whether its performing the same calculations in an Excel file that your boss sends you every morning, or publishing your company’s quarterly financial statements, the same principle applies.\nSecondly, we are living in a world where data is everywhere, and the ability to code allows one to dig into the data and draw valuable insights from it. For anyone in an analytical position (whether Financial Analyst, Medical Researcher, or CEO), this is extremely important and allows you to stand on the shoulders of giants.\nThirdly, you can leverage tools that others have built. There is so much free code on the web and someone else may have already built a tool or completed a task that you are trying to do. This is extremely helpful.\nLastly, a word of caution: coding is not everything. You can be the world’s greatest coder, but if you lack the ability to build a logical, easily-explainable narrative from data, then your value is limited to the tools that you can build for others. In other words, true value comes from the ability to not only work with data, but also derive meaning from it and think originally.\nOk, that’s all; let’s get into it!"
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "href": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "title": "R in 5 Minutes",
    "section": "The Basics of Data",
    "text": "The Basics of Data\nData is simply a spreadsheet of values, and we would like our data to be in a ‘tidy’ format.\n\nTidy Data\nData is considered tidy when each column represents a variable and each row consists of an observation. Consider the following dataset (and feel free to inspect the code and guess what each line means):\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNotice how this data is tidy; each column represents a variable (price, color, etc.) and each row is an observed diamond. Your goal should be to have your data in this format because it is easy to manipulate."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#gathering-data",
    "href": "instructional_articles/00_r_basics/index.html#gathering-data",
    "title": "R in 5 Minutes",
    "section": "Gathering Data",
    "text": "Gathering Data\nData is typically gathered from an API, a database, or simply an Excel/csv spreadsheet that you may have. For now, we will use a built-in R dataset called diamonds."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "href": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "title": "R in 5 Minutes",
    "section": "Manipulating Data",
    "text": "Manipulating Data\nAs long as data is in a tidy format, there are only a few actions that we need to do when manipulating data:\n\n\n\nfilter\nfilter data according to certain conditions\n\n\nsummarize\nsummarize the data (e.g. finding the average)\n\n\ngroup\ngroup similar observations\n\n\npivot\n‘pivoting’ the data in different ways\n\n\nselect\nselect relevant information\n\n\nmutate\nchanging the data in some fashion\n\n\n\n\nFiltering\nLet’s pretend that we only want to consider diamonds with a carat greater than .7 and a depth greater than 63: (click on the “Code” section)\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56\n2759\n5.81\n5.85\n3.72\n\n\n0.96\nFair\nF\nSI2\n66.3\n62\n2759\n6.27\n5.95\n4.07\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56\n2760\n5.80\n5.75\n3.65\n\n\n0.91\nFair\nH\nSI2\n64.4\n57\n2763\n6.11\n6.09\n3.93\n\n\n0.91\nFair\nH\nSI2\n65.7\n60\n2763\n6.03\n5.99\n3.95\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58\n2764\n5.64\n5.68\n3.60\n\n\n\n\n\n\nLet’s continue to filter down and consider only the subset with a cut of “Very Good”:\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    filter(cut == \"Very Good\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56.0\n2759\n5.81\n5.85\n3.72\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56.0\n2760\n5.80\n5.75\n3.65\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58.0\n2764\n5.64\n5.68\n3.60\n\n\n0.71\nVery Good\nG\nVS1\n63.3\n59.0\n2768\n5.52\n5.61\n3.52\n\n\n0.72\nVery Good\nG\nVS2\n63.7\n56.4\n2776\n5.62\n5.69\n3.61\n\n\n0.75\nVery Good\nD\nSI2\n63.1\n58.0\n2782\n5.78\n5.73\n3.63\n\n\n\n\n\n\nYou will now see that we have from our original 53,940 diamonds, we have filtered down to 1,550 that adhere to our conditions.\nAt this point you may have three questions:\n\nWhat is the %>%?\n\nThis is called a pipe and you can translate it to “and then”. It allows us to perform several operations consecutively. So if we look at the code, we first start with the diamonds dataset by typing diamonds, and then we filter according to carat and depth, and then we filter according to cut. The pipe is extremely useful and it is native to R.\n\nWhat does the head() function do?\n\nIt prints only the first 6 observations, that way you don’t have a table with 50,000 rows on your screen.\n\nWhat if I want to filter down to several different cuts, not just “Very Good”\n\nGreat question, here’s what you would do:\n\n\nCode\ndiamonds %>% \n    filter(cut %in% c(\"Ideal\", \"Premium\")) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.23\nIdeal\nJ\nVS1\n62.8\n56\n340\n3.93\n3.90\n2.46\n\n\n0.22\nPremium\nF\nSI1\n60.4\n61\n342\n3.88\n3.84\n2.33\n\n\n0.31\nIdeal\nJ\nSI2\n62.2\n54\n344\n4.35\n4.37\n2.71\n\n\n\n\n\n\nWe tell R to filter down to the observations where cut matches one of the strings in the vector c(\"Ideal\", \"Premium\"). The c() function creates a vector.\n\n\nSummarizing\nLet’s say we want to summarize the data and find the average diamond price, along with its standard deviation:\n\n\nCode\ndiamonds %>% \n    summarize(avg_price = mean(price),\n              st_dev    = sd(price))\n\n\n\n\n\n\navg_price\nst_dev\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\nNotice that we can take our 50,000+ diamonds and summarize the data down to an average price…\nYou will notice that in the summarize function I start by naming the column I want (avg_price) and then I tell R what to do (find the mean of the price variable/column. The mean() & sd() functions calculate mean and standard deviation respectively). I could just as easily call the columns “thing1” & “thing2”:\n\n\nCode\ndiamonds %>% \n    summarize(thing1 = mean(price),\n              thing2    = sd(price))\n\n\n\n\n\n\nthing1\nthing2\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\n\n\nGrouping\nSummarizing the entire data is important, but let’s say we want to find the average diamond price within each color group…\n\n\nCode\ndiamonds %>% \n    group_by(color) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup()\n\n\n\n\n\n\ncolor\navg_price\n\n\n\n\nD\n3169.954\n\n\nE\n3076.752\n\n\nF\n3724.886\n\n\nG\n3999.136\n\n\nH\n4486.669\n\n\nI\n5091.875\n\n\nJ\n5323.818\n\n\n\n\n\n\nWe can take things a step further and group by color and cut…\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nYou will notice that we now have average price for each color and cut. I also only showed the first 10 rows of output by using the slice() function.\n\n\nPivoting\nPivoting is probably the most complicated of the broad actions I am showing you, but the previous segment allows for a great transition. I decided to show only the first 10 rows of output rather than inundate you with 35 rows, but there must be a better way of showing the output, right? I mean we have letters repeating in the color column. This would make more sense:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    )\n\n\n\n\n\n\ncolor\nFair\nGood\nVery Good\nPremium\nIdeal\n\n\n\n\nD\n4291.061\n3405.382\n3470.467\n3631.293\n2629.095\n\n\nE\n3682.312\n3423.644\n3214.652\n3538.914\n2597.550\n\n\nF\n3827.003\n3495.750\n3778.820\n4324.890\n3374.939\n\n\nG\n4239.255\n4123.482\n3872.754\n4500.742\n3720.706\n\n\nH\n5135.683\n4276.255\n4535.390\n5216.707\n3889.335\n\n\nI\n4685.446\n5078.533\n5255.880\n5946.181\n4451.970\n\n\nJ\n4975.655\n4574.173\n5103.513\n6294.592\n4918.186\n\n\n\n\n\n\nWe tell R to take our 35 row table, and pivot it so that we have a color column followed by columns with the different cuts, wherein each value is the average price.\nThe names_from argument asks us what variable to we want to pivot on (we said ‘cut’ and therefore R took all of the cut values and made them columns). The values_from argument asks us which variable we would like to R to occupy the new columns with (we said ‘avg_price’ and therefore R occupied all of the ‘cells’ in our pivot table with the corresponding values from the avg_price column).\nQuick Tip: hitting the tab key when your cursor is inside of a function’s parentheses will show all of the function’s available arguments (2 of which are names_from and values_from for the pivot_longer() function.)\nImportant Note: You will notice that now we have violated the premise of tidy data. The columns Fair:Ideal are not variables. They are types of “cut” (cut is the variable). For the purposes of coding, and data manipulation, we want our data to be in a tidy format. However, for the purposes of presentation, we typically want our data to be in a ‘wide’ format (hence pivot_wider).\nWe can do the opposite and revert our table back into a ‘long’ format with pivot_longer() :\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols = Fair:Ideal\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\nname\nvalue\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nWe can also rename the columns back to their original names within the pivot_longer() function:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols      = Fair:Ideal,\n        names_to  = \"cut\",\n        values_to = \"avg_price\"\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nThat’s on pivoting…\n\n\nSelecting\nSelecting is straightforward. Here are the first 6 rows of our original dataset:\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nLet’s say we are about to investigate something but we only need price, carat, and cut… then it is best practice to select those variables/columns first (imagine we have thousands of variables/columns…):\n\n\nCode\ndiamonds %>% \n    select(price, carat, cut) %>% \n    head()\n\n\n\n\n\n\nprice\ncarat\ncut\n\n\n\n\n326\n0.23\nIdeal\n\n\n326\n0.21\nPremium\n\n\n327\n0.23\nGood\n\n\n334\n0.29\nPremium\n\n\n335\n0.31\nGood\n\n\n336\n0.24\nVery Good\n\n\n\n\n\n\nWe can also select by omission:\n\n\nCode\ndiamonds %>% \n    select(-x, -y, -z) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n\n\n\n\n\n\nWe can select variables carat through clarity:\n\n\nCode\ndiamonds %>% \n    select(carat:clarity) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\n\n\n\n\n0.23\nIdeal\nE\nSI2\n\n\n0.21\nPremium\nE\nSI1\n\n\n0.23\nGood\nE\nVS1\n\n\n0.29\nPremium\nI\nVS2\n\n\n0.31\nGood\nJ\nSI2\n\n\n0.24\nVery Good\nJ\nVVS2\n\n\n\n\n\n\nAnd again by omission:\n\n\nCode\ndiamonds %>% \n    select(-carat:-clarity) %>% \n    head()\n\n\n\n\n\n\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nVery simple.\n\n\nMutating\nWhat if we want to perform some sort of calculation or change the data in some way? This is the purpose of mutating…\nIn our dataset, we have the variables x, y, z which represent the length, width, and height of the diamond. If we pretend all the diamonds are cubes, we can calculate the cubic volume of each diamond by multiplying the dimensions of each diamond. Let’s do this:\n\n\nCode\ndiamonds %>% \n    select(x:z) %>% \n    mutate(volume = x * y * z) %>% \n    head()\n\n\n\n\n\n\nx\ny\nz\nvolume\n\n\n\n\n3.95\n3.98\n2.43\n38.20203\n\n\n3.89\n3.84\n2.31\n34.50586\n\n\n4.05\n4.07\n2.31\n38.07688\n\n\n4.20\n4.23\n2.63\n46.72458\n\n\n4.34\n4.35\n2.75\n51.91725\n\n\n3.94\n3.96\n2.48\n38.69395\n\n\n\n\n\n\nNotice how mutate() is similar in structure to summarize(); first we tell R what we would like name our new variable/column (“volume”), and then we tell R how to calculate it.\nMutate can also change a current column:\n\n\nCode\ndiamonds %>% \n    mutate(carat = \"Hello World\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\nHello World\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\nHello World\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\nHello World\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\nHello World\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\nHello World\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\nHello World\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNow, all observations of carat are “Hello World”."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "href": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "title": "R in 5 Minutes",
    "section": "Basic Modeling",
    "text": "Basic Modeling\nWe will build a linear model to explain diamond prices. In R, the function to create a linear model is lm():\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ carat) %>% \n    summary()\n\n\n\nCall:\nlm(formula = price ~ carat, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2256.36      13.06  -172.8   <2e-16 ***\ncarat        7756.43      14.07   551.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: < 2.2e-16\n\n\nWe just built a linear model that regressed carat on diamond price. As you can see, we can use a diamond’s caratage to explain 85% of price variation. Our model also tells us that for every 1 unit increase in caratage, diamond prices increases by $7,756 on average.\nHowever, I’m sure you will agree that the output is not visually pleasing. Moreover, it is not easy to manipulate since it is not in a tabular format.\nLet’s, once again, stand on the shoulders of giants and utilize a tool that someone else has built to clean up the output. Just like you installed tidyverse, install the broom package by running install.packages(\"broom\"). Then, load the package by running library(broom).\n\n\nCode\nlibrary(broom)\n\n\nThis time let’s regress price on all other variables and use the tidy() function from the broom package to tidy the output:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    tidy()\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n5753.761857\n396.629824\n14.5066294\n0.0000000\n\n\ncarat\n11256.978307\n48.627509\n231.4940348\n0.0000000\n\n\ncut.L\n584.457278\n22.478150\n26.0011290\n0.0000000\n\n\ncut.Q\n-301.908158\n17.993919\n-16.7783441\n0.0000000\n\n\ncut.C\n148.034703\n15.483328\n9.5609097\n0.0000000\n\n\ncut^4\n-20.793893\n12.376508\n-1.6801098\n0.0929418\n\n\ncolor.L\n-1952.160010\n17.341767\n-112.5698421\n0.0000000\n\n\ncolor.Q\n-672.053621\n15.776995\n-42.5970601\n0.0000000\n\n\ncolor.C\n-165.282926\n14.724927\n-11.2247022\n0.0000000\n\n\ncolor^4\n38.195186\n13.526539\n2.8237221\n0.0047487\n\n\ncolor^5\n-95.792932\n12.776114\n-7.4978145\n0.0000000\n\n\ncolor^6\n-48.466440\n11.613917\n-4.1731348\n0.0000301\n\n\nclarity.L\n4097.431318\n30.258596\n135.4137965\n0.0000000\n\n\nclarity.Q\n-1925.004097\n28.227228\n-68.1967102\n0.0000000\n\n\nclarity.C\n982.204550\n24.151516\n40.6684433\n0.0000000\n\n\nclarity^4\n-364.918493\n19.285011\n-18.9223900\n0.0000000\n\n\nclarity^5\n233.563110\n15.751700\n14.8278029\n0.0000000\n\n\nclarity^6\n6.883492\n13.715100\n0.5018915\n0.6157459\n\n\nclarity^7\n90.639737\n12.103482\n7.4887321\n0.0000000\n\n\ndepth\n-63.806100\n4.534554\n-14.0710870\n0.0000000\n\n\ntable\n-26.474085\n2.911655\n-9.0924516\n0.0000000\n\n\nx\n-1008.261098\n32.897748\n-30.6483316\n0.0000000\n\n\ny\n9.608887\n19.332896\n0.4970226\n0.6191751\n\n\nz\n-50.118891\n33.486301\n-1.4966983\n0.1344776\n\n\n\n\n\n\nYou will notice that I used ‘.’ to tell R ‘all other variables’ rather than type each of them out. More importantly, the output is much cleaner and easier to manipulate.\nHowever, we cannot see the model’s accuracy. For this, we need to use the glance() function from broom:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    glance()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\ndf.residual\nnobs\n\n\n\n\n0.9197915\n0.9197573\n1130.094\n26881.83\n0\n23\n53916\n53940\n\n\n\n\n\n\nNow we have accuracy metrics in a nice format.\nLastly, if we would like to see the model’s fit for each observation, we can use the augment() function from broom (scroll to the right):\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    augment() %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprice\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nx\ny\nz\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n326\n0.23\nIdeal\nE\nSI2\n61.5\n55\n3.95\n3.98\n2.43\n-1346.3643\n1672.3643\n0.0003742\n1130.082\n0.0000342\n1.4801217\n\n\n326\n0.21\nPremium\nE\nSI1\n59.8\n61\n3.89\n3.84\n2.31\n-664.5954\n990.5954\n0.0004133\n1130.097\n0.0000132\n0.8767411\n\n\n327\n0.23\nGood\nE\nVS1\n56.9\n65\n4.05\n4.07\n2.31\n211.1071\n115.8929\n0.0009098\n1130.105\n0.0000004\n0.1025982\n\n\n334\n0.29\nPremium\nI\nVS2\n62.4\n58\n4.20\n4.23\n2.63\n-830.7372\n1164.7372\n0.0004062\n1130.094\n0.0000180\n1.0308641\n\n\n335\n0.31\nGood\nJ\nSI2\n63.3\n58\n4.34\n4.35\n2.75\n-3459.2242\n3794.2242\n0.0007715\n1129.987\n0.0003629\n3.3587358\n\n\n336\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n3.94\n3.96\n2.48\n-1380.4876\n1716.4876\n0.0007230\n1130.081\n0.0000696\n1.5194380\n\n\n\n\n\n\nThe broom package is so useful because it cleans up model output, but more importantly, it can be used with many other (more complex) models."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "href": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "title": "R in 5 Minutes",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nBeing able to visualize data is essential for understanding it; the famous saying “a picture is worth a thousands words” is doubly true in today’s age.\nLet’s start out by plotting diamond price against caratage.\n\nCreating a Canvas\nFirst we need to create a canvas with the ggplot() function:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price))\n\n\n\n\n\nNotice that we start with the diamonds dataset and then we create a canvas with the ggplot() function. The aes() function stands for aesthetic and allows us to pick which variables/columns we want to use in our plot. In this case we tell R that we want to plot carat on the x-axis and price on the y-axis.\n\n\nAdding Geoms\nIn our plot we would like to add dots that represent each data point. In R adding these elements are called geometries (i.e. geoms):\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point()\n\n\n\n\n\nNotice how when creating plots with ggplot, we can no longer use the pipe (%>%). Instead, we use a + sign to add layers to the plot.\nFrom our plot we can tell that there is a clear positive relationship between price and caratage.\n\n\nModifying Geoms\nOur plot contains so many points and it is overwhelming; let’s modify the plot so that the points are more transparent with the alpha argument of geom_point().\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point(alpha = .15, color = \"midnightblue\") +\n    geom_smooth()\n\n\n\n\n\nYou will notice that the points are more transparent and that we also modified their color. We also included a smoother line with geom_smooth().\n\n\nAdding Aesthetics\nUp to now our plot has had only 2 aesthetics (x and y). But, all of the arguments that can be passed to geoms (alpha, color, etc.) are actually aesthetics that can be passed in the main aes() function. This probably sounds confusing but the following code will make much more sense:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth()\n\n\n\n\n\nYou will notice that instead of locally changing the color argument in the geom_point() function, we have put in the main aes() function wherein we set it equal to cut. By doing this, we are telling R that the color of each geometry should be defined by the cut variable/column.\n\n\nFaceting\nOur plot is overwhelming with all the different colors on one canvas so lets create a faceted canvas… Rather than explain in words, the following code should be self evident:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut)\n\n\n\n\n\nThis is called a faceted plot because we have created facets according to the cut variable/column. You will note that we need to put a ~ before the specified variable; this is just how the facet_wrap function works.\nWe can also decide to facet according to some other variables, like so:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~clarity, scales = \"free\")\n\n\n\n\n\nYou will notice that I also supplied the scales argument within the facet_wrap() function which allows each faceted plot to have different x and y scales that fit accordingly. Compare the x and y axes of the ‘VS1’ plot with those of the ‘VVS2’. They have different scales.\n\n\nAdding Labels\nLet’s add labels to our plot…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    )\n\n\n\n\n\n\n\nChanging Theme\nR has some preset plotting themes…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_linedraw()\n\n\n\n\n\n…there are several others.\n\n\nModifying Scales\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw() +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nNotice we converted the axis/scale on the plot to a dollar format…\n\n\nExample of More Plots\nWith these basic tools, you now have the ability to create so many different types of plots to gain insights from your data.\nHere are a few more plots with code to give you a flavor…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram(position = \"dodge\") +\n    theme_bw() +\n    scale_fill_brewer()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_density() +\n    theme_bw() +\n    scale_fill_brewer() +\n    facet_wrap(~cut)\n\n\n\n\n\nThere are other packages that help with creating nice plots… install and load ggridges.\n\n\nCode\nlibrary(ggridges)\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = stat(x))) +\n    geom_density_ridges_gradient(scale = 2) +\n    scale_fill_viridis_c(name = \"Price (in $)\", option = \"C\") +\n    theme_minimal() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = factor(stat(quantile)))) +\n    stat_density_ridges(\n        geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n        quantiles = 4, quantile_lines = TRUE\n    ) +\n    scale_fill_brewer() +\n    theme_linedraw() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCreating Interactive Plots\nWe also have the ability to create interactive plots with the help of a package called plotly. This is another example of the power of open-source coding, which gives us the ability to leverage code that others have built (that we may not have the expertise to create ourselves…). Like we did with the tidyverse, run install.packages(\"plotly\") and then load it into your environment with library(plotly). All we have to do to make a plot interactive, is to save it into our environment using the assignment operator - <-. I am going to save my plot as g and then we have to run ggplotly(g).\nLook at the code below:\n\n\nCode\nlibrary(plotly)\ng <- diamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\nggplotly(g)\n\n\n\n\n\n\nThis is just a taste of the plots that can be generated…"
  },
  {
    "objectID": "instructional_articles.html",
    "href": "instructional_articles.html",
    "title": "Instructional Articles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR in 5 Minutes\n\n\n\nR\n\n\n\nApplication-based Learning\n\n\n\nMax Sands\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#key-takeaways",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#key-takeaways",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nAs we can surmise from the analysis above, EPS Revision Breadth is a useful financial metric that reflects general market sentiment, which can help an investor position their portfolio accordingly."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#additional-areas-of-exploration",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#additional-areas-of-exploration",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Additional Areas of Exploration",
    "text": "Additional Areas of Exploration\nWhile I will not do so here, the above can be extended and replicated for each sector within the SP500 - allowing for an active portfolio that dynamically overweights and underweights specific sectors over time according to each sectors EPS Revision Breadth. It would also be wise to include several other variables, attempt to build a major tough vs. minor trough classifier, and incorporate other asset classes in the portfolio for diversification benefits."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#final-remarks",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#final-remarks",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Final Remarks",
    "text": "Final Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "research/00_high_yield_analysis/index.html",
    "href": "research/00_high_yield_analysis/index.html",
    "title": "★High Yield Debt vs. Equities",
    "section": "",
    "text": "Intro\nIn this article we will investigate:\n\nThe time periods where High Yield Debt outperformed Equities\nWhy these time periods occurred\nWhere both of these asset classes may be heading over the next few years\n\nLet’s load the data:\n\nDataVariable DefinitionsPlot\n\n\n\n\nCode\nhigh_yield_data_filled %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(across(.cols = c(2,3,4,6,8), .fns = ~scales::percent(.x, accuracy = .01, scale = 1))) %>% \n    mutate(across(.cols = c(5,7), .fns = ~scales::number(.x, accuracy = 1))) %>% \n    head() %>% \n    rename(Date = date) %>% \n    gt()\n\n\n\n\n\n\n  \n  \n    \n      Date\n      Inflation\n      Coupon Rate\n      Federal Funds Rate\n      High Yield Index\n      High Yield Spread\n      SP500 Index\n      Yield to Worst\n    \n  \n  \n    1997-12-31\n1.70%\n8.29%\n5.56%\n527\n2.96%\n970\n8.88%\n    1998-01-01\n1.70%\n8.28%\n5.56%\n527\n2.99%\n973\n8.87%\n    1998-01-02\n1.70%\n8.28%\n5.56%\n527\n3.02%\n975\n8.87%\n    1998-01-03\n1.70%\n8.28%\n5.56%\n528\n3.04%\n976\n8.87%\n    1998-01-04\n1.70%\n8.27%\n5.56%\n528\n3.07%\n976\n8.86%\n    1998-01-05\n1.70%\n8.27%\n5.55%\n528\n3.09%\n977\n8.86%\n  \n  \n  \n\n\n\n\n\n\n\n\n\nSP500 Index\nA stock market index composed of 500 large companies traded on U.S. stock exchanges. Each constituent’s weight in the index is proportional to its market capitalization.\n\n\nHigh Yield Index\nA bond index composed of high yield debt - corporate bonds with an investment grade of BB and below.\n\n\nHigh Yield Spread\nThe yield difference between High Yield Debt and U.S. Treasuries (U.S. Government Debt). For specific information on this series and how it is calculated please visit the FRED.\n\n\nFederal Funds Rate\nThe rate at which major U.S. banks lend their ‘federal funds’/reserve balances to each other overnight. This is generally the rate referred to when people mention ‘the interest rate’.\n\n\nCoupon Rate\nThe annual rate of income received by an investor for holding a bond. A bond with a 5% coupon rate and face value of $100 will result in annual income receipts of $5.\n\n\nYield to Worst\nThe lowest possible yield that can be earned by a bond that fully adheres to its contracted terms (perhaps a bond has a callable provision etc.). This measure does not include default risk.\n\n\nInflation\nThe yearly rate of increase in prices.\n\n\n\n\n\n\n\nCode\nhigh_yield_data %>% \n    filter(date >= min_date) %>% \n    plot_time_series(10) +\n    theme(text = element_text(size = 16))\n\n\n\n\n\nShaded regions indicate a time period when High Yield spreads exceeded 10%\n\n\n\n\n\n\n\n\n\nPerformance Comparison\nLet’s visualize how Equities have performed relative to High Yield Debt:\n\nAggregateYearly\n\n\n\n\nCode\nhigh_yield_data %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    group_by(symbol) %>% \n    arrange(date) %>% \n    mutate(value = value / first(value)) %>% \n    ungroup() %>% \n    ggplot(aes(date, value, color = name)) +\n    geom_line() +\n    theme_bw() +\n    labs(\n        color = \"\",\n        y = \"Wealth Index ($1)\",\n        x = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 16))\n\n\n\n\n\nWe can see that, since 1984, the performance of Equities relative to High Yield Debt has been roughly comparable, with Equities slightly outperforming. Equities returned approximately 24x and High Yield Debt returned 20x. However, let’s see if we can identify the time periods where High Yield Debt outperformed Equities, and why.\n\n\n\n\nCode\nyearly_performance_tbl <- high_yield_data %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(year = year(date)) %>% \n    group_by(name) %>% \n    arrange(date) %>% \n    ungroup() %>% \n    group_by(name, year) %>% \n    summarize(pct_ret = (last(value) / first(value)) - 1) %>% \n    ungroup()\n\ndates_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(difference > 0) %>% \n    pull(year)\n\nyearly_performance_tbl %>% \n    ggplot() +\n    geom_col(data = yearly_performance_tbl,\n             mapping = aes(year, pct_ret, fill = name),\n             position = \"dodge\", color = \"black\") +\n    geom_rect(\n        data = tibble(start_date = c(dates_vec - .5),\n                      end_date = c(dates_vec + .5)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .3, fill = \"grey\") +\n    theme_bw() +\n    scale_fill_brewer(direction = 1) +\n    labs(x = \"\",\n         y = \"Return (%)\",\n         fill = \"\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_continuous(guide = guide_axis(angle = 45), n.breaks = 12) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nShaded regions indicate years where High Yield Debt ourperformed Equities\n\n\n\n\nFrom the above, we can see there seems to be cyclicality in the performance of High Yield Debt relative to Equities, with alternating 2-4 year periods of outperformance followed by 1-3 year periods of underperformance.\n\n\n\n\n\nKey Takeaways\nIn order to better understand the logical cause-and-effect relationships at play between our variables, let’s re-visualize our yearly performance comparison, but this time we will add our other relevant variables into the mix and attempt to find helpful trends:\n\nActualSmooth\n\n\n\n\nCode\ndates_positive_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(`High Yield Index` > 0 & difference > 0) %>% \n    pull(year)\n\ng3 <- high_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_line(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name),\n        size = .75,\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\ng3\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\n\nCode\nhigh_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>%\n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_smooth(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name), se = F\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\nHere are some primary takeaways from the visuals:\n\nOutperformance generally occurs when:\n\n‘Interest Rates’ are falling\nYield Spreads and the YTW are elevated\n\nEspecially in relation to the 2-5 years prior\n\nPreceeding periods are associated with economic bubbles\n\n1990 Oil Shock / Recession\n2001 Dot Com Bubble\n2008 Great Financial Crisis\n2022 Covid/Tech Bubble\n\n\nPositive outperformance occurs later in the period\n\n\n\nBuilding a Simple Narrative\nObviously, these broad points are interrelated; let’s try to build an extremely simple narrative that captures these relationships:\n\n\nThis narrative is by no means detailed, nor complete. In, fact it can be superbly incorrect, but that is for the reader to judge.\nDuring prolonged periods of strong stock market upswings, especially those wherein current growth exceeds productive growth, market participants bet on good times continuing - typically levering up in the process. This makes complete sense; why finance growth with equity if it is performing so well? Better to issue debt instead… Investors and banks are happy to accept the debt; these companies are performing well (too well). But, just like it’s not a good idea to push yourself past physical capacity for extended periods of time, it is not good for a company to grow faster than its economic potential for extended periods of time; eventually something breaks.\nDuring this initial period of hurt, both equity holders and debt holders get whacked, valuations drop and the first bankruptcies occur. However, since debt holders get paid first, they get hurt slightly less than equity holders. Now, we are at a critical point in time… investors start to panic and make some back of the napkin calculations… and conclude that at current rates, debt is no longer a profitable investment, so they demand higher yields. Notice that YTW and High Yield Spreads briefly shoot up during these periods (1990, 2001, 2008…). However, the market seems to systematically underestimate two important things: 1) the Federal Reserve’s willingness to supply the proverbial medicine by lowering interest rates and 2) the associated aid from lower interest rates.\nWe can notice from our graph that, after the tough first year where both asset classes perform badly, the Fed steps in and stimulates the economy by lowering the cost of borrowing (… we also notice that yields will quickly follow suit). Naturally, companies are now able to negotiate better deals on borrowing money, so they take on more debt at a lower rate and use it to service their older, more expensive debt. Obviously, these companies are kicking the can down the road, and we can see that problems will occur when the Fed can no longer lower interest rates, but for now, this narrative sort of makes sense. Notice the prolonged periods of High Yield’s outperformance occur approximately a year after the initial crash (1990, 2001, 2008) for 2-4 years because investors were able to invest when yields were at their peak, and the Fed stimulated the economy shortly thereafter.\nNow, we flip back to the origin of the story… High Yield Debt is starting to see a prolonged period of strong performance… things seem less risky and therefore yields decline and the pendulum starts to swing in favor of equities again…\n\nA Note from OpenAI’s ChatGPT\nWhen tasked with the following - “Identify time periods where High Yield Debt outperformed Equities. Give a hypothesis for why it outperforms during periods of high inflation and economic downturns.” - OpenAI’s model gave the following answer:\n\n“High Yield Debt has historically outperformed Equities during periods of economic downturns, such as the 2008 Financial Crisis and the 2020 Coronavirus pandemic. Additionally, High Yield Debt has outperformed Equities during periods of rising inflation, such as the 1970s and 1980s.\nThe reason why High Yield Debt outperforms Equities during periods of high inflation and economic downturns is because it offers a higher yield than Equities, which can help to offset the effects of inflation and provide a more stable return in a volatile market. Additionally, High Yield Debt is generally less volatile than Equities, making it a more attractive investment during periods of economic uncertainty. Finally, High Yield Debt is typically less sensitive to changes in interest rates, which can be beneficial during periods of rising inflation.”\n- ChatGPT\n\n\n\n\nGoing Back Even Further\nWhile we don’t have data on a High Yield Bond Index, its Yield Spread, YTW, and Coupon Rate, before 1984, Aswath Damadoran, a professor at NYU known for gathering and sharing financial data, published the following helpful dataset:\n\n\nCode\nread_excel(here(\"raw_data\", \"High Yield Analysis\", \"damadoran_return_data.xlsx\")) %>% \n    select(-Inflation) %>% \n    head() %>% \n    gt() %>% \n    gt::tab_header(title = \"Yearly Real Returns\") %>% \n    gt::fmt_percent(-year) %>% \n    gt::tab_footnote(footnote = \"All returns are stated in real terms (i.e. adjusted for inflation)\")\n\n\n\n\n\n\n  \n    \n      Yearly Real Returns\n    \n    \n  \n  \n    \n      year\n      SP500\n      3 Month Treasury Bill\n      10 Year Treasury Bond\n      Baa Corporate Bonds\n      Real Estate\n    \n  \n  \n    1928\n45.49%\n4.29%\n2.01%\n4.43%\n2.68%\n    1929\n−8.83%\n2.56%\n3.60%\n2.42%\n−2.63%\n    1930\n−20.01%\n11.69%\n11.68%\n7.41%\n2.24%\n    1931\n−38.07%\n12.82%\n7.45%\n−7.02%\n1.29%\n    1932\n1.82%\n12.64%\n21.25%\n37.74%\n−0.21%\n    1933\n48.85%\n0.20%\n1.08%\n12.11%\n−4.54%\n  \n  \n  \n    \n       All returns are stated in real terms (i.e. adjusted for inflation)\n    \n  \n\n\n\n\nLet’s treat Baa Corporate Bonds as a proxy for ‘High Yield’ Debt and visualize its performance relative to the SP500 since 1928.\n\n\nCode\ndates_vec <- damadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(difference = `Baa Corporate Bonds` - SP500) %>% \n    filter(difference >= 0) %>% \n    pull(date)\n\ndates_positive_vec <- damadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(difference = `Baa Corporate Bonds` - SP500) %>% \n    filter(difference >= 0) %>% \n    filter(`Baa Corporate Bonds` > 0) %>% \n    pull(date)\n\ndamadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>% \n            mutate(end_date = end_date + years(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"grey\", alpha = .6\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(end_date = end_date + years(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"purple\", alpha = .15\n    ) +\n    geom_col(aes(date, value, fill = name), position = \"dodge\") +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(size = 16)) +\n    labs(y=\"\",x=\"\",fill=\"\") +\n    scale_y_continuous(labels = scales::percent_format(), n.breaks = 12) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year)\n\n\n\n\n\nThe above chart, while limited in its usefulness, provides us with some possible amendments to our key takeaways:\n\nOur observation that positive outperformance occurs later in the outperformance period does not seem to hold well during the 1929-1934 & 1940-1943 time-frames.\nThere appears to be marginally less cyclicality in outperformance.\nMost importantly, the above solidifies our claim that High Yield Outperforms during periods associated with financial bubbles (1929, 1940, etc.).\n\n\n\nApplying the Key Takeaways and Narrative\nLet’s attempt to codify our narrative and key takeaways, so that we can isolate periods of strong High Yield Debt returns (and hopefully identify future ones as well). I will tell the computer to go through the data and highlight a region if:\n\nthe current YTW is 30% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Yield Spread is 75% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Federal Funds Rate is 33% lower than its 3 Year Moving Average w/ a 2-year lag\n\nHere is the resulting plot:\n\n\nCode\nytw_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ytw\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 8),\n        .f = mean,\n        na.rm = T,\n        .period = 12,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.3 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nhys_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"hys\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 365 * 2),\n        .f = mean,\n        na.rm = T,\n        .period = 365 * 3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.75 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nffr_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ffr\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 12*2),\n        .f = mean,\n        na.rm = T,\n        .period = 12*3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value < .67 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\ng1 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    bind_rows(\n        tibble(\n            start_date = ffr_signal_dates_vec,\n            end_date = ffr_signal_dates_vec + months(1),\n            type = \"Fed Funds Rate\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    scale_fill_brewer() +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng2 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng3 <- g3 +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ncowplot::plot_grid(g1, g2, g3, ncol = 1, align = \"hv\")\n\n\n\n\n\nPlot 2 is the same as Plot 1, but Fed Funds Rate is removed. Plot 3 is being re-provided for ease of comparison.\n\n\n\n\nAs we can see, these 3 simple metrics, especially the YTW metric, provide a decent signal for periods of strong High Yield Debt returns. And while these metrics have been created with a knowledge of the past, they certainly corroborate the simple narrative we created above. Possible next steps include applying these signals to other developed markets while being mindful of their current position in their interest rate cycle, improving these signals, and assessing hypothetical performance, etc… I leave these as exercises to the reader…\n\n\nThe Future: Where are we heading?\nIn our above research, one of our key takeaways involved the Federal Funds Rate (i.e. the ‘interest’ rate). This should be self-evident as the interest rate measures the cost of borrowing money and is therefore the primary drivers of the economy and financial markets. We noted that during periods of economic hurt (typically after some sort of ‘bubble’) the Fed would lower rates and kick the can down the road. However, we have now reached that special point in time wherein interest rates can no longer be lowered. Instead, the Fed has been forced to aggressively raise rates, subsequently popping the Tech bubble. This makes for an interesting investment environment going forward. We have noted that High Yield Debt performs better during these reactionary, declining interest rate environments, and equities during the post-recovery ‘rising’ rate environment. However, in spite of all this, my outlook for equities is still slightly more pessimistic. Instead, I think that High Yield Debt is likely to outperform over the next 2 years, followed by Equities picking back up…\n\n\nFinal Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Sands",
    "section": "",
    "text": "Hi All - I’m Max and Welcome to my Site. I’m a Business Analyst with a passion for Data Science and Investment Research. On here, you can find mini Research Articles covering these topics; feel free to reach out via LinkedIn if they pique your interest, you would like to collaborate, or if you have questions. Cheers :)\n\n  \n    \n\n    \n  \n    \n     Instagram\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \n\n\nMcGill University | Finance | 2019 - 2022\n\n\n\nChoate Investment Advisors | Business Analyst | 2022 - Present\nMcGill University | T.A. - Adv. Bus. Stats | 2021 - 2022\nIronHold Capital | Investment Analyst | 2020 - 2021"
  },
  {
    "objectID": "index.html#current-projects",
    "href": "index.html#current-projects",
    "title": "Max Sands",
    "section": "Current Projects",
    "text": "Current Projects\nReplicating Ray Dalio’s Work: Ray Dalio is personal idol of mine because he is simultaneously an economist, data scientist, and narrator, but, above all else, he exemplifies the power of ‘first-principles’ thinking. He is able to synthesize centuries of history and data into logical, measurable, cause-and-effect relationships. I am currently attempting to build a system that forecasts major countries’ “Big” and “Debt” Cycles using Machine Learning (see Ray Dalio’s Principles for Dealing with With the Changing World Order and Principles for Navigating Big Debt Crises).\nInsider Trading - Scraping SEC Form 4 Filings: Wrote code in R that scrapes the SEC Edgar site and all of its Form 4 filings. I am currently developing a trading strategy that incorporates this data.\nIf you are interested about these projects, would like to collaborate, or would like to request data I’ve gathered, please reach out via LinkedIn - thank you."
  }
]