[
  {
    "objectID": "research/01_management_trading_analysis/index.html",
    "href": "research/01_management_trading_analysis/index.html",
    "title": "Management Trading",
    "section": "",
    "text": "Intro\nIn this article we will investigate if following management trading can provide superior investment returns. To do this, I’ve gathered a list of all the U.S. companies wherein management accounted for .01% (or more) of all volume traded for that company’s stock in the week prior. Other constraints, like a minimum company market capitalization, were also applied. I then retrieved each company’s stock prices for the following 3 months. Here is the resulting data since 2000:\n\nDataVariable Definitions\n\n\n\n\nCode\nmgmt_data %>% \n    head() %>% \n    gt() %>% \n    gt::fmt_currency(\n        columns = c(market_cap, price),\n        suffixing = T\n    ) %>% \n    gt::fmt_number(\n        columns = c(p_e), decimals = 0\n    ) %>% \n    gt::fmt_percent(\n        columns = c(mgmt_buy_volume, total_return_ytd)\n    )\n\n\n\n\n\n\n  \n  \n    \n      screen_date\n      date\n      symbol\n      short_name\n      mgmt_buy_volume\n      market_cap\n      p_e\n      total_return_ytd\n      price\n    \n  \n  \n    2000-07-07\n2000-07-07\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$24.06\n    2000-07-07\n2000-07-10\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$24.06\n    2000-07-07\n2000-07-11\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$23.31\n    2000-07-07\n2000-07-12\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$23.62\n    2000-07-07\n2000-07-13\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$25.69\n    2000-07-07\n2000-07-14\nIFCIQ\nINTL FIBERCOM\n19.59%\n$733.64M\n80\n205.56%\n$25.25\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\n\nscreen_date\nThe date where the computer went back in time and filtered all the companies that had management buy volume account for .01% (or more) of all trading volume in the week prior. You will notice that screen_date is always a Saturday.\n\n\ndate\nThe date associated with the company’s stock price (all other variables are constant).\n\n\nsymbol\nThe company’s stock ticker\n\n\nshort_name\nThe company’s name\n\n\nmgmt_buy_volume\nManagement’s proportion of trade volume\n\n\nmarket_cap\nThe company’s current market value (number of shares * share price)\n\n\np_e\nThe company’s Price-to-Earnings ratio (the amount of money paid for $1 of the company’s earnings)\n\n\ntotal_return_ytd\nThe Year-to-Date return on the company’s stock\n\n\nprice\nThe company’s closing stock price\n\n\n\n\n\n\n\n\nExamining the Data\nLet’s dig into the data and get a feel for what we are looking at. As usual, we will use visuals to help us.\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date) %>% \n    distinct(symbol) %>% \n    ungroup() %>% \n    count(screen_date) %>% \n    ggplot(aes(screen_date, n)) +\n    geom_col() +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        title = \"Count of Companies that Adhere to our Conditions\",\n        subtitle = \"Each Bar represents a Week\"\n    ) +\n    theme(text = element_text(size = 15))\n\n\n\n\n\nAs we can see from our plot, the weekly number of companies that adhere to our conditions gradually increase over time and rapidly increase after 2020. There are a few logical reasons for this:\n\nI filtered for companies with market caps greater than $200M, but $200M in 2001 is worth much more than $200M in 2022. It would have been better to filter while adjusting for inflation, but I forgot to do this…\nThe overall number of companies in the U.S. has greatly increased since 2001\nOver the past several years, we have had very low interest rates. With the cost of borrowing money so low, people have been spending money, creating companies, and driving valuations up.\n\nSince we don’t have many companies during 2000-2004, let’s make the executive decision to only use data from 2005 and on. Here is the same plot as before but lets group by year this time:\n\n\nCode\nmgmt_data <- mgmt_data %>% \n    filter(screen_date >= ymd(\"2005-01-01\"))\n\nmgmt_data %>% \n    group_by(screen_date) %>% \n    distinct(symbol) %>% \n    ungroup() %>% \n    mutate(year = year(screen_date)) %>% \n    count(year) %>% \n    ggplot(aes(year, n)) +\n    geom_col() +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        title = \"Count of Companies that Adhere to our Conditions\",\n        subtitle = \"Each Bar represents a Year\"\n    ) +\n    theme(text = element_text(size = 15))\n\n\n\n\n\nWhile we would still like to see more companies is the earlier years, this will have to do. Let’s continue forward and assess the performance of our filtered stocks relative to traditional index funds.\n\n\nPerformance Comparison\nLet’s use the Russell 2000 Index and the SP500 Index as our benchmarks. However, rather than use the indices themselves, let’s use ETFs instead as these are a better representation of actual investment performance since individuals cannot actually invest in the indices. Here is the code to obtain that data and the following graph representing their performance since 2005:\n\n\nCode\nprice_data <- tq_get(c(\"IWM\", \"SPY\"), from = \"2005-01-01\") %>% \n    select(symbol, date, adjusted) %>% \n    mutate(name = ifelse(symbol == \"IWM\", \"Russell 2000 ETF\", \"SP500 ETF\"))\n\nprice_data %>% \n    group_by(symbol) %>% \n    mutate(adjusted = adjusted / first(adjusted)) %>% \n    ungroup() %>% \n    ggplot(aes(date, adjusted, color = name)) +\n    geom_line() +\n    theme_bw() +\n    scale_color_grey() +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    labs(y = \"Portfolio Wealth ($1)\",\n         x = \"\",\n         color = \"\") +\n    theme(legend.position = \"top\", text = element_text(size = 15))\n\n\n\n\n\nNow that we have this data, let’s compare the performance of the ETFs to that of our companies. In order to do this, we need to establish an investment horizon that makes sense for our companies. In other words, should we pretend that we invest in our companies for a day? 3 days? A week? A Year? etc. Let’s investigate this:\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date, symbol) %>% \n    mutate(days_after_screen = row_number() - 1,\n           price_index = price / first(price)) %>% \n    ungroup() %>% \n    filter(days_after_screen <= 60) %>% \n    group_by(days_after_screen) %>% \n    summarize(average_return = mean(price_index, na.rm = T) - 1) %>% \n    ggplot(aes(days_after_screen, average_return)) +\n    geom_col() +\n    theme_bw() +\n    labs(\n        title = \"Average Return vs. Investment Horizon\",\n        y = \"Average Return\",\n        x = \"Investment Horizon (in Days)\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format(), n.breaks = 6) +\n    scale_x_continuous(n.breaks = 7) +\n    theme(text = element_text(size = 15))\n\n\n\n\n\nWe can already tell from the data that management clearly has some inside information that the markets are oblivious to; the average return for each of these companies after just 5 days is approximately 0.5%. This may not sound like much at first, but if you stop to do some simple calculations, you will realize that if you invest $1 at this rate, and compound your investment at a weekly (5 day) frequency over 52 weeks, then you will have approximately $1.3 at the end of the year. This equates to a 30% yearly return while the SP500 averages 10%.\nMoving forward, lets continue with a hypothetical investment horizon of a week (5 days) as this will deliver the most ‘bang for our buck’. While the 60 day average return is approximately 2.5%, we will lose out on the benefits from a shorter compounding frequency. Moreover, a week is a clean frequency to work with, and it matches nicely with the fact that our screens are run at a weekly frequency.\nNow that we have established our ‘investment horizon,’ let’s pretend that we invest equally in these companies each week and compare our performance to that of the SP500 and the Russell 2000:\n\nAggregateYearly\n\n\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date, symbol) %>% \n    mutate(days_after_screen = row_number() - 1,\n           price_index = price / first(price)) %>% \n    ungroup() %>% \n    filter(days_after_screen == 5) %>% \n    group_by(screen_date) %>% \n    summarize(portfolio_index = mean(price_index, na.rm = T)) %>% \n    left_join(\n        price_data %>% \n    mutate(date = floor_date(date, unit = \"weeks\") - days(2)) %>% \n    group_by(name, date) %>% \n    summarize(return = adjusted/first(adjusted)) %>% \n    slice_tail() %>% \n    ungroup() %>% \n    pivot_wider(names_from = name, values_from = return) %>% \n    janitor::clean_names(),\n        by = c(\"screen_date\" = \"date\")\n    ) %>% \n    mutate(across(-screen_date, .fns = cumprod)) %>% \n    pivot_longer(-screen_date) %>% \n    \n    ggplot(aes(screen_date, value, color = name)) +\n    geom_line() +\n    theme_bw() +\n    scale_color_grey() +\n    scale_y_continuous(labels = scales::dollar_format()) +\n    labs(\n        y = \"Portfolio Wealth Index ($1)\",\n        color = \"\",\n        x = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 15))\n\n\n\n\n\n\n\n\n\nCode\nmgmt_data %>% \n    group_by(screen_date, symbol) %>% \n    mutate(days_after_screen = row_number() - 1,\n           price_index = price / first(price)) %>% \n    ungroup() %>% \n    filter(days_after_screen == 5) %>% \n    group_by(screen_date) %>% \n    summarize(portfolio_index = mean(price_index, na.rm = T)) %>% \n    left_join(\n        price_data %>% \n    mutate(date = floor_date(date, unit = \"weeks\") - days(2)) %>% \n    group_by(name, date) %>% \n    summarize(return = adjusted/first(adjusted)) %>% \n    slice_tail() %>% \n    ungroup() %>% \n    pivot_wider(names_from = name, values_from = return) %>% \n    janitor::clean_names(),\n        by = c(\"screen_date\" = \"date\")\n    ) %>% \n    mutate(year = year(screen_date)) %>% \n    group_by(year) %>% \n    mutate(across(-screen_date, .fns = cumprod)) %>% \n    slice_tail() %>% \n    ungroup() %>% \n    select(-screen_date, -russell_2000_etf) %>%\n    pivot_longer(-year) %>% \n    mutate(value = value - 1) %>% \n    ggplot(aes(year, value, fill = name)) +\n    geom_col(position = \"dodge\") +\n    theme_bw() +\n    scale_fill_grey() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(\n        y = \"Return (%)\",\n        x = \"\",\n        fill = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 15))\n\n\n\n\n\n\n\n\nIt is obviously apparent, from the above, that following management trading can provide superior investment returns…\n\n\nFinal Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "research/00_high_yield_analysis/index.html",
    "href": "research/00_high_yield_analysis/index.html",
    "title": "★High Yield Debt vs. Equities",
    "section": "",
    "text": "Intro\nIn this article we will investigate:\n\nThe time periods where High Yield Debt outperformed Equities\nWhy these time periods occurred\nWhere both of these asset classes may be heading over the next few years\n\nLet’s load the data:\n\nDataVariable DefinitionsPlot\n\n\n\n\nCode\nhigh_yield_data_filled %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(across(.cols = c(2,3,4,6,8), .fns = ~scales::percent(.x, accuracy = .01, scale = 1))) %>% \n    mutate(across(.cols = c(5,7), .fns = ~scales::number(.x, accuracy = 1))) %>% \n    head() %>% \n    rename(Date = date)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nInflation\nCoupon Rate\nFederal Funds Rate\nHigh Yield Index\nHigh Yield Spread\nSP500 Index\nYield to Worst\n\n\n\n\n1997-12-31\n1.70%\n8.29%\n5.56%\n527\n2.96%\n970\n8.88%\n\n\n1998-01-01\n1.70%\n8.28%\n5.56%\n527\n2.99%\n973\n8.87%\n\n\n1998-01-02\n1.70%\n8.28%\n5.56%\n527\n3.02%\n975\n8.87%\n\n\n1998-01-03\n1.70%\n8.28%\n5.56%\n528\n3.04%\n976\n8.87%\n\n\n1998-01-04\n1.70%\n8.27%\n5.56%\n528\n3.07%\n976\n8.86%\n\n\n1998-01-05\n1.70%\n8.27%\n5.55%\n528\n3.09%\n977\n8.86%\n\n\n\n\n\n\n\n\n\n\n\nSP500 Index\nA stock market index composed of 500 large companies traded on U.S. stock exchanges. Each constituent’s weight in the index is proportional to its market capitalization.\n\n\nHigh Yield Index\nA bond index composed of high yield debt - corporate bonds with an investment grade of BB and below.\n\n\nHigh Yield Spread\nThe yield difference between High Yield Debt and U.S. Treasuries (U.S. Government Debt). For specific information on this series and how it is calculated please visit the FRED.\n\n\nFederal Funds Rate\nThe rate at which major U.S. banks lend their ‘federal funds’/reserve balances to each other overnight. This is generally the rate referred to when people mention ‘the interest rate’.\n\n\nCoupon Rate\nThe annual rate of income received by an investor for holding a bond. A bond with a 5% coupon rate and face value of $100 will result in annual income receipts of $5.\n\n\nYield to Worst\nThe lowest possible yield that can be earned by a bond that fully adheres to its contracted terms (perhaps a bond has a callable provision etc.). This measure does not include default risk.\n\n\nInflation\nThe yearly rate of increase in prices.\n\n\n\n\n\n\n\nCode\nhigh_yield_data %>% \n    filter(date >= min_date) %>% \n    plot_time_series(10) +\n    theme(text = element_text(size = 16))\n\n\n\n\n\nShaded regions indicate a time period when High Yield spreads exceeded 10%\n\n\n\n\n\n\n\n\n\nPerformance Comparison\nLet’s visualize how Equities have performed relative to High Yield Debt:\n\nAggregateYearly\n\n\n\n\nCode\nhigh_yield_data %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    group_by(symbol) %>% \n    arrange(date) %>% \n    mutate(value = value / first(value)) %>% \n    ungroup() %>% \n    ggplot(aes(date, value, color = name)) +\n    geom_line() +\n    theme_bw() +\n    labs(\n        color = \"\",\n        y = \"Wealth Index ($1)\",\n        x = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 16))\n\n\n\n\n\nWe can see that, since 1984, the performance of Equities relative to High Yield Debt has been roughly comparable, with Equities slightly outperforming. Equities returned approximately 24x and High Yield Debt returned 20x. However, let’s see if we can identify the time periods where High Yield Debt outperformed Equities, and why.\n\n\n\n\nCode\nyearly_performance_tbl <- high_yield_data %>% \n    filter(symbol %in% c(\"hyi\", \"spy\")) %>% \n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(year = year(date)) %>% \n    group_by(name) %>% \n    arrange(date) %>% \n    ungroup() %>% \n    group_by(name, year) %>% \n    summarize(pct_ret = (last(value) / first(value)) - 1) %>% \n    ungroup()\n\ndates_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(difference > 0) %>% \n    pull(year)\n\nyearly_performance_tbl %>% \n    ggplot() +\n    geom_col(data = yearly_performance_tbl,\n             mapping = aes(year, pct_ret, fill = name),\n             position = \"dodge\", color = \"black\") +\n    geom_rect(\n        data = tibble(start_date = c(dates_vec - .5),\n                      end_date = c(dates_vec + .5)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .3, fill = \"grey\") +\n    theme_bw() +\n    scale_fill_brewer(direction = 1) +\n    labs(x = \"\",\n         y = \"Return (%)\",\n         fill = \"\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_continuous(guide = guide_axis(angle = 45), n.breaks = 12) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nShaded regions indicate years where High Yield Debt ourperformed Equities\n\n\n\n\nFrom the above, we can see there seems to be cyclicality in the performance of High Yield Debt relative to Equities, with alternating 2-4 year periods of outperformance followed by 1-3 year periods of underperformance.\n\n\n\n\n\nKey Takeaways\nIn order to better understand the logical cause-and-effect relationships at play between our variables, let’s re-visualize our yearly performance comparison, but this time we will add our other relevant variables into the mix and attempt to find helpful trends:\n\nActualSmooth\n\n\n\n\nCode\ndates_positive_vec <- yearly_performance_tbl %>% \n    pivot_wider(names_from = name, values_from = pct_ret) %>% \n    mutate(difference = `High Yield Index` - `SP500 Index`) %>% \n    filter(`High Yield Index` > 0 & difference > 0) %>% \n    pull(year)\n\ng3 <- high_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_line(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name),\n        size = .75,\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\ng3\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\n\nCode\nhigh_yield_data %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>%\n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>%\n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .6, fill = \"grey\"\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(start_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = as.Date(paste(start_date, 1, 1, sep = \"-\"))) %>% \n            mutate(end_date = end_date %>% ceiling_date(unit = \"years\") - days(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        alpha = .1, fill = \"purple\"\n    ) +\n    geom_smooth(\n        data = high_yield_data %>%\n            filter(date >= ymd(\"1984-01-01\")) %>%\n            filter(!(symbol %in% c(\"hyi\", \"spy\"))) %>%\n            filter(value != 0) %>% \n            mutate(value = value/100),\n        mapping = aes(date, value, color = name), se = F\n        ) +\n    theme_bw() +\n    labs(\n        x = \"\",\n        y = \"\",\n        color = \"\"\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year) +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16))\n\n\n\n\n\nRegions shaded in purple indicate positive outperformance\n\n\n\n\n\n\n\nHere are some primary takeaways from the visuals:\n\nOutperformance generally occurs when:\n\n‘Interest Rates’ are falling\nYield Spreads and the YTW are elevated\n\nEspecially in relation to the 2-5 years prior\n\nPreceeding periods are associated with economic bubbles\n\n1990 Oil Shock / Recession\n2001 Dot Com Bubble\n2008 Great Financial Crisis\n2022 Covid/Tech Bubble\n\n\nPositive outperformance occurs later in the period\n\n\n\nBuilding a Simple Narrative\nObviously, these broad points are interrelated; let’s try to build an extremely simple narrative that captures these relationships:\n\n\nThis narrative is by no means detailed, nor complete. In, fact it can be superbly incorrect, but that is for the reader to judge.\nDuring prolonged periods of strong stock market upswings, especially those wherein current growth exceeds productive growth, market participants bet on good times continuing - typically levering up in the process. This makes complete sense; why finance growth with equity if it is performing so well? Better to issue debt instead… Investors and banks are happy to accept the debt; these companies are performing well (too well). But, just like it’s not a good idea to push yourself past physical capacity for extended periods of time, it is not good for a company to grow faster than its economic potential for extended periods of time; eventually something breaks.\nDuring this initial period of hurt, both equity holders and debt holders get whacked, valuations drop and the first bankruptcies occur. However, since debt holders get paid first, they get hurt slightly less than equity holders. Now, we are at a critical point in time… investors start to panic and make some back of the napkin calculations… and conclude that at current rates, debt is no longer a profitable investment, so they demand higher yields. Notice that YTW and High Yield Spreads briefly shoot up during these periods (1990, 2001, 2008…). However, the market seems to systematically underestimate two important things: 1) the Federal Reserve’s willingness to supply the proverbial medicine by lowering interest rates and 2) the associated aid from lower interest rates.\nWe can notice from our graph that, after the tough first year where both asset classes perform badly, the Fed steps in and stimulates the economy by lowering the cost of borrowing (… we also notice that yields will quickly follow suit). Naturally, companies are now able to negotiate better deals on borrowing money, so they take on more debt at a lower rate and use it to service their older, more expensive debt. Obviously, these companies are kicking the can down the road, and we can see that problems will occur when the Fed can no longer lower interest rates, but for now, this narrative sort of makes sense. Notice the prolonged periods of High Yield’s outperformance occur approximately a year after the initial crash (1990, 2001, 2008) for 2-4 years because investors were able to invest when yields were at their peak, and the Fed stimulated the economy shortly thereafter.\nNow, we flip back to the origin of the story… High Yield Debt is starting to see a prolonged period of strong performance… things seem less risky and therefore yields decline and the pendulum starts to swing in favor of equities again…\n\nA Note from OpenAI’s ChatGPT\nWhen tasked with the following - “Identify time periods where High Yield Debt outperformed Equities. Give a hypothesis for why it outperforms during these time periods.” - OpenAI’s GPT4 model gave the following answer:\n\n“While it’s important to note that past performance is not indicative of future results, there are a few historical time periods when high yield debt (also known as junk bonds) outperformed equities. Two notable periods include:\n\n2000-2002: During the dot-com bubble burst and the subsequent market downturn, high yield debt outperformed equities. The S&P 500 experienced a significant decline during this period while high yield debt saw relatively smaller losses.\n\nHypothesis: High yield debt may have outperformed equities during this period due to a flight to safety by investors. As the technology sector crashed and equities experienced a broad decline, investors sought investments with higher yields and potentially lower volatility compared to the plunging stock market. High yield bonds, despite their riskier nature, may have been perceived as a safer alternative to the rapidly falling equities market.\n\n2008-2009: During the Global Financial Crisis, high yield debt also outperformed equities, particularly in the early stages of the crisis. While both asset classes experienced losses, high yield bonds started to recover faster than equities in 2009.\n\nHypothesis: One possible reason for high yield debt outperforming equities during this period is the massive intervention by central banks and governments in the form of quantitative easing and fiscal stimulus. These measures helped stabilize financial markets and injected liquidity into the credit markets, which in turn lowered borrowing costs for high yield issuers. As a result, high yield bonds became more attractive to investors searching for higher returns in a low-interest-rate environment, leading to a faster recovery in high yield debt compared to equities.\nIt’s important to note that these are specific historical periods, and the relative performance of high yield debt and equities can vary depending on economic conditions, market sentiment, and interest rate environments.”\n- GPT 4\n\n\n\n\nGoing Back Even Further\nWhile we don’t have data on a High Yield Bond Index, its Yield Spread, YTW, and Coupon Rate, before 1984, Aswath Damadoran, a professor at NYU known for gathering and sharing financial data, published the following helpful dataset:\n\n\nCode\nread_excel(here(\"raw_data\", \"High Yield Analysis\", \"damadoran_return_data.xlsx\")) %>% \n    select(-Inflation) %>% \n    head() %>% \n    gt() %>% \n    gt::tab_header(title = \"Yearly Real Returns\") %>% \n    gt::fmt_percent(-year) %>% \n    gt::tab_footnote(footnote = \"All returns are stated in real terms (i.e. adjusted for inflation)\")\n\n\n\n\n\n\n  \n    \n      Yearly Real Returns\n    \n    \n  \n  \n    \n      year\n      SP500\n      3 Month Treasury Bill\n      10 Year Treasury Bond\n      Baa Corporate Bonds\n      Real Estate\n    \n  \n  \n    1928\n45.49%\n4.29%\n2.01%\n4.43%\n2.68%\n    1929\n−8.83%\n2.56%\n3.60%\n2.42%\n−2.63%\n    1930\n−20.01%\n11.69%\n11.68%\n7.41%\n2.24%\n    1931\n−38.07%\n12.82%\n7.45%\n−7.02%\n1.29%\n    1932\n1.82%\n12.64%\n21.25%\n37.74%\n−0.21%\n    1933\n48.85%\n0.20%\n1.08%\n12.11%\n−4.54%\n  \n  \n  \n    \n       All returns are stated in real terms (i.e. adjusted for inflation)\n    \n  \n\n\n\n\nLet’s treat Baa Corporate Bonds as a proxy for ‘High Yield’ Debt and visualize its performance relative to the SP500 since 1928.\n\n\nCode\ndates_vec <- damadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(difference = `Baa Corporate Bonds` - SP500) %>% \n    filter(difference >= 0) %>% \n    pull(date)\n\ndates_positive_vec <- damadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    pivot_wider(names_from = name, values_from = value) %>% \n    mutate(difference = `Baa Corporate Bonds` - SP500) %>% \n    filter(difference >= 0) %>% \n    filter(`Baa Corporate Bonds` > 0) %>% \n    pull(date)\n\ndamadoran_data %>% \n    filter(name %in% c(\"Baa Corporate Bonds\", \"SP500\")) %>% \n    filter(date %>% between(ymd(\"1928-01-01\"), ymd(\"1984-01-01\"))) %>% \n    ggplot() +\n    geom_rect(\n        data = tibble(start_date = dates_vec, end_date = dates_vec) %>% \n            mutate(end_date = end_date + years(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"grey\", alpha = .6\n    ) +\n    geom_rect(\n        data = tibble(start_date = dates_positive_vec, end_date = dates_positive_vec) %>% \n            mutate(end_date = end_date + years(1)),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"purple\", alpha = .15\n    ) +\n    geom_col(aes(date, value, fill = name), position = \"dodge\") +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(size = 16)) +\n    labs(y=\"\",x=\"\",fill=\"\") +\n    scale_y_continuous(labels = scales::percent_format(), n.breaks = 12) +\n    scale_x_date(date_breaks = \"3 years\", guide = guide_axis(angle = 45), labels = year)\n\n\n\n\n\nThe above chart, while limited in its usefulness, provides us with some possible amendments to our key takeaways:\n\nOur observation that positive outperformance occurs later in the outperformance period does not seem to hold well during the 1929-1934 & 1940-1943 time-frames.\nThere appears to be marginally less cyclicality in outperformance.\nMost importantly, the above solidifies our claim that High Yield Outperforms during periods associated with financial bubbles (1929, 1940, etc.).\n\n\n\nApplying the Key Takeaways and Narrative\nLet’s attempt to codify our narrative and key takeaways, so that we can isolate periods of strong High Yield Debt returns (and hopefully identify future ones as well). I will tell the computer to go through the data and highlight a region if:\n\nthe current YTW is 30% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Yield Spread is 75% greater than its 3 Year Moving Average w/ a 2-year lag\nthe current Federal Funds Rate is 33% lower than its 3 Year Moving Average w/ a 2-year lag\n\nHere is the resulting plot:\n\n\nCode\nytw_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ytw\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 8),\n        .f = mean,\n        na.rm = T,\n        .period = 12,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.3 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nhys_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"hys\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 365 * 2),\n        .f = mean,\n        na.rm = T,\n        .period = 365 * 3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value > 1.75 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\nffr_signal_dates_vec <- high_yield_data %>%\n    filter(symbol == \"ffr\") %>%\n    arrange(date) %>%\n    filter(value != 0) %>%\n    filter(date >= ymd(\"1984-01-01\")) %>% \n    mutate(roll_mean_3yr_lag2 = slidify_vec(\n        .x = lag(value, n = 12*2),\n        .f = mean,\n        na.rm = T,\n        .period = 12*3,\n        .align = c(\"right\")\n    )) %>% \n    drop_na() %>% \n    filter(value < .67 * roll_mean_3yr_lag2) %>% \n    pull(date)\n\ng1 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    bind_rows(\n        tibble(\n            start_date = ffr_signal_dates_vec,\n            end_date = ffr_signal_dates_vec + months(1),\n            type = \"Fed Funds Rate\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    scale_fill_brewer() +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng2 <- tibble(\n    start_date = ytw_signal_dates_vec,\n    end_date = ceiling_date(ytw_signal_dates_vec, unit = \"months\") + months(3) - days(1),\n    type = \"Yield to Worst\"\n) %>% \n    bind_rows(\n        tibble(\n            start_date = hys_signal_dates_vec,\n            end_date = hys_signal_dates_vec + days(1),\n            type = \"High Yield Spread\"\n        )\n    ) %>% \n    ggplot(aes(xmin = start_date, xmax = end_date, ymin = -1, ymax = 1, fill = type)) +\n    geom_rect(alpha = .9) +\n    theme_bw() +\n    theme(legend.position = \"top\", text = element_text(face = \"bold\", size = 16)) +\n    labs(\n        fill = \"\"\n    ) +\n    scale_y_continuous(breaks = NULL) +\n    scale_x_date(date_breaks = \"3 years\", labels = year, guide = guide_axis(angle = 45)) +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ng3 <- g3 +\n    coord_cartesian(xlim = c(ymd(\"1988-01-01\", ymd(\"2023-01-01\"))))\n\ncowplot::plot_grid(g1, g2, g3, ncol = 1, align = \"hv\")\n\n\n\n\n\nPlot 2 is the same as Plot 1, but Fed Funds Rate is removed. Plot 3 is being re-provided for ease of comparison.\n\n\n\n\nAs we can see, these 3 simple metrics, especially the YTW metric, provide a decent signal for periods of strong High Yield Debt returns. And while these metrics have been created with a knowledge of the past, they certainly corroborate the simple narrative we created above. Possible next steps include applying these signals to other developed markets while being mindful of their current position in their interest rate cycle, improving these signals, and assessing hypothetical performance, etc… I leave these as exercises to the reader…\n\n\nThe Future: Where are we Heading?\nIn our above research, one of our key takeaways involved the Federal Funds Rate (i.e. the ‘interest’ rate). This should be self-evident as the interest rate measures the cost of borrowing money and is therefore the primary drivers of the economy and financial markets. We noted that during periods of economic hurt (typically after some sort of ‘bubble’) the Fed would lower rates and kick the can down the road. However, we have now reached that special point in time wherein interest rates can no longer be lowered. Instead, the Fed has been forced to aggressively raise rates, subsequently popping the Tech bubble. This makes for an interesting investment environment going forward. We have noted that High Yield Debt performs better during these reactionary, declining interest rate environments, and equities during the post-recovery ‘rising’ rate environment. However, in spite of all this, my outlook for equities is still slightly more pessimistic. Instead, I think that High Yield Debt is likely to outperform over the next 2 years, followed by Equities picking back up…\n\n\nFinal Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "research/02_weather_and_markets/index.html",
    "href": "research/02_weather_and_markets/index.html",
    "title": "Weather & the Stock Market",
    "section": "",
    "text": "In recent years, behavioral finance - the field of study that combines psychology and economics to better understand financial decision making - has grown in popularity. There have been many studies that prove the irrationality of human decision-making processes due to psychological and emotional factors. With this in mind, we will investigate if weather conditions in New York have any noticeable impact on daily stock market returns.\nLet’s load the data…\n\nWeather DataStock Market DataVariable Definitions\n\n\n\n\nCode\nweather_data <- read_rds(here(\"raw_data\", \"Weather and Markets\", \"new_york_weather_data_clean.rds\"))\n\nweather_data %>%\n    # head() %>%\n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>%\n    mutate(across(.cols = where(~is.numeric(.x)), .fns = round)) %>%\n    datatable()\n\n\n\n\n\n\n\n\n\nWe will consider the SP500 Index as a proxy for the stock market:\n\n\nCode\nstock_data <- tq_get(\"^GSPC\", from = \"1978-12-29\")\n\nstock_data <- stock_data %>% \n    mutate(pct_ret = (adjusted / lag(adjusted)) - 1) %>% \n    slice(-1) %>% \n    select(date, pct_ret)\n\nstock_data %>% \n    head() %>% \n    set_names(c(\"Date\", \"Return (%)\")) %>% \n    gt() %>% \n    gt::fmt_percent(columns = 2)\n\n\n\n\n\n\n  \n  \n    \n      Date\n      Return (%)\n    \n  \n  \n    1979-01-02\n0.65%\n    1979-01-03\n1.11%\n    1979-01-04\n0.80%\n    1979-01-05\n0.56%\n    1979-01-08\n−0.33%\n    1979-01-09\n0.54%\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDefinition\n\n\n\n\nTod\nThe time of day (Morning, Midday, Afternoon).\n\n\nTemp\nThe temperature in degrees Fahrenheit.\n\n\nVisibility\nThe maximum distance at which an object can clearly be discerned.\n\n\nDew Point\nThe minimum threshold temperature that results in a relative humidity level of 100%.\n\n\nFeels Like\nA measure of how hot/cold it feels like outside when accounting for other variables like wind chill, humidity, etc.\n\n\nTemp Min\nThe minimum temperature during the associated time stamp.\n\n\nTemp Max\nThe maximum temperature during the associated time stamp.\n\n\nPressure\nThe weight of the air. High air pressure (heavy air) is associated with calm weather conditions whereas low air pressure (light air) is associated with active weather conditions.\n\n\nHumidity\nThe amount of water vapor in the air.\n\n\nWind Speed\nThe speed of the wind in miles per hour.\n\n\nWind Deg\nThe direction of the wind in circular degrees.\n\n\nClouds All\nCloudiness of the sky in percent.\n\n\nWeather Id\nThe ID code associated with the weather.\n\n\nWeather Main\nThe Primary Weather Category.\n\n\nWeather Description\nThe Secondary Weather Category.\n\n\nWeather Icon\nThe ID code of the icon being displayed on weather apps."
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#sp500-returns",
    "href": "research/02_weather_and_markets/index.html#sp500-returns",
    "title": "Weather & the Stock Market",
    "section": "SP500 Returns",
    "text": "SP500 Returns\n\n\nCode\navg_ret <- stock_data %>% \n    summarize(avg_ret = mean(pct_ret)) %>% \n    pull(avg_ret)\n\nstock_data %>% \n    ggplot(aes(date, pct_ret)) +\n    geom_point(alpha = .5) +\n    geom_hline(yintercept = avg_ret, color = \"red\") +\n    theme_bw() +\n    labs(\n        y = \"\", x = \"\",\n        title = \"SP500 Daily Return (%)\",\n        subtitle = str_glue(\"Average: {scales::percent(avg_ret, accuracy = .0001)}\")\n    ) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size=15))\n\n\n\n\n\nAs we can see from the data, there are several days with extreme returns; on October 19, 1987 (‘Black Monday’) the market declined by approximately 22%, and in March of 2020, the stock market dipped when news of the Covid-19 pandemic arose. While these events are extremely important from a historical perspective, it is unlikely that the weather contributed significantly to these extreme returns. Therefore, we will consider days like these to be outliers, and we will remove them from our data. Here is a cleaned version of the data:\n\n\nCode\nstock_summary <- stock_data %>% \n    summarize(\n        mean = mean(pct_ret, na.rm = T),\n        st_dev = sd(pct_ret, na.rm = T)\n    )\n\nstock_data %>% \n    mutate(is_outlier = case_when(\n        pct_ret > stock_summary$mean + 2*stock_summary$st_dev ~ \"Outlier\",\n        pct_ret < stock_summary$mean - 2*stock_summary$st_dev ~ \"Outlier\",\n        T ~ \"Not Outlier\"\n    )) %>% \n    ggplot(aes(date, pct_ret, color = is_outlier)) +\n    geom_point() +\n    theme_bw() +\n    labs(\n        y = \"\", x = \"\",\n        title = \"SP500 Daily Return (%)\",\n        color = \"\"\n    ) +\n    scale_color_hue(direction = -1) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size=15), legend.position = \"top\")\n\n\n\n\n\nCode\nstock_data <- stock_data %>% \n    mutate(is_outlier = case_when(\n        pct_ret > stock_summary$mean + 2*stock_summary$st_dev ~ \"Outlier\",\n        pct_ret < stock_summary$mean - 2*stock_summary$st_dev ~ \"Outlier\",\n        T ~ \"Not Outlier\"\n    )) %>% \n    filter(is_outlier == \"Not Outlier\") %>% \n    select(-is_outlier)\n\n\nGoing forward, we will solely use the blue data points…"
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#the-impact-of-the-weather",
    "href": "research/02_weather_and_markets/index.html#the-impact-of-the-weather",
    "title": "Weather & the Stock Market",
    "section": "The Impact of the Weather",
    "text": "The Impact of the Weather\nLet’s examine the returns on days with different morning weather conditions for each month:\n\nJanFebMarAprMayJunJulAugSepOctNovDec\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\n\n\n\nBlack lines represent the average of the group whereas the red line represents the average across all groups\n\n\n\n\n\n\nAs we can see from the above plots, each of the distributions are relatively similar for different morning weather conditions. Therefore, morning weather seems to have little effect on the distribution of daily stock market returns.\nLet’s investigate if temperature differences have any impact on market returns…"
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#the-impact-of-temperature-differences",
    "href": "research/02_weather_and_markets/index.html#the-impact-of-temperature-differences",
    "title": "Weather & the Stock Market",
    "section": "The Impact of Temperature Differences",
    "text": "The Impact of Temperature Differences\nLet’s hypothesize that on days where it is colder than usual, returns are worse than days where it is warmer than usual. To quantify this hypothesis, let’s see if the difference of Feels Like from that month’s average Feels Like yields any interesting results on stock market returns:\n\nJanFebMarAprMayJunJulAugSepOctNovDec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLikewise, there is no evidence that variations in temperature can help explain daily stock returns."
  },
  {
    "objectID": "research/02_weather_and_markets/index.html#simple-modelling---linear-regression",
    "href": "research/02_weather_and_markets/index.html#simple-modelling---linear-regression",
    "title": "Weather & the Stock Market",
    "section": "Simple Modelling - Linear Regression",
    "text": "Simple Modelling - Linear Regression\nFrom our brief analysis above, it seems unlikely that we will be able to use weather data to model stock market returns accurately, but let’s run through a quick linear regression and examine the results.\n\n\nCode\ndata_prep <- data %>% \n    left_join(feels_like_summary) %>% \n    mutate(feels_like_difference = feels_like - avg_feels_like) %>% \n    select(-avg_feels_like) %>% \n    filter(tod == \"Morning\") %>% \n    filter(!is.na(pct_ret)) %>% \n    mutate(wday = wday(date, label = T)) %>% \n    select(date, month, wday, everything(), -tod, -weather_id, -weather_icon)\n\nlm_output <- data_prep %>% \n    select(pct_ret, month, weather_main, feels_like, feels_like_difference) %>% \n    lm(formula = pct_ret ~ . - 1) %>% \n    summary()\n\nlm_output %>% \n    broom::glance() %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>%\n    mutate(across(.cols = where(~is.numeric(.x)), .fns = ~round(.x, digits = 4))) %>% \n    gt()\n\n\n\n\n\n\n  \n  \n    \n      R.squared\n      Adj.r.squared\n      Sigma\n      Statistic\n      P.value\n      Df\n      Df.residual\n      Nobs\n    \n  \n  \n    0.0068\n0.0048\n0.0081\n3.298\n0\n22\n10544\n10567\n  \n  \n  \n\n\n\n\n\n\nCode\nlm_output %>% \n    broom::tidy() %>% \n    arrange(p.value) %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>%\n    mutate(across(.cols = where(~is.numeric(.x)), .fns = ~round(.x, digits = 3))) %>%\n    DT::datatable()\n\n\n\n\n\n\n\nOnce again, we confirm that the weather cannot help explain variation in daily stock returns (with our model only explaining .7%). In fact, the month of the year seems to be more significant than the weather when explaining daily stock return variation."
  },
  {
    "objectID": "research/05_PEAD/PEAD.html",
    "href": "research/05_PEAD/PEAD.html",
    "title": "★ Post-Earnings Announcement Drift (PEAD) Anomaly & Return Cyclicality",
    "section": "",
    "text": "Post-Earnings Announcement Drift (PEAD) anomaly is a phenomenon where the stock prices of firms continue to exhibit abnormal returns after they release their earnings reports. Specifically, the stock prices of firms that have reported positive earnings surprises tend to continue to rise in the days and weeks following the release of their earnings report, while the stock prices of firms that have reported negative earnings surprises tend to continue to decline.\nTo be clear, we will consider earnings surprise to be equal to:\n\\[\nAvg\\ Analyst\\ Estimate\\ of\\ EPS - Actual\\ Reported\\ EPS\n\\]\nThe PEAD anomaly is considered an anomaly because it contradicts the Efficient Market Hypothesis (EMH), which suggests that all available information is immediately reflected in stock prices, leaving no room for abnormal returns.\nSeveral explanations have been proposed to explain the PEAD anomaly like the fact that investors may underestimate the persistence of a firm’s earnings surprises, leading to delayed price adjustments. Another explanation is that earnings surprises may contain information that is not captured in traditional accounting measures, leading to a delayed reaction by the market.\nRegardless of the reason for its existence, the PEAD anomaly has important implications for investors and financial managers, as it suggests that trading strategies based on earnings surprises can generate abnormal returns. In addition, it also highlights the limitations of the EMH, which is a fundamental theory in finance.\n\nLet’s investigate the presence of PEAD and attempt to build a trading strategy with the use of Machine Learning. To do this I have gathered:\n\nEarnings History on 1,844 large, publicly-traded companies\nPrice, Volume, and Miscellaneous Financial data for each of these companies"
  },
  {
    "objectID": "research/05_PEAD/PEAD.html#comparing-performance-to-a-benchmark",
    "href": "research/05_PEAD/PEAD.html#comparing-performance-to-a-benchmark",
    "title": "★ Post-Earnings Announcement Drift (PEAD) Anomaly & Return Cyclicality",
    "section": "Comparing Performance to a Benchmark",
    "text": "Comparing Performance to a Benchmark\nRather than assess nominal performance, we will now calculate performance relative to the SP500. To do this, we will create a new metric called Abnormal Return which is calculated as follows:\n\\[\nAR = Return_{(i,\\ t)} - B_i(Return_{(SP500, \\ t)})\n\\]\nwhere i represents the ith company, t represents the time period, and B represents the stock’s Beta.\nTo be clear, a company’s Beta can be thought of as follows - if the market (i.e. SP500) moves +1% in a day, on average, the company moves B%. Therefore, in the above, we are calculating each company’s daily returns in excess of a basic expectation."
  },
  {
    "objectID": "research/05_PEAD/PEAD.html#assessing-the-impact-of-after-market-trading",
    "href": "research/05_PEAD/PEAD.html#assessing-the-impact-of-after-market-trading",
    "title": "★ Post-Earnings Announcement Drift (PEAD) Anomaly & Return Cyclicality",
    "section": "Assessing the Impact of After-Market Trading",
    "text": "Assessing the Impact of After-Market Trading\nIt is important to note that rather than executing a trading halt and releasing earnings during trading hours, most firms release earnings during after-market hours. As such, we would expect much of the stock price movement to occur during after-hours trading. In the above, daily returns were calculated between closing prices of consecutive days which implicitly includes stock price movement that occurs after-hours, but we should compare stock price movements when removing the effect of after-hours trading.\nTo do this, we will replace the return for each day after an earnings announcement with the return that occurs between the open and close. This will allow us to distinguish the amount of stock price movement that is realized after-hours."
  },
  {
    "objectID": "research/05_PEAD/PEAD.html#visualization",
    "href": "research/05_PEAD/PEAD.html#visualization",
    "title": "★ Post-Earnings Announcement Drift (PEAD) Anomaly & Return Cyclicality",
    "section": "Visualization",
    "text": "Visualization\nAfter accounting for the above, we can again visualize company returns around earnings announcements:\n\nNominal ($) Earnings SurprisePct (%) Earnings Surprise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot above demonstrates a few key points:\n\nThe majority of alpha is generated during after-hours trading\n\nThis is logical as we would expect the quantitative investment firms with fast computers to act quickly on this new information.\n\nFor the most extreme earnings surprise observations (buckets 1 & 9) the market appears to somewhat expect these results in the days leading to the earnings release.\nIn comparison to our previous plots, the PEAD effect is less noticeable when considering SP500 companies only which indicates that PEAD is more pronounced for smaller companies.\n\nI would also like to point out that the variation that occurs at t = 0 (Earnings Release Day) is abnormal and it is likely due to the fact that some companies release earnings in the morning. However, I was not able to gather information regarding the time of announcement. Therefore, this variation should be ignored. Let’s visualize average CAR starting at time t = 1.\n\nNominal ($) Earnings SurprisePct (%) Earnings Surprise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy focusing our starting period at the day after an earnings release, we clearly see that almost all of the alpha is generated during after-hours. It is also clear that PEAD exists; on average, stocks continue to rise (in excess of what would be expected given current market conditions) when they beat on earnings, or they continue to fall when they under-perform on earnings. However, as we can see from the plot, the marginal benefit of entering a position the morning after an earnings release for 20 trading days ranges from approximately 0% to 0.5%.\nBecause this marginal benefit is relatively small, and there is likely a lot of variation around these averages, it is unlikely that building a PEAD trading strategy would be significantly profitable relative to a simple long position in the SP500, especially when considering frictional trading costs."
  },
  {
    "objectID": "research/04_financial_bubbles/financial_bubbles.html",
    "href": "research/04_financial_bubbles/financial_bubbles.html",
    "title": "★Assessing Financial Bubbles",
    "section": "",
    "text": "Financial markets have experienced several notable financial bubbles in recent history. The Dot-Com bubble in 2001, the Great Financial Crisis in 2008, and the Tech bubble in 2022 are three examples. In each case, the period leading up to, during, and after the bubble’s collapse brought varying degrees of market volatility and returns. Investors who exited before the crash were able to achieve impressive returns, while those who remained in the market experienced significant losses. As such, it is valuable for investors to be able to identify where financial bubbles are forming and their stage of development to avoid potential losses.\nIn his book Principles for Navigating Big Debt Crises, Ray Dalio posits that markets are cyclical and that financial bubbles form for the same underlying reasons - healthy growth turns to unhealthy growth as entities over-lever which eventually yields the bust of a financial bubble; he outlines the following rough framework for identifying financial bubbles with the associated message:\n\n\n\n\n\n\nAt this point I want to emphasize that it is a mistake to think that any one metric can serve as an indicator of an impending debt crisis. The ratio of debt to income for the economy as a whole, or even debt service payments to income for the economy as a whole, which is better, are useful but ultimately inadequate measures. To anticipate a debt crisis well, one has to look at the specific debt-service abilities of the individual entities, which are lost in these averages. More specifically, a high level of debt or debt service to income is less problematic if the average is well distributed across the economy than if it is concentrated — especially if it is concentrated in key entities.\n- Ray Dalio\n\nWhile the above framework is tailored for large-scale economic bubbles that drastically affected national economies, we will extend, adapt, and apply this framework to create a Grand Financial Bubble Metric and identify financial bubbles within various sectors of the U.S. economy and markets."
  },
  {
    "objectID": "research/04_financial_bubbles/financial_bubbles.html#methods-for-quantifying-the-framework",
    "href": "research/04_financial_bubbles/financial_bubbles.html#methods-for-quantifying-the-framework",
    "title": "★Assessing Financial Bubbles",
    "section": "Methods for Quantifying the Framework",
    "text": "Methods for Quantifying the Framework\nThe following table outlines certain figures that can help us measure each component of Dalio’s Rubric:\n\n\n\n\n\n\n\nFramework Component\nWays to Quantify\n\n\n\n\nAre prices high relative to traditional measures?\n\nPrice-to-Earnings (P/E) Ratio: The P/E ratio is a popular metric for valuing stocks that compares the stock price to the earnings per share. A high P/E ratio suggests that the stock is overvalued relative to its earnings.\nPrice-to-Book (P/B) Ratio: The P/B ratio compares a company’s stock price to its book value (i.e., the value of its assets minus its liabilities). A high P/B ratio may suggest that the stock is overvalued relative to its book value.\nPrice-to-Sales (P/S) Ratio: The P/S ratio compares a company’s stock price to its sales per share. A high P/S ratio may suggest that the stock is overvalued relative to its sales.\n\n\n\nAre prices discounting future rapid price appreciation?\n\nOptions Market: The options market can provide clues as to whether investors are expecting rapid price appreciation in the future. For example, a high call option volume or a low put option volume may suggest that investors are optimistic about future price movements.\n\n\n\nAre purchases being financed by high leverage?\n\nDebt-to-Equity Ratio: The debt-to-equity ratio is a common measure of a company’s leverage, and it compares the amount of debt the company has to its equity. A high debt-to-equity ratio may suggest that the company is financing its purchases with a significant amount of debt.\nDebt-to-GDP: A measure of debt relative to the income of an entire economy.\n\n\n\nAre buyers/companies making forward purchases?\n\nInventory Levels: If a company’s inventory levels are high, it may suggest that the company is making forward purchases to secure goods at current prices for future use or resale.\nPurchase Orders: Publicly traded companies are required to disclose purchase orders and contracts with suppliers in their financial statements. By analyzing these disclosures, investors can gain insight into the company’s purchasing activities and determine if they are making forward purchases.\nCapital Expenditures: Companies may make forward purchases as part of their capital expenditures, which are investments in long-term assets. For example, a manufacturing company may purchase machinery and equipment in advance to increase production capacity.\n\n\n\nHave new participants entered the market?\n\nIPO data from SEC Regulatory Filings: Publicly traded companies are required to file regulatory reports to the SEC when going public.\nComposition of Indices: The composition of Indices like the Russell 3000 across different sectors can give clues as to whether new participants have entered a specific market segment.\nNumber of Employees: Tracking the proportion of employees in a certain sector relative to the entire workforce may provide clues to whether new participants have entered a specific market.\n\n\n\nIs there broad bullish sentiment?\n\nNews Sentiment…\nInterview Market Leaders…\n\n\n\nDoes tightening risk popping the bubble?\n\nInterest Coverage Ratio: The interest coverage ratio measures a company’s ability to pay its interest expenses on its debt. If a company’s interest coverage ratio is low, it may suggest that the company is struggling to make its interest payments. Assessing hypothetical interest coverage ratios if interest rates increase will likely help answer this question.\n\n\n\n\nGoing forward, we will only consider the following framework components, as I believe they best encapsulate the main underpinnings of the framework and are simultaneously easy to quantify:\n\nAre prices high relative to traditional measures?\nAre purchases being financed w/ high leverage?\nHave new participants entered the market?\nDoes monetary tightening risk popping the bubble?*\n\n\n\n*We will qualitatively consider a variation of this section of the framework after building our Grand Financial Bubble Metric"
  },
  {
    "objectID": "research/04_financial_bubbles/financial_bubbles.html#the-data",
    "href": "research/04_financial_bubbles/financial_bubbles.html#the-data",
    "title": "★Assessing Financial Bubbles",
    "section": "The Data",
    "text": "The Data\nIn order to do this, I have collected:\n\nKey figures from the Income Statements, Balance Sheets, and Cash Flow Statements of all the companies in the Russell 3000 Index since 1996, including the sector classification of each company\nGlobal IPO data since 1993\nEconomic Data (GDP, Federal Funds Rate, etc.)\n\nHere is a glimpse of each data-set:\n\nRussell 3000 DataIPO Data\n\n\n\nIncome StatementBalance SheetCash Flow StatementMiscellaneousClassification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTicker\nName\nRevenue\nCogs\nGross Profit\nInterest Expense\nInterest Income\nOperating Income\nNet Income\n\n\n\n\n2022\nA\nAgilent Technologies Inc\n$6,848M\n$3,126M\n$3,722M\n$84M\n$9M\n$1,618M\n$1,254M\n\n\n2022\nAA\nAlcoa Corp\n$12,152M\n$9,153M\n$2,999M\n$195M\n$0M\n$949M\n$429M\n\n\n2022\nAADI\nAadi Bioscience Inc\n$1M\nNA\nNA\n$1M\n$0M\n($111M)\n($110M)\n\n\n2022\nAAL\nAmerican Airlines Group Inc\n$29,882M\nNA\nNA\n$1,800M\n$18M\n($1,059M)\n($1,993M)\n\n\n2022\nAAN\nAaron’s Co Inc/The\n$1,846M\n$686M\n$1,159M\n$1M\n$0M\n$146M\n$110M\n\n\n2022\nAAON\nAAON Inc\n$535M\n$397M\n$138M\n$0M\n$0M\n$69M\n$59M\n\n\n2022\nAAP\nAdvance Auto Parts Inc\n$10,998M\n$6,069M\n$4,929M\n$38M\n$0M\n$839M\n$616M\n\n\n2022\nAAPL\nApple Inc\n$394,328M\n$223,546M\n$170,782M\n$2,931M\n$2,825M\n$119,437M\n$99,803M\n\n\n2022\nAAT\nAmerican Assets Trust Inc\n$376M\nNA\nNA\n$59M\n$0M\n$100M\n$37M\n\n\n2022\nAAWW\nAtlas Air Worldwide Holdings Inc\n$4,031M\n$2,728M\n$1,303M\n$99M\n$1M\n$711M\n$493M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTicker\nName\nCurrent Assets\nNon Current Assets\nTotal Assets\nInventory\nCurrent Liabilities\nNon Current Liabilities\nTotal Liabilities\nSt Debt\nLt Debt\nTotal Debt\nTotal Equity\n\n\n\n\n2022\nA\nAgilent Technologies Inc\n$3,778M\n$6,754M\n$10,532M\n$1,038M\n$1,861M\n$3,366M\n$5,227M\n$87M\n$2,834M\n$2,921M\n$5,305M\n\n\n2022\nAA\nAlcoa Corp\n$5,026M\n$9,999M\n$15,025M\n$1,956M\n$3,223M\n$5,518M\n$8,741M\n$36M\n$1,790M\n$1,826M\n$6,284M\n\n\n2022\nAADI\nAadi Bioscience Inc\n$151M\n$7M\n$158M\n$0M\n$15M\n$6M\n$22M\n$0M\n$0M\n$1M\n$136M\n\n\n2022\nAAL\nAmerican Airlines Group Inc\n$17,336M\n$49,131M\n$66,467M\n$1,795M\n$19,006M\n$54,801M\n$73,807M\n$3,996M\n$42,181M\n$46,177M\n($7,340M)\n\n\n2022\nAAN\nAaron’s Co Inc/The\n$824M\n$617M\n$1,441M\n$772M\n$245M\n$478M\n$723M\n$95M\n$320M\n$415M\n$718M\n\n\n2022\nAAON\nAAON Inc\n$218M\n$432M\n$650M\n$130M\n$87M\n$97M\n$184M\n$0M\n$40M\n$40M\n$466M\n\n\n2022\nAAP\nAdvance Auto Parts Inc\n$6,275M\n$5,919M\n$12,194M\n$4,659M\n$5,180M\n$3,886M\n$9,066M\n$465M\n$3,372M\n$3,837M\n$3,128M\n\n\n2022\nAAPL\nApple Inc\n$135,405M\n$217,350M\n$352,755M\n$4,946M\n$153,982M\n$148,101M\n$302,083M\n$22,773M\n$109,707M\n$132,480M\n$50,672M\n\n\n2022\nAAT\nAmerican Assets Trust Inc\nNA\nNA\n$3,018M\nNA\nNA\nNA\nNA\n$1,538M\n$142M\n$1,680M\n$1,210M\n\n\n2022\nAAWW\nAtlas Air Worldwide Holdings Inc\n$1,326M\n$5,117M\n$6,443M\n$0M\n$1,420M\n$2,214M\n$3,634M\n$695M\n$1,821M\n$2,516M\n$2,809M\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTicker\nName\nFree Cash Flow\nCapex\n\n\n\n\n2022\nA\nAgilent Technologies Inc\n$1,021M\n($291M)\n\n\n2022\nAA\nAlcoa Corp\n$530M\n($390M)\n\n\n2022\nAADI\nAadi Bioscience Inc\n($22M)\n$0M\n\n\n2022\nAAL\nAmerican Airlines Group Inc\n$496M\n($208M)\n\n\n2022\nAAN\nAaron’s Co Inc/The\n$43M\n($93M)\n\n\n2022\nAAON\nAAON Inc\n$6M\n($55M)\n\n\n2022\nAAP\nAdvance Auto Parts Inc\n$823M\n($290M)\n\n\n2022\nAAPL\nApple Inc\n$111,443M\n($10,708M)\n\n\n2022\nAAT\nAmerican Assets Trust Inc\n$64M\n($105M)\n\n\n2022\nAAWW\nAtlas Air Worldwide Holdings Inc\n$833M\n($90M)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTicker\nName\nNum Employees\nPrice\nMarket Cap\nPrice To Sales\nPrice To Book\nPrice To Earnings\nNum Shares\nInventory Turnover\n\n\n\n\n2022\nA\nAgilent Technologies Inc\n18.1K\n$150.04\n$40,813M\n6.0x\n7.7x\n29.4x\n295.00M\n3.3x\n\n\n2022\nAA\nAlcoa Corp\n12.2K\n$44.58\n$5,956M\n0.5x\n1.1x\n4.2x\n176.94M\n5.5x\n\n\n2022\nAADI\nAadi Bioscience Inc\n12\n$12.31\n$345M\n27.1x\n2.0x\nNA\n24.40M\nNA\n\n\n2022\nAAL\nAmerican Airlines Group Inc\n123.4K\n$12.74\n$7,824M\n0.2x\nNA\nNA\n649.86M\nNA\n\n\n2022\nAAN\nAaron’s Co Inc/The\n9.2K\n$11.85\n$299M\n0.1x\n0.4x\n5.8x\n30.78M\n0.9x\n\n\n2022\nAAON\nAAON Inc\n2.9K\n$74.83\n$2,867M\n3.7x\n5.5x\n40.9x\n53.21M\n3.7x\n\n\n2022\nAAP\nAdvance Auto Parts Inc\n41.0K\n$151.54\n$9,630M\n0.9x\n3.5x\n14.6x\n59.70M\n1.3x\n\n\n2022\nAAPL\nApple Inc\n164.0K\n$125.07\n$2,398,369M\n6.2x\n47.3x\n24.6x\n15,943.43M\n38.8x\n\n\n2022\nAAT\nAmerican Assets Trust Inc\n208\n$26.51\n$1,557M\n3.7x\n1.3x\n36.2x\n60.53M\nNA\n\n\n2022\nAAWW\nAtlas Air Worldwide Holdings Inc\n4.1K\n$101.25\n$2,711M\n0.6x\n0.9x\n7.2x\n28.36M\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTicker\nName\nClassification 1\nClassification 2\nClassification 3\nClassification 4\nClassification 5\nClassification 6\nClassification 7\nClassification 8\n\n\n\n\n2022\nA\nAgilent Technologies Inc\nHealth Care\nHealth Care\nMedical Equipment & Devices\nLife Science & Diagnostics\nLife Science Equipment\nLife Science Equipment\nLife Science Equipment\nLife Science Equipment\n\n\n2022\nAA\nAlcoa Corp\nMaterials\nMaterials\nMetals & Mining\nBase Metals\nAluminum\nAluminum\nAluminum\nAluminum\n\n\n2022\nAADI\nAadi Bioscience Inc\nHealth Care\nHealth Care\nBiotech & Pharma\nBiotech\nBiotech\nBiotech\nBiotech\nBiotech\n\n\n2022\nAAL\nAmerican Airlines Group Inc\nIndustrials\nIndustrial Services\nTransportation & Logistics\nAirlines\nFull Service Airline\nFull Service Airline\nFull Service Airline\nFull Service Airline\n\n\n2022\nAAN\nAaron’s Co Inc/The\nConsumer Discretionary\nConsumer Discretionary Services\nConsumer Services\nConsumer Goods Rental\nConsumer Goods Rental\nConsumer Goods Rental\nConsumer Goods Rental\nConsumer Goods Rental\n\n\n2022\nAAON\nAAON Inc\nIndustrials\nIndustrial Products\nElectrical Equipment\nComml & Res Bldg Equip & Sys\nHVAC Building Products\nA/C Heating & Fridge Equip\nA/C Heating & Fridge Equip\nA/C Heating & Fridge Equip\n\n\n2022\nAAP\nAdvance Auto Parts Inc\nConsumer Discretionary\nRetail & Whsle - Discretionary\nRetail - Discretionary\nAutomotive Retailers\nAuto Parts & Accessories Stores\nAuto Parts & Accessories Stores\nAuto Parts & Accessories Stores\nAuto Parts & Accessories Stores\n\n\n2022\nAAPL\nApple Inc\nTechnology\nTech Hardware & Semiconductors\nTechnology Hardware\nCommunications Equipment\nMobile Phones\nMobile Phones\nMobile Phones\nMobile Phones\n\n\n2022\nAAT\nAmerican Assets Trust Inc\nReal Estate\nReal Estate\nREIT\nMulti Asset Class REIT\nMulti Asset Class REIT\nMulti Asset Class REIT\nMulti Asset Class REIT\nMulti Asset Class REIT\n\n\n2022\nAAWW\nAtlas Air Worldwide Holdings Inc\nIndustrials\nIndustrial Services\nTransportation & Logistics\nAir Freight\nAir Freight\nAir Freight\nAir Freight\nAir Freight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnounced Date\nTicker\nIssuer Name\nSize\nClassification 1\n\n\n\n\n2021-10-01\nRIVN US EQUITY\nRivian Automotive Inc\n$13,724,100,000M\nConsumer Discretionary\n\n\n2021-10-04\nGFS US EQUITY\nGLOBALFOUNDRIES Inc\n$2,855,160,000M\nTechnology\n\n\n2021-07-01\nHOOD US EQUITY\nRobinhood Markets Inc\n$2,255,460,000M\nFinancials\n\n\n2021-06-24\nSPNGU US EQUITY\nSpinning Eagle Acquisition Cor\n$2,000,000,000M\nFinancials\n\n\n2021-08-27\nOLPX US EQUITY\nOlaplex Holdings Inc\n$1,779,850,000M\nConsumer Staples\n\n\n2022-03-28\nCRBG US EQUITY\nCorebridge Financial Inc\n$1,680,000,000M\nFinancials\n\n\n2021-06-21\nRYAN US EQUITY\nRyan Specialty Holdings Inc\n$1,538,220,000M\nFinancials\n\n\n2021-06-03\nS US EQUITY\nSentinelOne Inc\n$1,408,750,000M\nTechnology\n\n\n2021-11-04\nHCP US EQUITY\nHashiCorp Inc\n$1,322,400,000M\nTechnology\n\n\n2021-10-15\nHTZ US EQUITY\nHertz Global Holdings Inc\n$1,291,080,000M\nConsumer Discretionary"
  },
  {
    "objectID": "research/04_financial_bubbles/financial_bubbles.html#quantifying-the-framework",
    "href": "research/04_financial_bubbles/financial_bubbles.html#quantifying-the-framework",
    "title": "★Assessing Financial Bubbles",
    "section": "Quantifying the Framework",
    "text": "Quantifying the Framework\nIn the following, we will inspect each element of the framework with the data available to us:\n\n\n1) Prices are High Relative to Traditional Measures\nLet’s take a look at the historical P/S, P/B, and P/E ratios of each sector. To do this, we will plot the median value of these ratios since 1995 for different sectors while also aggregating by company size according to the following schema:\n\n\n\n\n\n\n\nSmall Cap\nMarket Cap between 0th-40th Percentile for its Sector\n\n\nMid Cap\nMarket Cap between 40th-80th Percentile for its Sector\n\n\nLarge Cap\nMarket Cap between 80th-95th Percentile for its Sector\n\n\nUltra-Large Cap\nMarket Cap between 95th-100th Percentile for its Sector\n\n\n\n\nPrice-to-SalesPrice-to-BookPrice-to-Earnings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above is overwhelming, but it does paint the general picture that financial bubbles have formed when prices are high relative to traditional measures. For instance, the Dot-Com Bubble of ’00/’01 is seen most uniformly in the above with high price ratios across the board.\nHowever, let’s try to represent the data in a more concise and standardized fashion.\n\nCreating a Price Metric\nLet’s create a standard Price Metric by:\n\nCalculating the distance of each of these ratios from a median line\nDividing this distance by the average absolute distance from the median line\nAveraging the standardized distances for the P/S, P/B, & P/E ratios together to a create a single Price Metric\n\nBy doing this we will have a rough estimation of how elevated prices are for each of these sectors and market cap buckets.\n\n\n\n\n\n\nTakeaways:\nFrom eye-balling the above charts we can conclude that the following sectors were highly priced relative to traditional measures:\n\n\n\n\n\n\n\nSector\nTime Periods\n\n\n\n\nCommunications\n’99 - ’01, ’20 - ’22\n\n\nConsumer Discretionary\n’20 - ’22\n\n\nConsumer Staples\n’97 - ’02, ’22\n\n\nEnergy\n’00, ’17 - ’18\n\n\nFinancials\n’98 - ’02, ’18\n\n\nHealth Care\n’98-’02, ’20 - ’22\n\n\nIndustrials\n’20 - ’22\n\n\nMaterials\n’20 - ’22\n\n\nReal Estate\n’19 - ’22\n\n\nTechnology\n’99 - ’02, ’20 - ’23\n\n\nUtilities\n’01, ’07, ’18 - ’23\n\n\n\nAs we continue through our framework, we will see if the same time periods resurface.\n\n\n\nDigging Deeper\nNow that we have time periods in which certain broad sectors were highly priced, we can repeat the above at a more granular level. This is important since large values can be washed away in averages when aggregating at broader hierarchical sector levels. For instance, we could have a high price metric of +3 for Semiconductors and a low metric of -3 for Software; if these were the only two sub-sectors in the Technology space, then the price metric for Technology would likely lie around 0. This scenario is much different from one wherein both Semiconductors and Software have price metrics of 0, which would also consequently yield a price metric of around 0 for Technology.\nIn the following, we consider only Large Cap and Ultra-Large Cap as there are fewer companies in each bucket when aggregating at a more granular sector level:\n\nHealth CareMaterialsIndustrialsConsumer DiscretionaryTechnologyReal EstateFinancialsCommunicationsEnergyConsumer StaplesUtilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2) Purchases are Being Financed w/ High Leverage\nLet’s take a look at the historical debt levels of each sector since 1995. Once again, we will aggregate by company size and plot the median value for different sectors. In this section we will consider two metrics to assess debt levels:\n\\[\nMetric_1 =  \\frac{D_t}{E_t} = \\frac{LT\\ Debt_t + ST\\ Debt_t}{Equity_t}\n\\]\n\\[\nMetric_2 = \\frac{Avg\\ Debt_t}{GDP_t} = \\frac{(\\frac{\\sum{LT\\ Debt_t + ST\\ Debt_t}}{No.\\ of\\ Companies_t})}{GDP_t}\n\\]\nThe first metric aims to represent leverage at the company level, whereas the second metric aims to represent the significance of that debt in relation to the size of the general economy.\nIn the second metric, we consider the average debt level as a ratio to GDP because the number of companies within each sector of the Russell 3000 varies each year. Therefore, simply calculating the sum for each year could overstate debt levels for years in which there are more companies than usual. Instead, we hope to measure this in isolation in the section 3) New entrants are entering the market.\n\nMetric 1Metric 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the data is not crystal clear, the above generally corroborates the idea that financial bubbles have formed during times of increased leverage. Again, let’s try to represent the data in a more concise and standard fashion.\n\nCreating a Debt Metric\nLet’s create a standard Debt Metric by:\n\nCalculating the distance of each of these ratios from a median line\nDividing this distance by the average absolute distance from the median line\nAveraging the standardized distances for Metric 1 & Metric 2 with the respective weights of 90% and 10%*\n\n\n\n*Metric 1 is more stationary and cyclical; therefore, we are giving it a greater weight as it will be more reliable when measuring absolute debt levels.\n\n\n\n\n\n\nTakeaways:\nLet’s repeat our process and identify time periods where debt levels were high in each sector:\n\n\n\n\n\n\n\nSector\nTime Periods\n\n\n\n\nCommunications\n’97 - ’99, ’02, ’10, ’18 - ’21\n\n\nConsumer Discretionary\n’18, ’20-’22\n\n\nConsumer Staples\n\n\n\nEnergy\n’01, ’03, ’19 - ’22\n\n\nFinancials\n’01 - ’09\n\n\nHealth Care\n’18 - ’23\n\n\nIndustrials\n’09, ’19-’23\n\n\nMaterials\n’01 - ’03, ’15, ’21\n\n\nReal Estate\n’08-’10\n\n\nTechnology\n’18 - ’23\n\n\nUtilities\n’03 - ’05, ’18 - ’23\n\n\n\n\n\n\nDigging Deeper\nOnce again, we can take a more granular look at the data:\n\nHealth CareMaterialsIndustrialsConsumer DiscretionaryTechnologyReal EstateFinancialsCommunicationsEnergyConsumer StaplesUtilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3) New Entrants to the Market\nThere are several ways that we can assess whether new players are entering markets. The most direct and simple method is to track when private companies become public (i.e. IPO data). Let’s track IPO activity by:\n\nCounting the # of IPOs each month\nSumming the IPO Offer Size each month\n\n\nIPO CountSum Offer Size ($)Avg Offer Size ($)Median Offer Size ($)\n\n\n\nAggregateSector-Level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregateSector-Level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregateSector-Level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregateSector-Level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom eye-balling the data, we notice that IPO data is a helpful indicator when assessing if markets are growing past capacity. We notice that the count of monthly IPOs is somewhat cyclical with peaks before time periods typically associated with financial bubbles (’01, ’08, ’22).\n\nCreating an IPO Metric\nLet’s create a standard IPO Metric by:\n\nCalculating the distance between a 3-month rolling average and a 4-year rolling average of IPO Count & Avg. Offer Size\nCalculating a standardized distance for both of these rolling averages\nAveraging the standardized distances together to a create a single IPO Metric\n\nBy doing this we will have a rough estimation of how ‘hot’ markets are and if the number of entrants is high relative to historical averages:\n\nAggregateSector-Level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigging Deeper\nOnce again, let’s take a more granular look at the data:\n\nHealth CareMaterialsIndustrialsConsumer DiscretionaryTechnologyReal EstateFinancialsCommunicationsEnergyConsumer StaplesUtilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom observing the above at Sector Level 3, we notice that the IPO metric behaves oddly for several segments. This odd behavior is the result of limited IPO data for those segments and we will have to address this when creating our grand summary Financial Bubble Metric."
  },
  {
    "objectID": "research/04_financial_bubbles/financial_bubbles.html#putting-it-all-together",
    "href": "research/04_financial_bubbles/financial_bubbles.html#putting-it-all-together",
    "title": "★Assessing Financial Bubbles",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nLet’s take each of our metrics, scale them to values between 0 and 1, and average them together to create a Grand Financial Bubble Metric. We will then re-scale our resulting metric between 0 and 1.\nIt is important to note that our IPO metric can take on extreme values at the sector level - Health Care in 2001 and Financials (w/ the SPAC surge) in 2021. Because these extreme points reduce the relative importance of other time periods, we will water them down and keep this in the back of our minds going forward.\n\n\n\n\n\nAt this point, I will caution the reader that any inferences taken from our Grand Financial Bubble Metric should be held with reservation. The metric allows for a good basic framing of where sectors are relative to history, but understanding the key dynamics at play within each sector, and the current macroeconomic and political environment is equally as important. In addition, when comparing across history, it is important to have a knowledge of context and qualitative factors that may affect inferential ability. For instance, the Covid-19 Pandemic drastically affected the world economy and markets in 2020-2022, so the reader may decide to skew metrics during those years according to their beliefs and theses.\nWith that said, we can see that our Grand Financial Bubble Metric may be helpful in determining sector performance. In general, we observe positive returns when the metric is relatively low and increasing. As the market frenzies and there are new competitors, higher valuations, and debt-financed growth, market returns are particularly strong. Eventually, our metric peaks, and subsequent returns are generally poor.\nThere are a few things worth noting from our plot:\n\nOur confidence in the metric varies for different market segments and understanding why is crucial.\n\nFor instance, our metric is particularly informative for the Energy, Technology, and Consumer segments, but it is difficult to decipher for the Real Estate and Financials segment. This is likely due to the fact that the former segments are cyclical in nature, and therefore distance from a measure of central tendency is informative, but for Real Estate, this is not the case. Financials, however, is certainly cyclical in relative pricing, but due to the nature of the industry, is not cyclical in its leverage; instead banks tend to have uniformly high leverage throughout time.\n\nOur metric indicated potential bubbles in ’00 and ’22, but completely missed the mark in 2008.\n\nThis is partly a result of the above explanation as well as the fact that our model does not capture any proxy for the quality of a company’s assets (this is mainly applicable to banks only). In reality, the 2008 GFC was not the result of over leverage relative to historical averages (although the fact that banks are inherently extremely levered was certainly crucial), but instead was the result of poor investment decisions made by these financial institutions, which could only have been deciphered with logic and analysis from the balance sheet of banking institutions. It is likely that this will be the case for all traditional banking crises, due to the nature of the industry, and therefore it would be a good idea to try and built a robust, sector-specific Banking Crisis Metric.\n\nOur metric provides a framework for historical comparison, but it does not provide a time-based trading signal (and it should not be looked at in isolation when making investment decisions). For instance, if we were to look at our plot in 2018, it would have been very reasonable to conjecture that Technology was in a bubble and poised for poor investment returns in the coming years. Yet, ’19 - ’21 were extremely strong years for that industry, and the bubble did not pop until 2022. Why is that & what are the main catalysts that cause the bubble to pop?\n\nWe will discuss this shortly…\n\nThe effects of bubbles in a particular segment tend to affect other segments as well, even if they appear unrelated, with varying degrees of magnitude.\n\nIn particular, we note that the Financials sector is often affected by bubbles in other segments. This is fairly intuitive, as the Financials sector implicitly interacts with all other sectors, and is inherently levered.\n\n\nLet’s create our Grand Financial Bubble Metric at a more granular level. We are only going to include the sectors that have sufficient data. For those with sufficient Price & Debt data, but insufficient IPO data, we will calculate our Grand Financial Bubble Metric as the average of Price Metric and Debt Metric and exclude the IPO Metric.\n\n\n\n\nCommunicationsIndustrialsConsumer DiscretionaryFinancialsConsumer StaplesHealth CareMaterialsEnergyReal EstateTechnology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‘Timing’ the Bubble\nThe ability to time market bubbles is nearly impossible, but the main indicators that many people pay attention to are interest rates/measures of borrowing cost. Since a large part of bubbles are caused by debt, it is logical that the cost of borrowing money is highly relevant.\nOne of the most observed interest rate metrics in the industry is the “10-2” Yield Spread which measures the difference in the yield of a 10-year Treasury Bond and that of a 2-year Treasury bond. Before we plot this measure below, I think it is important to understand the significance of this metric and its implications. Let’s consider two scenarios:\n\nA 10-year TBond is yielding 7% per year, and a 2-year TBond is yielding 3% per year, thus netting out to a 4% spread.\n\nThis scenario implies that the average person demands 4% more of a return on their investment to loan out their money for 10 years, rather than 2 years. At the most basic level, this is logical. The overarching rationale for this can be boiled down to General Uncertainty about the Future: the longer that maturity of the bond, the more time there is for unexpected events to occur that may affect the bond’s value, or the buyer’s perceived value of the bond, and therefore lenders demand higher rates of return for this increased uncertainty. Economists have outlined key risks attributable to this uncertainty:\n\nFuture Inflation Expectations/Inflation Risk: When investors expect higher inflation in the future, they demand higher yields to compensate for the loss of purchasing power. The longer the maturity of the bond, the more sensitive it is to the inflation expectations, which results in higher yields.\nCredit Risk: Generally, longer-term bonds have a higher credit risk than shorter-term bonds. This is because the longer the maturity of the bond, the more time there is for the borrower’s creditworthiness to change. If investors perceive that there is a higher risk that the issuer may default on the bond, they will demand a higher yield on longer-term bonds to compensate for this credit risk.\nLiquidity Risk: Bonds with varying maturities may have various degrees of ease in which they can be bought or sold without affecting its price. This can be due to a lack of market participants, sudden increases or decreases in supply and demand, etc.\n\n\nA 10-year TBond is yielding 3% per year, and a 2-year TBond is yielding 2% per year, thus netting out to a -1% spread. When this spread is negative, it is called a yield curve inversion.\n\nThis scenario implies that the average person is willing to give up a 1% return on their investment to loan out their money for an additional 8 years. In other words, lenders are more uncertain about the near-term than the long-term (for any of the aforementioned reasons)!\n\n\nIf market consensus indicates greater uncertainty regarding the short-term, as it does in the second scenario, this does not bode well for the economy and markets. Therefore, it is likely that the “10-2” Yield Spread is an insightful metric for our purposes.\nMany people, rightfully so, will push back and repeat the famous adage “yield-curve inversions predict 4 out of every 3 recessions,” and they are right; it is impossible to predict markets with the use of solely one metric, and there are times when the yield-curve has inverted and markets rallied. This is normal, there are absolutely times when the short-term is uncertain, but the underlying foundation of markets are structurally sound, and therefore the future still holds prospect for healthy growth. However, I believe studying yield curve inversions in conjunction with our Grand Financial Bubble Metric allows for powerful insights - if markets are uncertain about the near-term and markets are structurally unsound, or have been growing beyond potential, then it is likely that future growth prospects are weak.\nWith that said, let’s inspect interest rates and the “10-2” Yield spread:\n\n\n\n\n\nAs we can see from our plots, real equity returns tend to be negative following yield curve inversions. We can also see that the Fed, when possible, drops interest rates to stimulate the economy. However, we notice that it cannot afford to do so when a) inflation is out of control and/or b) when rates are already at 0%. These scenarios are structurally different and are therefore worth investigating in depth, but we will not discuss that here.\nLet’s overlay the time periods when the yield curve was inverted to our Grand Financial Bubble Metric:\n\n\n\n\n\nAs we can see, looking at yield curve inversions in conjunction with our Grand Financial Bubble Metric is somewhat helpful in informing the user on the formation of financial bubbles, sector equity performance, and possible asset allocation decisions. I would like to highlight that the above is a rough framework (that can be greatly improved) and that much more analysis should be conducted before making a possible investment decision. While our framework may aid in sector allocation decisions, a savvy investor will then pursue company-specific analysis that either supports or opposes their thesis."
  },
  {
    "objectID": "research/04_financial_bubbles/financial_bubbles.html#additional-areas-of-explanation-possible-amendments",
    "href": "research/04_financial_bubbles/financial_bubbles.html#additional-areas-of-explanation-possible-amendments",
    "title": "★Assessing Financial Bubbles",
    "section": "Additional Areas of Explanation & Possible Amendments",
    "text": "Additional Areas of Explanation & Possible Amendments\nThis article scratches the surface of possible ways to identify financial bubbles and can be drastically improved. A few points worth noting include:\n\nCalculating our Price Metric & Debt Metric with rolling averages (or medians) rather than using distance from the population median (similar to our IPO Metric)\n\nThe above is an examination of history, not an attempt at building a predictive model. Because of this, I used all available data to calculate the Price Metric & Debt Metric, which involved calculating distance from the median of all years. In the long-run, this method is acceptable for predictability purposes if we assume that these metrics are stationary and cyclical in nature (which implies the assumption that market dynamics are the same through time), and that we have enough data. However, this would not have been the case in 2000, where we would only have had 6 years of data to assess, and metrics cannot yet be considered cyclical.\nThe investigation of the rolling average (or median) growth rate of these metrics, rather than absolute level, is also worthwhile.\n\nIncreasing the frequency of the data is likely to be extremely worthwhile\n\nIn the above, our Price Metric & Debt Metric were calculated at yearly intervals (unlike our IPO Metric which could be calculated monthly) because I gathered yearly figures from the financial statements of the constituents of the Russell 3000 Index. However, much can change in the span of a year, and repeating the above at a quarterly frequency is likely to paint a clearer picture in our illustrations.\n\nThe logical, qualitative narrative we built around the formation of bubbles is extremely simplistic and can be improved. As such, there are many additional quantitative ‘metrics’ that could be created to measure and explain this narrative.\nFor simplicity (and lack of data), we inspected the “10-2” Yield Spread on TBonds as a catalyst for the ‘popping’ of a financial bubble, but it is likely beneficial to inspect corporate yield spreads at the sector level.\n\nAgain, it is also important to understand the first, second, and third order effects that monetary tightening may have on the key entities within each sector, which was not done here.\n\nImproving sector knowledge and building sector-specific models would be tremendously worthwhile, especially when used in conjunction with our broader Grand Financial Bubble Metric.\n\n\nAn Overarching Takeaway\nI think the above examination of history hints at an over-arching principle that applies to every facet of life - everything has an optimal growth rate. Whether it’s a single company, the global economy, or even an individual’s ability to learn a topic - there is an optimal, healthy rate of progress that is predicated upon the underlying nature and quality of the system.\nIn the case of a single company, that rate of progress is a function of the quality of the workers, their abilities, location, tools, team chemistry, and so on. In the short-run, it is possible to grow beyond optimal; perhaps a company mandates overtime work, which will boost growth in the near-term, but if continued, this will adversely affect long-term growth, as employees will find suitable work elsewhere. In the case of an individual’s learning ability, optimal growth is a function of sleep quality, nutritional intake, time spent studying, and numerous other factors. Again, in the short-run, progress can be artificially inflated; perhaps the individual sacrifices sleep to study, which will boost learning-ability in the short-run, but this rate of learning is obviously unsustainable.\nWith that said, an entity’s optimal growth rate can obviously change with time, as the nature and the quality of the underlying system change - and it is an analyst’s job to identify when and why this occurs.\nAgain, it is my perspective that this idea of an ‘optimal growth rate’ is applicable to any subject, including investing. If we can understand an investment’s optimal growth rate, its current deviation from this growth rate, and the reasons for this deviation - then we have the ability to capitalize on these deviations and inefficiencies with the correct strategies and tactics.\n\nFinal Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html",
    "title": "★EPS Revision Breadth Analysis",
    "section": "",
    "text": "EPS Revision Breadth is a financial metric that measures the number of upward (positive) and downward (negative) revisions to earnings per share (EPS) estimates by analysts. It is calculated as the difference between the number of positive and negative revisions divided by the total number of revisions made.\nSo, if we consider 30 analysts, 20 of whom revised their earnings estimates upwards, and 10 of whom revised their earnings downwards, this would yield an EPS Revision Breadth of 33%.\nThis metric is important because it provides insight into the market’s confidence in a company’s earnings. A high breadth of positive revisions to EPS estimates indicates that analysts are becoming more optimistic about the company’s future earnings, which can be a sign of increased investor confidence and a potential increase in the company’s stock price. Conversely, a high breadth of negative revisions to EPS estimates can indicate declining investor confidence and a potential decrease in the company’s stock price.\nAs a result, EPS Revision Breadth has the potential to be a useful indicator for investors to assess the market’s sentiment towards a company’s earnings and make informed investment decisions. In the following, we will investigate EPS Revision Breadth for the SP500 as a whole, rather than for specific companies."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#thesis",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#thesis",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Thesis",
    "text": "Thesis\nIdentifying tops and bottoms in EPS Revision Breadth can aid in distinguishing periods of strong vs. weak equity returns."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#data-overview",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#data-overview",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Data Overview",
    "text": "Data Overview\n\n\nCode\ndata_raw %>% \n    head() %>% \n    set_names(c(\"date\", \"EPS Revision Breadth\", \"SP500 Price\")) %>% \n    gt(rowname_col = \"date\") %>% \n    fmt_percent(columns = 2, decimals = 1) %>% \n    fmt_currency(columns = 3, decimals = 0)\n\n\n\n\n\n\n  \n  \n    \n      \n      EPS Revision Breadth\n      SP500 Price\n    \n  \n  \n    2023-01-30\n−15.2%\n$4,018\n    2023-01-27\n−15.7%\n$4,071\n    2023-01-26\n−15.4%\n$4,060\n    2023-01-25\n−15.1%\n$4,016\n    2023-01-24\n−13.8%\n$4,017\n    2023-01-23\n−13.7%\n$4,020"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#eps-revisions-identifying-tops-bottoms",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#eps-revisions-identifying-tops-bottoms",
    "title": "★EPS Revision Breadth Analysis",
    "section": "EPS Revisions: Identifying Tops & Bottoms",
    "text": "EPS Revisions: Identifying Tops & Bottoms\nSince this data is cyclical and stationary, we can attempt to identify tops and bottoms, most simply, by partitioning the data into buckets based on its EPS Revision Breadth Value. The following chart demonstrates this by highlighting the top 10% of EPS RB in orange, and the bottom 10% in blue:\n\n\nCode\ndata_raw_bucketed <- data_raw %>% \n    mutate(quantile = ntile(eps_revision_breadth, n = 10)) %>% \n    arrange(date)\n\nmajor_troughs_tbl <- tibble(\n    start_date = c(ymd(\"2008-09-01\"), ymd(\"2020-03-01\")),\n    end_date   = c(ymd(\"2009-06-01\"), ymd(\"2020-07-10\"))\n)\n\n\nggplot() +\n    geom_line(\n        data = data_raw_bucketed,\n        mapping = aes(date, eps_revision_breadth)\n    ) +\n    geom_point(\n        data = data_raw_bucketed %>% \n            filter(quantile == 10),\n        mapping = aes(date, eps_revision_breadth),\n        color = \"darkorange\"\n    ) +\n    geom_point(\n        data = data_raw_bucketed %>% \n            filter(quantile == 1),\n        mapping = aes(date, eps_revision_breadth),\n        color = \"midnightblue\"\n    ) +\n    geom_rect(\n        data = major_troughs_tbl,\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"darkred\",\n        alpha = .3\n    ) +\n    labs(\n        x = \"\",\n        y = \"\",\n        title = \"EPS Revision Breadth (%)\"\n    ) +\n    theme_tq() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size = 16))\n\n\n\n\n\nMajor Troughs are shaded in red\n\n\n\n\nAs we can see from the plot, highlighting the top and bottom 10% of values adequately identifies the peaks and troughs in the time series. However we clearly need to address the fact that the troughs similar to those identified in 2015 are systematically different from those that occur in 2008 and 2020 (shaded in red). We will therefore categorize troughs into two groups, which should be treated and analyzed differently:\n\nMinor Troughs that arise as a natural artifact of the business cycle\nMajor Troughs that arise due to periods of extreme economic turbulence, typically after the popping of a financial bubble\n\nThe ability to distinguish between these two types of troughs is essential because it is unlikely that returns before, during, and after Major Troughs are similar to those for Minor troughs.\nWhile we won’t dive into predicting when these Major Troughs occur here, I believe that tracking sector-level debt, the velocity & acceleration of that debt, and the speed, ability, and ease in which major players in those sectors can service their debt, is a key ingredient in modelling this out (see Bridgewater Research).\nAs such a basic decision tree could look as follows:\n\n\nCode\nmermaid(\"\ngraph TD\n\nA(EPS RB < 10th Percentile?)-- No --> B[Not a Trough]\nB --> Z(Act accordingly...)\nA -- Yes --> C(Does separate model that includes Debt levels etc. indicate a Major Trough?)\n\nC -- No --> D[Minor Trough]\nC -- Yes --> E[Major Trough]\n\nD --> F(Act accordingly...)\nE --> G(Act accordingly...)\n\")"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#return-analysis",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#return-analysis",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Return Analysis",
    "text": "Return Analysis\nLets identify average 1M, 3M, 6M, and 1YR returns, had you invested at points during a peak or trough in EPS RB:\n\n\nCode\nreturn_summary_tbl_incl_major <- data_raw_bucketed %>% \n    mutate(\n        month_1_ret = (sp500_price / lag(sp500_price, n=25) - 1),\n        month_3_ret = (sp500_price / lag(sp500_price, n=25*3) - 1),\n        month_6_ret = (sp500_price / lag(sp500_price, n=25*6) - 1),\n        month_12_ret = (sp500_price / lag(sp500_price, n=25*12) - 1)\n    ) %>% \n    group_by(quantile) %>% \n    summarize(across(.cols = contains(\"month\"), .fns = ~mean(.x, na.rm = T)))\n\nreturn_summary_tbl_excl_major <- data_raw_bucketed %>% \n    mutate(\n        month_1_ret = (sp500_price / lag(sp500_price, n=25) - 1),\n        month_3_ret = (sp500_price / lag(sp500_price, n=25*3) - 1),\n        month_6_ret = (sp500_price / lag(sp500_price, n=25*6) - 1),\n        month_12_ret = (sp500_price / lag(sp500_price, n=25*12) - 1)\n    ) %>% \n    group_by(quantile) %>% \n    # filter out major trough time periods\n    filter(!(date %>% between(ymd(\"2008-09-01\"), ymd(\"2009-06-01\")))) %>%\n    filter(!(date %>% between(ymd(\"2020-03-01\"), ymd(\"2020-07-10\")))) %>% \n    summarize(across(.cols = contains(\"month\"), .fns = ~mean(.x, na.rm = T)))\n\n\n\nVisualTable\n\n\n\n\nCode\nreturn_summary_tbl_excl_major %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n    pivot_longer(cols = -Quantile, names_to = \"Time Horizon\", values_to = \"Return_excl\") %>% \n    bind_cols(return_summary_tbl_incl_major %>% \n                   set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n                   pivot_longer(cols = -Quantile, names_to = \"Time Horizon\", values_to = \"Return_incl\") %>% \n                   select(Return_incl)) %>% \n    pivot_longer(cols = contains(\"Return\")) %>%\n    mutate(Quantile = as.factor(Quantile),\n           `Time Horizon` = as_factor(`Time Horizon`)) %>% \n    ggplot(aes(Quantile, value)) +\n    geom_col(\n        data = . %>% \n            filter(name == \"Return_excl\"),\n        fill = \"midnightblue\",\n        color = \"black\",\n        alpha = .7\n    ) +\n    geom_point(\n        data = . %>% \n            filter(name == \"Return_incl\"),\n        size = 3,\n        color = \"darkorange\"\n    ) +\n    facet_wrap(~`Time Horizon`, ncol = 1, scales = \"free_y\") +\n    labs(y =\"\", title = \"Returns by Quantile\", subtitle = str_glue(\"Bar = Excluding Major Troughs\n                                                                   Point = Including Major Troughs\")) +\n    theme_tq() +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\nExcluding Major TroughsIncluding Major Troughs\n\n\n\n\nCode\nreturn_summary_tbl_excl_major %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n    gt() %>% \n    fmt_percent(columns = -Quantile, decimals = 1) %>% \n    tab_header(title = \"SP500 Returns by Quantile of EPS Revision Breadth\",\n               subtitle = \"Excluding Major Troughs\")\n\n\n\n\n\n\n  \n    \n      SP500 Returns by Quantile of EPS Revision Breadth\n    \n    \n      Excluding Major Troughs\n    \n  \n  \n    \n      Quantile\n      Month 1 Ret\n      Month 3 Ret\n      Month 6 Ret\n      Month 12 Ret\n    \n  \n  \n    1\n1.4%\n−2.3%\n−4.0%\n−0.7%\n    2\n1.1%\n−0.3%\n0.8%\n5.8%\n    3\n1.1%\n2.4%\n2.4%\n9.5%\n    4\n0.4%\n2.7%\n5.0%\n10.7%\n    5\n0.8%\n4.9%\n6.8%\n13.5%\n    6\n0.6%\n4.9%\n5.7%\n12.7%\n    7\n1.6%\n3.2%\n7.7%\n13.6%\n    8\n2.0%\n4.6%\n10.0%\n15.7%\n    9\n1.0%\n5.2%\n12.3%\n15.1%\n    10\n1.2%\n6.9%\n15.6%\n27.2%\n  \n  \n  \n\n\n\n\n\n\n\n\nCode\nreturn_summary_tbl_incl_major %>% \n    set_names(names(.) %>% str_replace_all(., \"_\", \" \") %>% str_to_title()) %>% \n    gt() %>% \n    fmt_percent(columns = -Quantile, decimals = 1) %>% \n    tab_header(title = \"SP500 Returns by Quantile of EPS Revision Breadth\",\n               subtitle = \"Including Major Troughs\")\n\n\n\n\n\n\n  \n    \n      SP500 Returns by Quantile of EPS Revision Breadth\n    \n    \n      Including Major Troughs\n    \n  \n  \n    \n      Quantile\n      Month 1 Ret\n      Month 3 Ret\n      Month 6 Ret\n      Month 12 Ret\n    \n  \n  \n    1\n−0.8%\n−9.0%\n−13.3%\n−12.3%\n    2\n1.0%\n−0.4%\n0.2%\n4.7%\n    3\n1.2%\n2.8%\n2.1%\n8.7%\n    4\n0.5%\n2.8%\n4.9%\n10.1%\n    5\n0.8%\n4.9%\n6.8%\n13.5%\n    6\n0.6%\n4.9%\n5.7%\n12.7%\n    7\n1.6%\n3.2%\n7.7%\n13.6%\n    8\n2.0%\n4.6%\n10.0%\n15.7%\n    9\n1.0%\n5.2%\n12.3%\n15.1%\n    10\n1.2%\n6.9%\n15.6%\n27.2%\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\nMain Takeaway\n\n\nCode\nqtile_1_epsrb_value <- data_raw_bucketed %>% \n    filter(quantile == 1) %>% \n    filter(eps_revision_breadth == max(eps_revision_breadth)) %>% \n    pull(eps_revision_breadth)\n\nqtile_10_epsrb_value <- data_raw_bucketed %>% \n    filter(quantile == 10) %>% \n    filter(eps_revision_breadth == min(eps_revision_breadth)) %>% \n    pull(eps_revision_breadth)\n\n\nEvidently, during times of lower EPS RB, returns are much lower than those during times when EPS RB is higher (for Time Horizons greater than 1 month). Therefore identifying where we are (and where we will be heading) in the EPS RB cycle is crucial.\nFrom what we have seen thus far, we can establish these basic rules of thumb:\n\nWhen EPS Revision Breadth is below its 10th Percentile (which translates to an EPS RB of approximately -16%), we should consider underweighting Equities\nWhen EPS Revision Breadth is above its 90th Percentile (which translates to an EPS RB of approximately 24%), we should consider overweighting Equities\nWhen EPS Revision Breadth is between these two values, weight Equities according to personal judgement"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#building-an-active-portfolio",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#building-an-active-portfolio",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Building an Active Portfolio",
    "text": "Building an Active Portfolio\nLet’s utilize our basic heuristics to build an active, long-only portfolio, consisting solely of Equities and Fixed Income, with asset class weights that vary according to EPS Revision Breadth. In other words, the portfolio’s allocation to Equities and Fixed Income is a function of EPS Revision Breadth at certain point in time. Therefore, we need to define a function that under-weights Equities during periods of very low EPS Revision Breadth, and does the converse for periods with moderate to high EPS Revision Breadth.\nA sample function could look as follows:\n\n\nCode\ndata_raw_bucketed %>% \n    group_by(quantile) %>% \n    summarize(epsrb_upper_bound = max(eps_revision_breadth)) %>% \n    add_row(quantile = c(0, 11), epsrb_upper_bound = c(-1, 1)) %>% \n    arrange(quantile) %>% \n    add_column(`Equity Allocation` = c(0, 0, 0.05, 0.45, 0.55, 0.70, 0.825, 0.9, 0.925, 0.95, 1, 1)) %>% \n    mutate(`Fixed Income Allocation` = 1-`Equity Allocation`) %>% \n    pivot_longer(contains(\"Allocation\")) %>% \n    ggplot(aes(epsrb_upper_bound, value, color = name)) +\n    geom_point() +\n    geom_line() +\n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    labs(\n        x = \"EPS Revision Breadth\",\n        y = \"Portfolio Weight\",\n        color = \"\"\n    ) +\n    theme_tq() +\n    scale_color_tq() +\n    theme(legend.position = \"top\", text = element_text(size = 16))\n\n\n\n\n\nAs we can see, our function should be bounded between 0 and 1 on the y-axis, and have a similar shape to our ‘prototype’ above. These are both characteristics of the following function:\n\\[\nf(x) = \\frac{1-c_3}{1 + e^{-c_1(x - c_2)}} + c_3\n\\]\nIf we test out a few constants, we see that the values of C1 = 20, C2 = -7.5%, and C3 = 0, generates a function similar to our sample:\n\n\nCode\nsigmoid <- function(x, c1, c2, c3){\n    y <- ((1-c3) / (1 + exp(-c1 * (x - c2)))) + c3\n    \n    return(y)\n}\n\ntibble(\n    eps_rb = seq(-1, 1 ,by=.01),\n    `Equity Allocation` = sigmoid(eps_rb, c1 = 20, c2 = -.075, c3 = 0),\n    `Fixed Income Allocation` = 1-`Equity Allocation`\n) %>% \n    pivot_longer(contains(\"Allocation\")) %>% \n    ggplot(aes(eps_rb, value, color = name)) +\n    geom_line() +\n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_tq() +\n    scale_color_tq() +\n    labs(\n        x = \"EPS Revision Breadth\", y = \"Portfolio Weight\", title = \"Allocation Function\", subtitle = \"C1 = 20, C2 = -7.5%, C3 = 0\", color = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 16))\n\n\n\n\n\n\nPortfolio Comparison\nLet’s apply our weighting scheme above and compare its performance to a traditional, static 60/40 Portfolio, an SP500-only Portfolio, and a Fixed-Income-only portfolio. We will consider the iShares Core U.S. Aggregate Bond ETF (“The AGG”) as a proxy for Fixed Income.\n\n\nCode\nprice_data <- tq_get(c(\"SPY\", \"AGG\"), from = \"2003-09-30\") %>% \n    select(symbol, date, adjusted)\n\nstart_tbl <- price_data %>% \n    group_by(symbol) %>% \n    mutate(pct_ret = (adjusted / lag(adjusted)) - 1) %>% \n    ungroup() %>% \n    select(-adjusted) %>% \n    pivot_wider(names_from = symbol, values_from = pct_ret) %>% \n    left_join(data_raw %>% \n                  select(-sp500_price))\n\nreturns_and_index <- start_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075, c3 = 0),\n           port_fi_weight = 1 - port_equity_weight) %>%\n    \n    drop_na() %>% \n    mutate(portfolio_return = (port_equity_weight * SPY) + (port_fi_weight * AGG),\n           portfolio_60_40_return = (.6 * SPY) + (.4 * AGG)) %>% \n    pivot_longer(cols = c(SPY, AGG, contains(\"portfolio\"))) %>% \n    group_by(name) %>% \n    mutate(price_index = cumprod(1+value)) %>%\n    ungroup() %>% \n    mutate(name = case_when(\n        name == \"SPY\" ~ \"SP500\",\n        name == \"portfolio_60_40_return\" ~ \"60/40 Portfolio\",\n        name == \"portfolio_return\" ~ \"Active Portfolio\",\n        name == \"AGG\" ~ \"AGG\"\n    ))\n\nreturns_and_index %>% \n    ggplot(aes(date, price_index, color = name)) +\n    geom_line() +\n    theme_tq() +\n    scale_color_tq() +\n    theme(legend.position = \"top\", text = element_text(size = 16)) +\n    labs(x = \"\", y = \"Wealth Index ($1)\", color = \"\", title = \"Portfolio Comparison\") +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nAs we can see, our portfolio outperforms the traditional 60/40 Portfolio, but under-performs the SP500, on a nominal basis.\nHowever, when comparing the returns of these 4 portfolios on a risk-adjusted basis, our Active Portfolio fares very well, which means that our portfolio yields the most return per unit of risk.\nFor clarification, we consider investment returns as reward, and the standard deviation of returns (volatility) as risk. The Sharpe Ratio divides the average return by the standard deviation of these returns, while the Sortino Ratio divides the average return by the standard deviation of the negative returns only.\n\n\nCode\nmonthly_returns <- returns_and_index %>% \n    select(date, name, value) %>% \n    mutate(year = year(date),\n           month = month(date)) %>% \n    group_by(month, year, name) %>% \n    mutate(month_return = cumprod(1+value) - 1) %>% \n    slice_tail(n=1) %>% \n    ungroup() %>% \n    group_by(name)\n\nmonthly_returns %>% \n    summarize(\n        mean_monthly_return = mean(month_return),\n        st_dev_monthly_return = sd(month_return)\n    ) %>% \n    left_join(\n       monthly_returns %>% \n           filter(month_return < 0) %>% \n           group_by(name) %>% \n           summarize(downside_st_dev_monthly_return = sd(month_return))\n    ) %>% \n    mutate(sharpe_ratio = mean_monthly_return / st_dev_monthly_return,\n           sortino_ratio = mean_monthly_return / downside_st_dev_monthly_return) %>% \n    arrange(desc(sharpe_ratio)) %>% \n    set_names(names(.) %>% str_remove_all(\"monthly_return\") %>% str_replace_all(\"st_dev\", \"standard deviation\") %>%  str_replace_all(\"_\", \" \") %>% str_to_title()) %>% \n    gt(rowname_col = \"Name\") %>% \n    fmt_percent(columns = 2, decimals = 2) %>% \n    fmt_percent(columns = 3:last_col(), decimals = 1) %>% \n    tab_style(\n        style = cell_text(weight = \"bold\"),\n        locations = cells_body(columns = contains(\"Ratio\"))\n    ) %>% \n    gt::tab_header(title = \"Return Summary\", subtitle = \"Monthly Periodicity\") %>% \n    gt::tab_footnote(locations = cells_column_labels(columns = contains(\"Ratio\")), footnote = \"Calculation does not include the risk-free rate\")\n\n\n\n\n\n\n  \n    \n      Return Summary\n    \n    \n      Monthly Periodicity\n    \n  \n  \n    \n      \n      Mean \n      Standard Deviation \n      Downside Standard Deviation \n      Sharpe Ratio1\n      Sortino Ratio1\n    \n  \n  \n    Active Portfolio\n0.74%\n3.0%\n2.0%\n25.1%\n36.9%\n    60/40 Portfolio\n0.63%\n2.7%\n2.1%\n23.6%\n30.0%\n    AGG\n0.25%\n1.2%\n0.8%\n21.2%\n30.4%\n    SP500\n0.85%\n4.3%\n3.3%\n19.9%\n26.2%\n  \n  \n  \n    \n      1 Calculation does not include the risk-free rate\n    \n  \n\n\n\n\nThis occurs because our Active Portfolio is able to identify time periods when it is preferable to hold fixed income rather than equities, and vice versa. As a result, our portfolio has an average allocation of 75/25!\n\n\nCode\nstart_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075, c3 = 0),\n           port_fi_weight = 1 - port_equity_weight) %>%\n    summarize(across(.cols = contains(\"port\"), .fns = ~mean(.x, na.rm = T))) %>% \n    set_names(c(\"Average Equity Allocation\", \"Average Fixed Income Allocation\")) %>% \n    gt() %>% \n    fmt_percent(columns = everything())\n\n\n\n\n\n\n  \n  \n    \n      Average Equity Allocation\n      Average Fixed Income Allocation\n    \n  \n  \n    74.17%\n25.83%\n  \n  \n  \n\n\n\n\nWe can plot our portfolio’s allocation to equities over time:\n\n\nCode\nggplot() +\n    geom_line(\n        data = start_tbl %>% \n                    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075, c3=0),\n                    port_fi_weight = 1 - port_equity_weight),\n        mapping = aes(date, port_equity_weight)\n    ) +\n    geom_rect(\n        data = major_troughs_tbl %>% \n            slice(2),\n        mapping = aes(xmin = start_date, xmax = end_date, ymin = -Inf, ymax = Inf),\n        fill = \"red\",\n        alpha = .3\n    ) +\n    theme_tq() +\n    scale_color_tq() +\n    scale_y_continuous(labels = scales::label_percent()) +\n    labs(x = \"\", y = \"Allocation to Equities\") +\n    theme(text = element_text(size = 16))\n\n\n\n\n\nThe red shaded region indicates the start of the Covid-19 Pandemic\n\n\n\n\nAs we can see, our portfolio underweights equities during times when EPS Revision Breadth is low, and overweights equities when EPS Revision breadth is high. However, we notice that it will therefore underweight equities in 2020, when the Covid-19 pandemic started.\nWhile the start of the Covid-19 pandemic was momentous, markets severely overreacted, and many savvy investors capitalized on these overreactions. Unlike in 2001 and 2008, when market expectations plummeted due to the burst of a financial bubble, in 2022 market expectations plummeted due to frenzied speculation. Because of this, it is unlikely that a rational investor would have sold all of his equities; so, lets course correct our portfolio with human judgement.\n\n\nCourse Correcting w/ Human Judgement\nLet’s assume that we are savvy investors, and that in March of 2022 we increased our allocation to equities to 75% because we understood that the markets were acting irrationally. If we do this while keeping everything else the same, here is how our active portfolio would have performed:\n\n\nCode\nreturns_and_index_mod <- start_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075, c3 = 0)) %>% \n    mutate(port_equity_weight = ifelse(\n        date %>% between(major_troughs_tbl$start_date[2], major_troughs_tbl$end_date[2]),\n        .75, port_equity_weight)) %>% \n    mutate(port_fi_weight = 1 - port_equity_weight) %>% \n    drop_na() %>% \n    mutate(portfolio_return = (port_equity_weight * SPY) + (port_fi_weight * AGG),\n           portfolio_60_40_return = (.6 * SPY) + (.4 * AGG)) %>% \n    pivot_longer(cols = c(SPY, AGG, contains(\"portfolio\"))) %>% \n    group_by(name) %>% \n    mutate(price_index = cumprod(1+value)) %>%\n    ungroup() %>% \n    mutate(name = case_when(\n        name == \"SPY\" ~ \"SP500\",\n        name == \"portfolio_60_40_return\" ~ \"60/40 Portfolio\",\n        name == \"portfolio_return\" ~ \"Active Portfolio\",\n        name == \"AGG\" ~ \"AGG\"\n    ))\n\nreturns_and_index_mod %>% \n    ggplot(aes(date, price_index, color = name)) +\n    geom_line() +\n    theme_tq() +\n    scale_color_tq() +\n    theme(legend.position = \"top\", text = element_text(size = 16)) +\n    labs(x = \"\", y = \"Wealth Index ($1)\", color = \"\", title = \"Portfolio Comparison\") +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nNow, we notice that our active portfolio barely under-performs the market while still mitigating risk:\n\nReturn SummaryAverage AllocationEquity Allocation over Time\n\n\n\n\nCode\nmonthly_returns_mod <- returns_and_index_mod %>% \n    select(date, name, value) %>% \n    mutate(year = year(date),\n           month = month(date)) %>% \n    group_by(month, year, name) %>% \n    mutate(month_return = cumprod(1+value) - 1) %>% \n    slice_tail(n=1) %>% \n    ungroup() %>% \n    group_by(name)\n\nmonthly_returns_mod %>% \n    summarize(\n        mean_monthly_return = mean(month_return),\n        st_dev_monthly_return = sd(month_return)\n    ) %>% \n    left_join(\n       monthly_returns %>% \n           filter(month_return < 0) %>% \n           group_by(name) %>% \n           summarize(downside_st_dev_monthly_return = sd(month_return))\n    ) %>% \n    mutate(sharpe_ratio = mean_monthly_return / st_dev_monthly_return,\n           sortino_ratio = mean_monthly_return / downside_st_dev_monthly_return) %>% \n    arrange(desc(sharpe_ratio)) %>% \n    set_names(names(.) %>% str_remove_all(\"monthly_return\") %>% str_replace_all(\"st_dev\", \"standard deviation\") %>%  str_replace_all(\"_\", \" \") %>% str_to_title()) %>% \n    gt(rowname_col = \"Name\") %>% \n    fmt_percent(columns = 2, decimals = 2) %>% \n    fmt_percent(columns = 3:last_col(), decimals = 1) %>% \n    tab_style(\n        style = cell_text(weight = \"bold\"),\n        locations = cells_body(columns = contains(\"Ratio\"))\n    ) %>% \n    gt::tab_header(title = \"Return Summary\", subtitle = \"Monthly Periodicity\") %>% \n    gt::tab_footnote(locations = cells_column_labels(columns = contains(\"Ratio\")), footnote = \"Calculation does not include the risk-free rate\")\n\n\n\n\n\n\n  \n    \n      Return Summary\n    \n    \n      Monthly Periodicity\n    \n  \n  \n    \n      \n      Mean \n      Standard Deviation \n      Downside Standard Deviation \n      Sharpe Ratio1\n      Sortino Ratio1\n    \n  \n  \n    Active Portfolio\n0.77%\n3.1%\n2.0%\n25.0%\n38.3%\n    60/40 Portfolio\n0.63%\n2.7%\n2.1%\n23.6%\n30.0%\n    AGG\n0.25%\n1.2%\n0.8%\n21.2%\n30.4%\n    SP500\n0.85%\n4.3%\n3.3%\n19.9%\n26.2%\n  \n  \n  \n    \n      1 Calculation does not include the risk-free rate\n    \n  \n\n\n\n\n\n\n\n\nCode\nstart_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075, c3 = 0)) %>% \n    mutate(port_equity_weight = ifelse(\n        date %>% between(major_troughs_tbl$start_date[2], major_troughs_tbl$end_date[2]),\n        .75, port_equity_weight)) %>% \n    mutate(port_fi_weight = 1 - port_equity_weight) %>% \n    summarize(across(.cols = contains(\"port\"), .fns = ~mean(.x, na.rm = T))) %>% \n    set_names(c(\"Average Equity Allocation\", \"Average Fixed Income Allocation\")) %>% \n    gt() %>% \n    fmt_percent(columns = everything())\n\n\n\n\n\n\n  \n  \n    \n      Average Equity Allocation\n      Average Fixed Income Allocation\n    \n  \n  \n    75.44%\n24.56%\n  \n  \n  \n\n\n\n\n\n\n\n\nCode\nggplot() +\n    geom_line(\n        data = start_tbl %>% \n    mutate(port_equity_weight = sigmoid(eps_revision_breadth, c1 = 20, c2 = -.075, c3 = 0)) %>% \n    mutate(port_equity_weight = ifelse(\n        date %>% between(major_troughs_tbl$start_date[2], major_troughs_tbl$end_date[2]),\n        .75, port_equity_weight)),\n        mapping = aes(date, port_equity_weight)\n    ) +\n    theme_tq() +\n    scale_color_tq() +\n    scale_y_continuous(labels = scales::label_percent()) +\n    labs(x = \"\", y = \"Allocation to Equities\") +\n    theme(text = element_text(size = 16))\n\n\n\n\n\n\n\n\n\n\nModifying C1 & C2\nDuring the construction of our portfolio, we arbitrarily used C1 = 20 and C2 = -.075 as inputs for our allocation function because they replicated our rough, ‘prototype’ weighting scheme. In addition, these inputs seemed sensible for the average investor as they yielded, on average, a 75/25 weighting scheme. However, we can adjust these parameters depending on the risk appetite of the investor.\nThe following demonstrates graphically what occurs as you change C1:\n\nAs we can see, C1 represents our sensitivity to changes in EPS Revision Breadth, C2 represents the EPS Revision Breadth value that corresponds to an equity allocation of 50% (if and only if C3 = 0), and C3 represents a minimum equity allocation. Therefore, we can elect to modify our weighting scheme for different investment styles. The following could represent a sample weighting scheme for an investor that would like to be less sensitive to changes in EPS Revision Breadth with a minimum equity allocation of 40%:\n\n\nCode\ntibble(\n    eps_rb = seq(-1, 1 ,by=.01),\n    `Equity Allocation` = sigmoid(eps_rb, c1 = 10, c2 = -.075, c3 = .4),\n    `Fixed Income Allocation` = 1-`Equity Allocation`\n) %>% \n    pivot_longer(contains(\"Allocation\")) %>% \n    ggplot(aes(eps_rb, value, color = name)) +\n    geom_line() +\n    scale_x_continuous(labels = scales::percent_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_tq() +\n    scale_color_tq() +\n    labs(\n        x = \"EPS Revision Breadth\", y = \"Portfolio Weight\", title = \"Allocation Function\", subtitle = \"C1 = 10, C2 = -7.5%%, C3 = 40%\", color = \"\"\n    ) +\n    theme(legend.position = \"top\", text = element_text(size = 16))"
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#key-takeaways",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#key-takeaways",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nAs we can surmise from the analysis above, EPS Revision Breadth is a useful financial metric that reflects general market sentiment, which can help an investor position their portfolio accordingly."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#additional-areas-of-exploration",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#additional-areas-of-exploration",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Additional Areas of Exploration",
    "text": "Additional Areas of Exploration\nWhile I will not do so here, the above can be extended and replicated for each sector within the SP500 - allowing for an active portfolio that dynamically overweights and underweights specific sectors over time according to each sectors EPS Revision Breadth. It would also be wise to include several other variables, attempt to build a major tough vs. minor trough classifier, and incorporate other asset classes in the portfolio for diversification benefits."
  },
  {
    "objectID": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#final-remarks",
    "href": "research/03_eps_revision_breadth/eps_revision_breadth_analysis.html#final-remarks",
    "title": "★EPS Revision Breadth Analysis",
    "section": "Final Remarks",
    "text": "Final Remarks\nThe above is intended as an exploration of historical data, and all statements and opinions are expressly my own; neither should be construed as investment advice."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n★ Post-Earnings Announcement Drift (PEAD) Anomaly & Return Cyclicality\n\n\n\nEquities\n\n\n★\n\n\n\nRead Time: 15 mins\n\n\n\nMax Sands\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n★Assessing Financial Bubbles\n\n\n\nEquities\n\n\n★\n\n\n\nRead Time: 15-30 mins\n\n\n\nMax Sands\n\n\nMar 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n★EPS Revision Breadth Analysis\n\n\n\nEquities\n\n\n★\n\n\n\nRead Time: 10-15 mins\n\n\n\nMax Sands\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeather & the Stock Market\n\n\n\nEquities\n\n\n\nRead Time: 5-10 mins\n\n\n\nMax Sands\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManagement Trading\n\n\n\nEquities\n\n\n\nRead Time: 5-10 mins\n\n\n\nMax Sands\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n★High Yield Debt vs. Equities\n\n\n\nMacro\n\n\nHigh Yield Debt\n\n\n★\n\n\n\nRead Time: 15-25 mins\n\n\n\nMax Sands\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "",
    "section": "",
    "text": "Download My Resume - PDF Download My Resume - DOCX\n\nEducationWork ExperiencePersonal ProjectsSkills & Interests\n\n\n\nBusiness Science University\n2022 - Present\nCertified:\n\nData Science for Business w/ R\nML & High Performance Time Series Forecasting w/ R\nPredictive Shiny Web Applications w/ R\nShiny Development w/ R & AWS (in progress)\n\n\n\nMcGill University\n2019 - 2022\nDesautels Faculty of Management, Montréal, Canada\nBachelor of Commerce, Major in Finance, Double Concentration in Accounting & Statistics\n\n\n\n\nChoate Hall & Stewart / Choate Investment Advisors\nBusiness & Investment Analyst | 2022 - Present | Boston, MA\n\nProviding general investment research for the Equity and Fixed Income Teams\n\nBuilt automated preliminary Investment Analysis report using R and Quarto\n\nBy providing a ticker and a list of comps, a report is generated that gathers data from Bloomberg and performs key metric ratio analysis, market share analysis, business segment analysis, etc. to provide a preliminary valuation\n\n\nDeveloping unique solutions and models for the Investment and Operations divisions:\n\nBuilt in-house Monte Carlo Simulation tool for client Retirement Analysis\n\nIncludes sensitivity analysis across several key inputs\nIncludes report generation - both static PDFs and interactive HTML\n\nBuilt automatic Drift Analysis report using R and Quarto that provides insights on accounts that are drifting off model\nCleaned and formatted data for upload to Addepar\n\n\n\n\n\nMcGill University\nTeaching Assistant | 2021 - 2022 | Montréal, Canada\n\nDelivering lectures, mentoring/tutoring students, and grading all assessments - typically reserved for Graduate students\n\n\n\n\nIronHold Capital\nInvestment Analyst & Executive Assistant | 2020 - 2021 | New York, United States\n\nAided in raising capital, marketing efforts, reviewing legal contracts, and meeting with Family Offices & HNIs on behalf of the CEO\nWorked directly under the CIO and conducted research on Indian and U.S. Equities - the fund followed a Value-based strategy\nAuthor of the fund’s newsletter that covered key macroeconomic events, political news, and Family Office industry insights\n\n\n\n\nPrivate Tutor\n2018 - 2021\n\nTutored students in Mathematics, Statistics, Physics, English, and Finance, and revised college admission essays\n\n\n\n\n\nReplicating Ray Dalio’s Work: Forecasting the ‘Big’ & ‘Debt’ Cycles\n2022 - Present\n\nBuilding a system that continuously retrieves data from various sources and uses Machine Learning to predict several countries’ current and future stages of their ‘Big’ and ‘Debt’ Cycles, with the eventual goal of predicting asset returns across classes, sectors, and geographies\n\n\n\n\nInsider Trading - Scraping SEC Form 4 Filings\n2022 - Present\n\nWrote code in R that scrapes the SEC Edgar site and all of its Form 4 filings. I am currently developing a trading strategy that incorporates this data.\n\n\n\n\nCoding w/ Bloomberg: Automating Equity Screens & Rough Draft DCFs\n2022\n\nWrote code in R that connects to and scrapes information from a Bloomberg Terminal by utilizing its Desktop API\n\nAutomated Bloomberg’s EQS function so that I could run continuous equity screens (i.e. every ‘Saturday’ since MM/DD/YY)\nUsed scraped information to automate the generation of a ‘rough draft’ DCF for any company; companies with a significant discrepancy between ‘rough draft’ implied share price and 7-day VWAP would garner additional, refined analysis\n\n\n\n\n\nCoding w/ Amazon Web Services (AWS), Gemini, & Kraken: Automating Cryptocurrency Trading Strategy\n2022\n\nWrote code in Python that connects to Gemini (cryptocurrency exchange) web API and allows me to place limit orders if certain conditions are met; automated trading by running the code externally every 5 minutes through AWS Cloudwatch & AWS Lambda\n\nRepeated the above with Kraken (cryptocurrency exchange) web API to place market orders\n\n\n\n\n\nLanguage & Computer Skills: Fluent French & English, Moderate R & Basic Python\nInterests: Financial Analytics/Data Science, Weightlifting, Guitar, Golf, Chess, Poker, Football, Basketball, Math, Creative Writing, Rock Climbing\nInfluential Role Models: Ray Dalio, Andrew Huberman, members of the All-In Podcast"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Max Sands",
    "section": "",
    "text": "Hi All - I’m Max and Welcome to my Site. I’m a Business Analyst with a passion for Data Science and Investment Research. On here, you can find mini Research Articles covering these topics; feel free to reach out via LinkedIn if they pique your interest, you would like to collaborate, or if you have questions. Cheers :)\n\n  \n    \n\n    \n  \n    \n     \n  \n  \n    \n     LinkedIn\n  \n  \n    \n     \n  \n  \n    \n     Resume\n  \n\n  \n  \nDownload My Resume - PDF\n\n\nMcGill University | Finance | 2019 - 2022\n\n\n\nChoate Investment Advisors | Business Analyst | 2022 - Present\nMcGill University | T.A. - Adv. Bus. Stats | 2021 - 2022\nIronHold Capital | Investment Analyst | 2020 - 2021"
  },
  {
    "objectID": "index.html#current-projects",
    "href": "index.html#current-projects",
    "title": "Max Sands",
    "section": "Current Projects",
    "text": "Current Projects\nReplicating Ray Dalio’s Work: Ray Dalio is personal idol of mine because he is simultaneously an economist, data scientist, and narrator, but, above all else, he exemplifies the power of ‘first-principles’ thinking. He is able to synthesize centuries of history and data into logical, measurable, cause-and-effect relationships. I am currently attempting to build a system that forecasts major countries’ “Big” and “Debt” Cycles using Machine Learning (see Ray Dalio’s Principles for Dealing with With the Changing World Order and Principles for Navigating Big Debt Crises).\nInsider Trading - Scraping SEC Form 4 Filings: Wrote code in R that scrapes the SEC Edgar site and all of its Form 4 filings. I am currently developing a trading strategy that incorporates this data.\nIf you are interested about these projects, would like to collaborate, or would like to request data I’ve gathered, please reach out via LinkedIn - thank you."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html",
    "href": "instructional_articles/00_r_basics/index.html",
    "title": "R in 5 Minutes",
    "section": "",
    "text": "The purpose of this article is 3-fold:\n\nto demonstrate the basics of R as concisely as possible so that you can get up and running on your own projects, even if you’ve had no exposure to coding.\nto act as a basic guide for the non-technical readers interested in following my Research Articles at a more granular level.\nto familiarize myself with the process of writing and explaining topics before I publish my research (and to make sure that my website is working…)\n\n\n\nI would quickly like to explain my background and why I think it is important to have a basic knowledge of ‘coding’:\nI am a Business & Investment Analyst, and 9 months ago I had absolutely no knowledge of ‘coding’; my technical ability was comparable to that of your average dog. I can now tell you 9 months in that understanding the basics of ‘coding’ goes a very long way.\nFirstly, as long as you do a task correctly the first time in code, you can then automate away that task (and its different variations). Whether its performing the same calculations in an Excel file that your boss sends you every morning, or publishing your company’s quarterly financial statements, the same principle applies.\nSecondly, we are living in a world where data is everywhere, and the ability to code allows one to dig into the data and draw valuable insights from it. For anyone in an analytical position (whether Financial Analyst, Medical Researcher, or CEO), this is extremely important and allows you to stand on the shoulders of giants.\nThirdly, you can leverage tools that others have built. There is so much free code on the web and someone else may have already built a tool or completed a task that you are trying to do. This is extremely helpful.\nLastly, a word of caution: coding is not everything. You can be the world’s greatest coder, but if you lack the ability to build a logical, easily-explainable narrative from data, then your value is limited to the tools that you can build for others. In other words, true value comes from the ability to not only work with data, but also derive meaning from it and think originally.\nOk, that’s all; let’s get into it!"
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "href": "instructional_articles/00_r_basics/index.html#the-basics-of-data",
    "title": "R in 5 Minutes",
    "section": "The Basics of Data",
    "text": "The Basics of Data\nData is simply a spreadsheet of values, and we would like our data to be in a ‘tidy’ format.\n\nTidy Data\nData is considered tidy when each column represents a variable and each row consists of an observation. Consider the following dataset (and feel free to inspect the code and guess what each line means):\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNotice how this data is tidy; each column represents a variable (price, color, etc.) and each row is an observed diamond. Your goal should be to have your data in this format because it is easy to manipulate."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#gathering-data",
    "href": "instructional_articles/00_r_basics/index.html#gathering-data",
    "title": "R in 5 Minutes",
    "section": "Gathering Data",
    "text": "Gathering Data\nData is typically gathered from an API, a database, or simply an Excel/csv spreadsheet that you may have. For now, we will use a built-in R dataset called diamonds."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "href": "instructional_articles/00_r_basics/index.html#manipulating-data",
    "title": "R in 5 Minutes",
    "section": "Manipulating Data",
    "text": "Manipulating Data\nAs long as data is in a tidy format, there are only a few actions that we need to do when manipulating data:\n\n\n\nfilter\nfilter data according to certain conditions\n\n\nsummarize\nsummarize the data (e.g. finding the average)\n\n\ngroup\ngroup similar observations\n\n\npivot\n‘pivoting’ the data in different ways\n\n\nselect\nselect relevant information\n\n\nmutate\nchanging the data in some fashion\n\n\n\n\nFiltering\nLet’s pretend that we only want to consider diamonds with a carat greater than .7 and a depth greater than 63: (click on the “Code” section)\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56\n2759\n5.81\n5.85\n3.72\n\n\n0.96\nFair\nF\nSI2\n66.3\n62\n2759\n6.27\n5.95\n4.07\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56\n2760\n5.80\n5.75\n3.65\n\n\n0.91\nFair\nH\nSI2\n64.4\n57\n2763\n6.11\n6.09\n3.93\n\n\n0.91\nFair\nH\nSI2\n65.7\n60\n2763\n6.03\n5.99\n3.95\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58\n2764\n5.64\n5.68\n3.60\n\n\n\n\n\n\nLet’s continue to filter down and consider only the subset with a cut of “Very Good”:\n\n\nCode\ndiamonds %>% \n    filter(carat > .7 & depth > 63) %>% \n    filter(cut == \"Very Good\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.78\nVery Good\nG\nSI2\n63.8\n56.0\n2759\n5.81\n5.85\n3.72\n\n\n0.75\nVery Good\nD\nSI1\n63.2\n56.0\n2760\n5.80\n5.75\n3.65\n\n\n0.71\nVery Good\nD\nSI1\n63.6\n58.0\n2764\n5.64\n5.68\n3.60\n\n\n0.71\nVery Good\nG\nVS1\n63.3\n59.0\n2768\n5.52\n5.61\n3.52\n\n\n0.72\nVery Good\nG\nVS2\n63.7\n56.4\n2776\n5.62\n5.69\n3.61\n\n\n0.75\nVery Good\nD\nSI2\n63.1\n58.0\n2782\n5.78\n5.73\n3.63\n\n\n\n\n\n\nYou will now see that we have from our original 53,940 diamonds, we have filtered down to 1,550 that adhere to our conditions.\nAt this point you may have three questions:\n\nWhat is the %>%?\n\nThis is called a pipe and you can translate it to “and then”. It allows us to perform several operations consecutively. So if we look at the code, we first start with the diamonds dataset by typing diamonds, and then we filter according to carat and depth, and then we filter according to cut. The pipe is extremely useful and it is native to R.\n\nWhat does the head() function do?\n\nIt prints only the first 6 observations, that way you don’t have a table with 50,000 rows on your screen.\n\nWhat if I want to filter down to several different cuts, not just “Very Good”\n\nGreat question, here’s what you would do:\n\n\nCode\ndiamonds %>% \n    filter(cut %in% c(\"Ideal\", \"Premium\")) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.23\nIdeal\nJ\nVS1\n62.8\n56\n340\n3.93\n3.90\n2.46\n\n\n0.22\nPremium\nF\nSI1\n60.4\n61\n342\n3.88\n3.84\n2.33\n\n\n0.31\nIdeal\nJ\nSI2\n62.2\n54\n344\n4.35\n4.37\n2.71\n\n\n\n\n\n\nWe tell R to filter down to the observations where cut matches one of the strings in the vector c(\"Ideal\", \"Premium\"). The c() function creates a vector.\n\n\nSummarizing\nLet’s say we want to summarize the data and find the average diamond price, along with its standard deviation:\n\n\nCode\ndiamonds %>% \n    summarize(avg_price = mean(price),\n              st_dev    = sd(price))\n\n\n\n\n\n\navg_price\nst_dev\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\nNotice that we can take our 50,000+ diamonds and summarize the data down to an average price…\nYou will notice that in the summarize function I start by naming the column I want (avg_price) and then I tell R what to do (find the mean of the price variable/column. The mean() & sd() functions calculate mean and standard deviation respectively). I could just as easily call the columns “thing1” & “thing2”:\n\n\nCode\ndiamonds %>% \n    summarize(thing1 = mean(price),\n              thing2    = sd(price))\n\n\n\n\n\n\nthing1\nthing2\n\n\n\n\n3932.8\n3989.44\n\n\n\n\n\n\n\n\nGrouping\nSummarizing the entire data is important, but let’s say we want to find the average diamond price within each color group…\n\n\nCode\ndiamonds %>% \n    group_by(color) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup()\n\n\n\n\n\n\ncolor\navg_price\n\n\n\n\nD\n3169.954\n\n\nE\n3076.752\n\n\nF\n3724.886\n\n\nG\n3999.136\n\n\nH\n4486.669\n\n\nI\n5091.875\n\n\nJ\n5323.818\n\n\n\n\n\n\nWe can take things a step further and group by color and cut…\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nYou will notice that we now have average price for each color and cut. I also only showed the first 10 rows of output by using the slice() function.\n\n\nPivoting\nPivoting is probably the most complicated of the broad actions I am showing you, but the previous segment allows for a great transition. I decided to show only the first 10 rows of output rather than inundate you with 35 rows, but there must be a better way of showing the output, right? I mean we have letters repeating in the color column. This would make more sense:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    )\n\n\n\n\n\n\ncolor\nFair\nGood\nVery Good\nPremium\nIdeal\n\n\n\n\nD\n4291.061\n3405.382\n3470.467\n3631.293\n2629.095\n\n\nE\n3682.312\n3423.644\n3214.652\n3538.914\n2597.550\n\n\nF\n3827.003\n3495.750\n3778.820\n4324.890\n3374.939\n\n\nG\n4239.255\n4123.482\n3872.754\n4500.742\n3720.706\n\n\nH\n5135.683\n4276.255\n4535.390\n5216.707\n3889.335\n\n\nI\n4685.446\n5078.533\n5255.880\n5946.181\n4451.970\n\n\nJ\n4975.655\n4574.173\n5103.513\n6294.592\n4918.186\n\n\n\n\n\n\nWe tell R to take our 35 row table, and pivot it so that we have a color column followed by columns with the different cuts, wherein each value is the average price.\nThe names_from argument asks us what variable to we want to pivot on (we said ‘cut’ and therefore R took all of the cut values and made them columns). The values_from argument asks us which variable we would like to R to occupy the new columns with (we said ‘avg_price’ and therefore R occupied all of the ‘cells’ in our pivot table with the corresponding values from the avg_price column).\nQuick Tip: hitting the tab key when your cursor is inside of a function’s parentheses will show all of the function’s available arguments (2 of which are names_from and values_from for the pivot_longer() function.)\nImportant Note: You will notice that now we have violated the premise of tidy data. The columns Fair:Ideal are not variables. They are types of “cut” (cut is the variable). For the purposes of coding, and data manipulation, we want our data to be in a tidy format. However, for the purposes of presentation, we typically want our data to be in a ‘wide’ format (hence pivot_wider).\nWe can do the opposite and revert our table back into a ‘long’ format with pivot_longer() :\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols = Fair:Ideal\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\nname\nvalue\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nWe can also rename the columns back to their original names within the pivot_longer() function:\n\n\nCode\ndiamonds %>% \n    group_by(color, cut) %>% \n    summarize(avg_price = mean(price)) %>% \n    ungroup() %>% \n    pivot_wider(\n        names_from  = cut,\n        values_from = avg_price\n    ) %>% \n    pivot_longer(\n        cols      = Fair:Ideal,\n        names_to  = \"cut\",\n        values_to = \"avg_price\"\n    ) %>% \n    slice(1:10)\n\n\n\n\n\n\ncolor\ncut\navg_price\n\n\n\n\nD\nFair\n4291.061\n\n\nD\nGood\n3405.382\n\n\nD\nVery Good\n3470.467\n\n\nD\nPremium\n3631.293\n\n\nD\nIdeal\n2629.095\n\n\nE\nFair\n3682.312\n\n\nE\nGood\n3423.644\n\n\nE\nVery Good\n3214.652\n\n\nE\nPremium\n3538.914\n\n\nE\nIdeal\n2597.550\n\n\n\n\n\n\nThat’s on pivoting…\n\n\nSelecting\nSelecting is straightforward. Here are the first 6 rows of our original dataset:\n\n\nCode\ndiamonds %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nLet’s say we are about to investigate something but we only need price, carat, and cut… then it is best practice to select those variables/columns first (imagine we have thousands of variables/columns…):\n\n\nCode\ndiamonds %>% \n    select(price, carat, cut) %>% \n    head()\n\n\n\n\n\n\nprice\ncarat\ncut\n\n\n\n\n326\n0.23\nIdeal\n\n\n326\n0.21\nPremium\n\n\n327\n0.23\nGood\n\n\n334\n0.29\nPremium\n\n\n335\n0.31\nGood\n\n\n336\n0.24\nVery Good\n\n\n\n\n\n\nWe can also select by omission:\n\n\nCode\ndiamonds %>% \n    select(-x, -y, -z) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\n\n\n\n\n0.23\nIdeal\nE\nSI2\n61.5\n55\n326\n\n\n0.21\nPremium\nE\nSI1\n59.8\n61\n326\n\n\n0.23\nGood\nE\nVS1\n56.9\n65\n327\n\n\n0.29\nPremium\nI\nVS2\n62.4\n58\n334\n\n\n0.31\nGood\nJ\nSI2\n63.3\n58\n335\n\n\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n336\n\n\n\n\n\n\nWe can select variables carat through clarity:\n\n\nCode\ndiamonds %>% \n    select(carat:clarity) %>% \n    head()\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\n\n\n\n\n0.23\nIdeal\nE\nSI2\n\n\n0.21\nPremium\nE\nSI1\n\n\n0.23\nGood\nE\nVS1\n\n\n0.29\nPremium\nI\nVS2\n\n\n0.31\nGood\nJ\nSI2\n\n\n0.24\nVery Good\nJ\nVVS2\n\n\n\n\n\n\nAnd again by omission:\n\n\nCode\ndiamonds %>% \n    select(-carat:-clarity) %>% \n    head()\n\n\n\n\n\n\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nVery simple.\n\n\nMutating\nWhat if we want to perform some sort of calculation or change the data in some way? This is the purpose of mutating…\nIn our dataset, we have the variables x, y, z which represent the length, width, and height of the diamond. If we pretend all the diamonds are cubes, we can calculate the cubic volume of each diamond by multiplying the dimensions of each diamond. Let’s do this:\n\n\nCode\ndiamonds %>% \n    select(x:z) %>% \n    mutate(volume = x * y * z) %>% \n    head()\n\n\n\n\n\n\nx\ny\nz\nvolume\n\n\n\n\n3.95\n3.98\n2.43\n38.20203\n\n\n3.89\n3.84\n2.31\n34.50586\n\n\n4.05\n4.07\n2.31\n38.07688\n\n\n4.20\n4.23\n2.63\n46.72458\n\n\n4.34\n4.35\n2.75\n51.91725\n\n\n3.94\n3.96\n2.48\n38.69395\n\n\n\n\n\n\nNotice how mutate() is similar in structure to summarize(); first we tell R what we would like name our new variable/column (“volume”), and then we tell R how to calculate it.\nMutate can also change a current column:\n\n\nCode\ndiamonds %>% \n    mutate(carat = \"Hello World\") %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\nHello World\nIdeal\nE\nSI2\n61.5\n55\n326\n3.95\n3.98\n2.43\n\n\nHello World\nPremium\nE\nSI1\n59.8\n61\n326\n3.89\n3.84\n2.31\n\n\nHello World\nGood\nE\nVS1\n56.9\n65\n327\n4.05\n4.07\n2.31\n\n\nHello World\nPremium\nI\nVS2\n62.4\n58\n334\n4.20\n4.23\n2.63\n\n\nHello World\nGood\nJ\nSI2\n63.3\n58\n335\n4.34\n4.35\n2.75\n\n\nHello World\nVery Good\nJ\nVVS2\n62.8\n57\n336\n3.94\n3.96\n2.48\n\n\n\n\n\n\nNow, all observations of carat are “Hello World”."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "href": "instructional_articles/00_r_basics/index.html#basic-modeling",
    "title": "R in 5 Minutes",
    "section": "Basic Modeling",
    "text": "Basic Modeling\nWe will build a linear model to explain diamond prices. In R, the function to create a linear model is lm():\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ carat) %>% \n    summary()\n\n\n\nCall:\nlm(formula = price ~ carat, data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18585.3   -804.8    -18.9    537.4  12731.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2256.36      13.06  -172.8   <2e-16 ***\ncarat        7756.43      14.07   551.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1549 on 53938 degrees of freedom\nMultiple R-squared:  0.8493,    Adjusted R-squared:  0.8493 \nF-statistic: 3.041e+05 on 1 and 53938 DF,  p-value: < 2.2e-16\n\n\nWe just built a linear model that regressed carat on diamond price. As you can see, we can use a diamond’s caratage to explain 85% of price variation. Our model also tells us that for every 1 unit increase in caratage, diamond prices increases by $7,756 on average.\nHowever, I’m sure you will agree that the output is not visually pleasing. Moreover, it is not easy to manipulate since it is not in a tabular format.\nLet’s, once again, stand on the shoulders of giants and utilize a tool that someone else has built to clean up the output. Just like you installed tidyverse, install the broom package by running install.packages(\"broom\"). Then, load the package by running library(broom).\n\n\nCode\nlibrary(broom)\n\n\nThis time let’s regress price on all other variables and use the tidy() function from the broom package to tidy the output:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    tidy()\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n5753.761857\n396.629824\n14.5066294\n0.0000000\n\n\ncarat\n11256.978307\n48.627509\n231.4940348\n0.0000000\n\n\ncut.L\n584.457278\n22.478150\n26.0011290\n0.0000000\n\n\ncut.Q\n-301.908158\n17.993919\n-16.7783441\n0.0000000\n\n\ncut.C\n148.034703\n15.483328\n9.5609097\n0.0000000\n\n\ncut^4\n-20.793893\n12.376508\n-1.6801098\n0.0929418\n\n\ncolor.L\n-1952.160010\n17.341767\n-112.5698421\n0.0000000\n\n\ncolor.Q\n-672.053621\n15.776995\n-42.5970601\n0.0000000\n\n\ncolor.C\n-165.282926\n14.724927\n-11.2247022\n0.0000000\n\n\ncolor^4\n38.195186\n13.526539\n2.8237221\n0.0047487\n\n\ncolor^5\n-95.792932\n12.776114\n-7.4978145\n0.0000000\n\n\ncolor^6\n-48.466440\n11.613917\n-4.1731348\n0.0000301\n\n\nclarity.L\n4097.431318\n30.258596\n135.4137965\n0.0000000\n\n\nclarity.Q\n-1925.004097\n28.227228\n-68.1967102\n0.0000000\n\n\nclarity.C\n982.204550\n24.151516\n40.6684433\n0.0000000\n\n\nclarity^4\n-364.918493\n19.285011\n-18.9223900\n0.0000000\n\n\nclarity^5\n233.563110\n15.751700\n14.8278029\n0.0000000\n\n\nclarity^6\n6.883492\n13.715100\n0.5018915\n0.6157459\n\n\nclarity^7\n90.639737\n12.103482\n7.4887321\n0.0000000\n\n\ndepth\n-63.806100\n4.534554\n-14.0710870\n0.0000000\n\n\ntable\n-26.474085\n2.911655\n-9.0924516\n0.0000000\n\n\nx\n-1008.261098\n32.897748\n-30.6483316\n0.0000000\n\n\ny\n9.608887\n19.332896\n0.4970226\n0.6191751\n\n\nz\n-50.118891\n33.486301\n-1.4966983\n0.1344776\n\n\n\n\n\n\nYou will notice that I used ‘.’ to tell R ‘all other variables’ rather than type each of them out. More importantly, the output is much cleaner and easier to manipulate.\nHowever, we cannot see the model’s accuracy. For this, we need to use the glance() function from broom:\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    summary() %>% \n    glance()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\ndf.residual\nnobs\n\n\n\n\n0.9197915\n0.9197573\n1130.094\n26881.83\n0\n23\n53916\n53940\n\n\n\n\n\n\nNow we have accuracy metrics in a nice format.\nLastly, if we would like to see the model’s fit for each observation, we can use the augment() function from broom (scroll to the right):\n\n\nCode\ndiamonds %>% \n    lm(formula = price ~ .) %>% \n    augment() %>% \n    head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprice\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nx\ny\nz\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n326\n0.23\nIdeal\nE\nSI2\n61.5\n55\n3.95\n3.98\n2.43\n-1346.3643\n1672.3643\n0.0003742\n1130.082\n0.0000342\n1.4801217\n\n\n326\n0.21\nPremium\nE\nSI1\n59.8\n61\n3.89\n3.84\n2.31\n-664.5954\n990.5954\n0.0004133\n1130.097\n0.0000132\n0.8767411\n\n\n327\n0.23\nGood\nE\nVS1\n56.9\n65\n4.05\n4.07\n2.31\n211.1071\n115.8929\n0.0009098\n1130.105\n0.0000004\n0.1025982\n\n\n334\n0.29\nPremium\nI\nVS2\n62.4\n58\n4.20\n4.23\n2.63\n-830.7372\n1164.7372\n0.0004062\n1130.094\n0.0000180\n1.0308641\n\n\n335\n0.31\nGood\nJ\nSI2\n63.3\n58\n4.34\n4.35\n2.75\n-3459.2242\n3794.2242\n0.0007715\n1129.987\n0.0003629\n3.3587358\n\n\n336\n0.24\nVery Good\nJ\nVVS2\n62.8\n57\n3.94\n3.96\n2.48\n-1380.4876\n1716.4876\n0.0007230\n1130.081\n0.0000696\n1.5194380\n\n\n\n\n\n\nThe broom package is so useful because it cleans up model output, but more importantly, it can be used with many other (more complex) models."
  },
  {
    "objectID": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "href": "instructional_articles/00_r_basics/index.html#visualizing-data",
    "title": "R in 5 Minutes",
    "section": "Visualizing Data",
    "text": "Visualizing Data\nBeing able to visualize data is essential for understanding it; the famous saying “a picture is worth a thousands words” is doubly true in today’s age.\nLet’s start out by plotting diamond price against caratage.\n\nCreating a Canvas\nFirst we need to create a canvas with the ggplot() function:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price))\n\n\n\n\n\nNotice that we start with the diamonds dataset and then we create a canvas with the ggplot() function. The aes() function stands for aesthetic and allows us to pick which variables/columns we want to use in our plot. In this case we tell R that we want to plot carat on the x-axis and price on the y-axis.\n\n\nAdding Geoms\nIn our plot we would like to add dots that represent each data point. In R adding these elements are called geometries (i.e. geoms):\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point()\n\n\n\n\n\nNotice how when creating plots with ggplot, we can no longer use the pipe (%>%). Instead, we use a + sign to add layers to the plot.\nFrom our plot we can tell that there is a clear positive relationship between price and caratage.\n\n\nModifying Geoms\nOur plot contains so many points and it is overwhelming; let’s modify the plot so that the points are more transparent with the alpha argument of geom_point().\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price)) +\n    geom_point(alpha = .15, color = \"midnightblue\") +\n    geom_smooth()\n\n\n\n\n\nYou will notice that the points are more transparent and that we also modified their color. We also included a smoother line with geom_smooth().\n\n\nAdding Aesthetics\nUp to now our plot has had only 2 aesthetics (x and y). But, all of the arguments that can be passed to geoms (alpha, color, etc.) are actually aesthetics that can be passed in the main aes() function. This probably sounds confusing but the following code will make much more sense:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth()\n\n\n\n\n\nYou will notice that instead of locally changing the color argument in the geom_point() function, we have put in the main aes() function wherein we set it equal to cut. By doing this, we are telling R that the color of each geometry should be defined by the cut variable/column.\n\n\nFaceting\nOur plot is overwhelming with all the different colors on one canvas so lets create a faceted canvas… Rather than explain in words, the following code should be self evident:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut)\n\n\n\n\n\nThis is called a faceted plot because we have created facets according to the cut variable/column. You will note that we need to put a ~ before the specified variable; this is just how the facet_wrap function works.\nWe can also decide to facet according to some other variables, like so:\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~clarity, scales = \"free\")\n\n\n\n\n\nYou will notice that I also supplied the scales argument within the facet_wrap() function which allows each faceted plot to have different x and y scales that fit accordingly. Compare the x and y axes of the ‘VS1’ plot with those of the ‘VVS2’. They have different scales.\n\n\nAdding Labels\nLet’s add labels to our plot…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    )\n\n\n\n\n\n\n\nChanging Theme\nR has some preset plotting themes…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_linedraw()\n\n\n\n\n\n…there are several others.\n\n\nModifying Scales\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = carat, y = price, color = cut)) +\n    geom_point(alpha = .15) +\n    geom_smooth() +\n    facet_wrap(~cut) +\n    labs(\n        title = \"Price vs. Carat\",\n        subtitle = \"ggplot makes plotting so easy...\",\n        y = \"Price (in $)\",\n        x = \"Carat\",\n        caption = \"This is a great-looking plot\"\n    ) +\n    theme_bw() +\n    scale_y_continuous(labels = scales::dollar_format())\n\n\n\n\n\nNotice we converted the axis/scale on the plot to a dollar format…\n\n\nExample of More Plots\nWith these basic tools, you now have the ability to create so many different types of plots to gain insights from your data.\nHere are a few more plots with code to give you a flavor…\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram(position = \"dodge\") +\n    theme_bw() +\n    scale_fill_brewer()\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_density() +\n    theme_bw() +\n    scale_fill_brewer() +\n    facet_wrap(~cut)\n\n\n\n\n\nThere are other packages that help with creating nice plots… install and load ggridges.\n\n\nCode\nlibrary(ggridges)\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = stat(x))) +\n    geom_density_ridges_gradient(scale = 2) +\n    scale_fill_viridis_c(name = \"Price (in $)\", option = \"C\") +\n    theme_minimal() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCode\ndiamonds %>% \n    ggplot(aes(x = price, y = cut, fill = factor(stat(quantile)))) +\n    stat_density_ridges(\n        geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n        quantiles = 4, quantile_lines = TRUE\n    ) +\n    scale_fill_brewer() +\n    theme_linedraw() +\n    scale_x_continuous(labels = scales::dollar_format())\n\n\n\n\n\n\n\nCreating Interactive Plots\nWe also have the ability to create interactive plots with the help of a package called plotly. This is another example of the power of open-source coding, which gives us the ability to leverage code that others have built (that we may not have the expertise to create ourselves…). Like we did with the tidyverse, run install.packages(\"plotly\") and then load it into your environment with library(plotly). All we have to do to make a plot interactive, is to save it into our environment using the assignment operator - <-. I am going to save my plot as g and then we have to run ggplotly(g).\nLook at the code below:\n\n\nCode\nlibrary(plotly)\ng <- diamonds %>% \n    ggplot(aes(price, fill = cut)) +\n    geom_histogram() +\n    theme_bw()\n\nggplotly(g)\n\n\n\n\n\n\nThis is just a taste of the plots that can be generated…"
  },
  {
    "objectID": "instructional_articles.html",
    "href": "instructional_articles.html",
    "title": "Instructional Articles",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nR in 5 Minutes\n\n\n\nR\n\n\n\nApplication-based Learning\n\n\n\nMax Sands\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/06_politicians/politicians.html",
    "href": "research/06_politicians/politicians.html",
    "title": "Political Power to Market Power: An Examination of U.S. Politicians’ Investment Performance",
    "section": "",
    "text": "Intro\nThe performance of investment portfolios held by U.S. politicians presents an intriguing paradox within the financial landscape. Various empirical studies have demonstrated that these portfolios tend to yield returns that surpass those of the average retail investor. The precise factors contributing to this outperformance remain a subject of ongoing investigation and debate.\nA pioneering study conducted by Alan J. Ziobrowski and his team in 2004 found that U.S. Senators’ stock portfolios surpassed market returns by an impressive 12% annually. In 2011, Ziobrowski expanded this line of investigation to members of the U.S. House of Representatives, whose portfolios were observed to outperform market returns by approximately 6% annually1.\nWhile these studies did not directly attribute such investment outperformance to the use of insider information, the consistent pattern of superior returns does raise questions about the sources of information and analysis guiding these investment decisions.\nSubsequent to these studies, the passage of the STOCK (Stop Trading on Congressional Knowledge) Act in 2012 sought to curtail potential misuse of nonpublic information by explicitly prohibiting members of Congress from engaging in stock trading based on such data. The effectiveness of this legislative measure, however, remains difficult to evaluate due to inherent challenges in determining the precise basis of investment decisions.\nAt a first glance, it appears that the STOCK Act of 2012 has had limited success in mitigating the abnormal returns generated in the portfolios of these politicians. Quiver Quant, a platform designed by 2 college students with the goal of providing alternative data-sets to retail investors, created a portfolio to measure just this:\n\n\n\n\n\nAs we can see, the back tested Quiver portfolio yielded a CAGR of 43.69% since 2020-04-01, whereas the market experienced a CAGR of approximately 11% during this time period. This 32% difference seems rather large, especially when compared to the findings provided by Ziobrowski and his team. Because of this, we will explore the current state of this phenomena, scrutinizing available data, and evaluating whether the investment outperformance trend among U.S. politicians persists to this day. We will analyze the implications of these patterns, probe the role of legislation such as the STOCK Act, and examine the potential factors contributing to the superior returns exhibited by this unique class of investors.\n\nThe Data\nBelow is a glimpse of the data, scraped from regulatory filings and provided by House Stock Watcher & Senate Stock Watcher:\n\n\nRows: 9,240\nColumns: 10\n$ transaction_date    <date> 2021-09-27, 2021-09-13, 2021-09-10, 2021-09-28, 2…\n$ disclosure_date     <date> 2021-10-04, 2021-10-04, 2021-10-04, 2021-10-04, 2…\n$ days_late           <dbl> 7, 21, 24, 6, 7, 7, 20, 25, 26, 29, 8, 7, 25, 18, …\n$ representative      <chr> \"Virginia Foxx\", \"Virginia Foxx\", \"Virginia Foxx\",…\n$ representative_type <chr> \"Congress Member\", \"Congress Member\", \"Congress Me…\n$ party               <chr> \"Republican\", \"Republican\", \"Republican\", \"Republi…\n$ state               <chr> \"NC\", \"NC\", \"NC\", \"NC\", \"NY\", \"NY\", \"NC\", \"NC\", \"N…\n$ owner               <chr> \"Joint\", \"Joint\", \"Joint\", \"Joint\", \"Unknown\", \"Un…\n$ amount              <fct> \"$1,001 - $15,000\", \"$1,001 - $15,000\", \"$15,001 -…\n$ stock_performance   <list> [<tbl_df[86 x 6]>], [<tbl_df[87 x 6]>], [<tbl_df[…\n\n\n\n\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nThe Journal of Financial and Quantitative Analysis - Vol. 39, No. 4 (Dec., 2004), pp. 661-676 (16 pages)↩︎"
  }
]