---
title: "★ Post-Earnings Announcement Drift (PEAD) Anomaly & Machine Learning"
author: "Max Sands"
date: "2023-04-01"
description: "Read Time: 10 mins"
categories: [Equities, ★]
editor_options: 
  chunk_output_type: console
warning: false
message: false
echo: false
fig-align: center
---

```{r, include=FALSE}
library(tidyverse)
library(tidyquant)
library(patchwork)
library(timetk)
library(here)

source(here("R Scripts", "plot_formatting.R"))
source(here("raw_data", "PEAD", "PEAD_scripts.R"))

data_prep <- read_rds(here("raw_data", "PEAD", "earnings_history_data_prep.rds"))
plots_list <- read_rds(here("raw_data", "PEAD", "plots_list.rds"))

num_companies <- data_prep %>% 
    distinct(ticker) %>% 
    nrow()

comparison_tbl <- read_rds(here("raw_data", "PEAD", "comparison_tbl.rds"))
```

# Intro - What is PEAD?

Post-Earnings Announcement Drift (PEAD) anomaly is a phenomenon where the stock prices of firms continue to exhibit abnormal returns after they release their earnings reports. Specifically, the stock prices of firms that have reported positive earnings surprises tend to continue to rise in the days and weeks following the release of their earnings report, while the stock prices of firms that have reported negative earnings surprises tend to continue to decline.

To be clear, we will consider earnings surprise to be equal to:

$$
Avg\ Analyst\ Estimate\ of\ EPS - Actual\ Reported\ EPS
$$

The PEAD anomaly is considered an anomaly because it contradicts the Efficient Market Hypothesis (EMH), which suggests that all available information is immediately reflected in stock prices, leaving no room for abnormal returns.

Several explanations have been proposed to explain the PEAD anomaly like the fact that investors may underestimate the persistence of a firm's earnings surprises, leading to delayed price adjustments. Another explanation is that earnings surprises may contain information that is not captured in traditional accounting measures, leading to a delayed reaction by the market.

Regardless of the reason for its existence, the PEAD anomaly has important implications for investors and financial managers, as it suggests that trading strategies based on earnings surprises can generate abnormal returns. In addition, it also highlights the limitations of the EMH, which is a fundamental theory in finance.

<hr>

Let's investigate the presence of PEAD and attempt to build a trading strategy with the use of Machine Learning. To do this I have gathered:

1.  Earnings History on `r num_companies` large, publicly-traded companies
2.  Price, Volume, and Miscellaneous Financial data for each of these companies

<hr>

# Illustrating the PEAD Anomaly

If the PEAD anomaly were to hold, we would expect to see abnormal returns following a company's earnings release, and we would expect the returns to be dependent on the magnitude in which the company surprised on earnings. Therefore, lets bucket each observation according to the magnitude of their earnings surprise and plot the average cumulative performance of these observations 1 day, 3 days, 5 days, 20 days, and 60 days after their earnings release.

::: panel-tabset
#### Nominal (\$) Earnings Surprise

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal
```

#### Pct (%) Earnings Surprise

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_pct
```
:::

From the above heat maps, we notice that *markets are inefficient in the short-term, following earnings releases*. If a company's nominal earnings surprise is in the 90th+ percentile (bucket 10), *on average*, the company's stock return *the next day* is 1.33%! Conversely, if a company's nominal earnings surprise is in the 0-10th percentile (bucket 1), *on average*, the company's stock return *the next day* is -1.09%!

Interestingly, we notice that the PEAD anomaly is more distinguished when measuring earnings surprise in nominal dollars rather than percent change, which is slightly un-intuitive.

While the above heat maps represent averages of all the data since 2015, it is also a good idea to make sure that we observe the PEAD anomaly for each individual year:

::: panel-tabset
#### 2015

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2015
```

#### 2016

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2016
```

#### 2017

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2017
```

#### 2018

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2018
```

#### 2019

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2019
```

#### 2020

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2020
```

#### 2021

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2021
```

#### 2022

```{r}
#| fig-width: 6.75
#| fig-height: 6.75
#| fig-align: center
plots_list$heatmap_nominal_2022
```
:::

It is clear that the PEAD anomaly is prevalent throughout time, especially in the extreme short-term (1 day post-earnings release).

Given that we are looking at nominal earnings surprise, which may be affected by company size, let's visualize the PEAD anomaly for different buckets of market cap / company size:

```{r}
#| fig-width: 8
#| fig-height: 8
plots_list$heatmap_nominal_grouped_by_market_cap
```

From the above, it is clear that the PEAD anomaly exists and that variables like company size have an affect on the magnitude of PEAD.

# Assessing the Viability of a PEAD Trading Strategy

Let's assess the potential viability of implementing a trading strategy that goes long 5 stocks that fall in deciles 9 & 10 each day for 90 days.

To do this, we will sample at random (with replacement) 5 returns that occurred on the day after a company's earnings release, contingent upon the fact that their earnings surprise falls in deciles 9 or 10. We will do this 90 times, to simulate 90 days. We will then repeat this process 1000 times so that we have 1000 simulations of 90 days. To make our simulation realistic, we will assume that we can only keep 60% of the profits due to typical trading costs (i.e. Short-Term Capital Gains Tax, Trading Fees, etc.).

The below plots demonstrate 1000 simulations of our trading strategy:

::: panel-tabset
#### Raw

```{r}
#| fig-width: 8
#| fig-height: 6.5
plots_list$monte_carlo_5day + scale_color_tq() + theme(text = element_text(size = 14))
```

#### By Decile

```{r}
#| fig-width: 8
#| fig-height: 6.5
plots_list$monte_carlo_decile_5day + scale_color_tq() + theme(text = element_text(size = 14))
```
:::

In the above scenario, we assumed we could invest in 5 companies a day, but for good measure, let's assume that we can only invest in 1 company per day. This will increase the volatility of our strategy:

::: panel-tabset
#### Raw

```{r}
#| fig-width: 8
#| fig-height: 6.5
plots_list$monte_carlo_1day + scale_color_tq() + theme(text = element_text(size = 14))
```

#### By Decile

```{r}
#| fig-width: 8
#| fig-height: 6.5
plots_list$monte_carlo_deciles_1day + scale_color_tq() + theme(text = element_text(size = 14))
```
:::

The above demonstrates that implementing a short-term PEAD trading strategy can be *immensely* profitable. More importantly, the above assumes equal weighting across investments (in the case with 5 investments/day) with no selection criterion other than earnings surprise in the 9th or 10th decile. With the use of Machine Learning and adequate feature engineering, it is *highly likely* that we can build a model that designates an allocation schema which drastically *improves returns* while *reducing* *variability.*

However, in the above, we assume that this strategy can be implemented for 90 trading days, but this may not be the case. If all companies release earnings on the same 4 days, then this trading strategy is fairly useless as it can only be implemented 4 times a year. Let's take a look at the count of company earnings releases by day for 2022:

```{r}
#| fig-width: 8
#| fig-height: 6

g1 <- data_prep %>% 
    filter(lubridate::year(announcement_date) == 2022) %>% 
    count(announcement_date) %>% 
    ggplot(aes(announcement_date, n)) +
    geom_col() +
    theme_tq(base_size = 14) +
    labs(
        x = "",
        y = "# of Earnings Releases"
    )

g2 <- data_prep %>% 
    filter(lubridate::year(announcement_date) == 2022) %>% 
    count(announcement_date) %>% 
    timetk::pad_by_time(.pad_value = 0) %>%
    mutate(dow = wday(announcement_date, label = T)) %>% 
    filter(!(dow %in% c("Sat", "Sun"))) %>% 
    ggplot(aes(n)) +
    geom_boxplot(fill = palette_light()[1],
        alpha = .8) +
    geom_label(
        data = . %>% 
            mutate(quantile = ntile(n, n = 4)) %>% 
            group_by(quantile) %>% 
            summarize(lower_bound = min(n)) %>% 
            slice(-1),
        mapping = aes(lower_bound, y = 0, label = lower_bound),
        size = 4.5
    ) +
    theme_tq(base_size = 14) +
    coord_cartesian(xlim = c(0, 50)) +
    scale_y_continuous(labels = NULL) +
    labs(x = "Distribution of Earnings Releases per Day",
         y = "")

g1 + g2
```

Of the 252 trading days in a year, we observe 211 unique days in which one or more companies released earnings. Moreover, we can see from our box-plot on the right that approximately half of the trading days experience 7 or more earnings releases for the 1633 companies we have gathered data on in 2022.

From this, we can assume that strategy can be implemented for a large portion of the trading year. However, an upper-bound on strategy size (i.e. how much money can be invested each day before the strategy is constrained by volume, etc.) is difficult to estimate and will have to be evaluated through trial and error.

# Improving Performance w/ Machine Learning

While not covered here, I performed some feature engineering and built a Stacked Ensemble Machine Learning Model that yielded an average MAE and RMSE of 3.6% and 5.5%, respectively, after 10-fold Cross Validation. For reference, the MAE indicates how wrong we are, on average. The RMSE indicates the same thing, but it is more sensitive to predictions that are wrong by a large margin. With time, and proper feature engineering and modelling, it is my belief that model accuracy can be improved by 20% or more.

This model was then used to predict on a test set of 5,565 observations. Below is a table with the model predictions vs. actual results:

```{r}
library(DT)
comparison_tbl %>% 
    mutate(across(contains("eps"), ~scales::dollar(.x, accuracy = .01))) %>%
    mutate(across(c(predict:last_col()), ~scales::percent(.x, accuracy = .01))) %>%
    set_names(c("Ticker", "Release Date", "Actual EPS", "Estimated EPS",
                "EPS Surprise", "Prediction (1 Day)", "1 Day", "5 Day", "20 Day", "60 Day")) %>% 
    DT::datatable()
```

<br>

From our table, we see observe that there is a lot of variation in returns, and it may be useful to try and predict 20 Day or 60 Day returns. It would also be extremely helpful to build a model that indicates if we should hold on to a poor investment or cut our losses. For instance, observation 10 (QLYS) saw a 25% drop in its stock price the day after its earnings release, but this quickly reversed and the company saw a net cumulative return of 14.6% after 20 days. Therefore, building a model that aims to answer the following - *conditional on the fact that we predicted +X% for a 1 Day return, and the investment yielded -Y% return, what is our prediction for a 5, 10 , 20, 60 Day (etc.) return? -* is likely to be of great value.

Let's assume that we can invest in 5 companies per day for 90 days, and simulate how we may have performed had we gone long the companies where we predicted a positive 1 day return, and gone short the companies where we predicted a negative 1 day return. Again, we will assume that we only retain 60% of the profits.

```{r}
#| fig-width: 8
#| fig-height: 6.5

scale_0_1 <- function(x){(x-min(x))/(max(x)-min(x))}

weighted_return_vec <- comparison_tbl %>% 
    transmute(weighted_return = ifelse(predict>0, post_ret_1d, -post_ret_1d)) %>% 
    pull()

monte_carlo(weighted_return_vec, num_simulations = 1000, number_of_returns_averaged = 5,
            prop_profits_kept = .6, replace = F, number_of_time_periods = 125) %>% 
    plot_monte_carlo(raw = F, quantiles = 10) +
    labs(
        y = "Wealth Index ($1)",
        x = "Days"
    ) +
    theme(text = element_text(size = 14))
```

Under these conditions, our trading strategy yields strong returns, and there is so much that can be improved...
